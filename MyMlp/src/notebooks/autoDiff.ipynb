{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f1ba62",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3221369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Repos/AWiD/MyMlp`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[92m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "    998.8 ms\u001b[32m  ✓ \u001b[39m\u001b[90mLearnBase\u001b[39m\n",
      "    954.8 ms\u001b[32m  ✓ \u001b[39m\u001b[90mMLLabelUtils\u001b[39m\n",
      "   1178.2 ms\u001b[32m  ✓ \u001b[39m\u001b[90mMLDataPattern\u001b[39m\n",
      "   3868.0 ms\u001b[32m  ✓ \u001b[39mDistributions\n",
      "   1728.8 ms\u001b[32m  ✓ \u001b[39mMLDataUtils\n",
      "   1136.4 ms\u001b[32m  ✓ \u001b[39mDistributions → DistributionsChainRulesCoreExt\n",
      "   1150.2 ms\u001b[32m  ✓ \u001b[39mDistributions → DistributionsTestExt\n",
      "   5027.6 ms\u001b[32m  ✓ \u001b[39m\u001b[90mMLUtils\u001b[39m\n",
      "   6163.7 ms\u001b[32m  ✓ \u001b[39mMLDatasets\n",
      "  43323.7 ms\u001b[32m  ✓ \u001b[39mPlots\n",
      "   2582.3 ms\u001b[32m  ✓ \u001b[39mPlots → UnitfulExt\n",
      "   2625.2 ms\u001b[32m  ✓ \u001b[39mPlots → FileIOExt\n",
      "   4541.2 ms\u001b[32m  ✓ \u001b[39mMyMlp\n",
      "  13 dependencies successfully precompiled in 54 seconds. 325 already precompiled.\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mProject\u001b[22m\u001b[39m MyMlp v0.1.0\n",
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "  \u001b[90m[6e4b80f9] \u001b[39mBenchmarkTools v1.6.0\n",
      "  \u001b[90m[a93c6f00] \u001b[39mDataFrames v1.7.0\n",
      "  \u001b[90m[31c24e10] \u001b[39mDistributions v0.25.120\n",
      "  \u001b[90m[cc2ba9b6] \u001b[39mMLDataUtils v0.5.4\n",
      "  \u001b[90m[eb30cadb] \u001b[39mMLDatasets v0.7.18\n",
      "  \u001b[90m[91a5bcdd] \u001b[39mPlots v1.40.13\n",
      "  \u001b[90m[10745b16] \u001b[39mStatistics v1.11.1\n",
      "  \u001b[90m[37e2e46d] \u001b[39mLinearAlgebra v1.11.0\n",
      "  \u001b[90m[9a3f8284] \u001b[39mRandom v1.11.0\n"
     ]
    }
   ],
   "source": [
    "# Make sure we're using the latest version\n",
    "import Pkg\n",
    "Pkg.activate(\"../..\")  # Activate the project environment\n",
    "\n",
    "Pkg.add(\"BenchmarkTools\")\n",
    "Pkg.add(\"LinearAlgebra\")\n",
    "Pkg.add(\"Statistics\")\n",
    "Pkg.add(\"Distributions\")\n",
    "Pkg.add(\"Random\")\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"MLDatasets\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"MLDataUtils\")\n",
    "Pkg.instantiate()      # Install any missing dependencies\n",
    "Pkg.status()          # Check if MyMlp is listed\n",
    "\n",
    "\n",
    "# Now try importing\n",
    "using BenchmarkTools\n",
    "using LinearAlgebra\n",
    "using Distributions\n",
    "using Random\n",
    "using MLDatasets\n",
    "using Plots\n",
    "using Statistics\n",
    "using DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb89e6c",
   "metadata": {},
   "source": [
    "## Comparison of different optimizations to Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc8f4feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Benchmark: READ access ###\n",
      "Original:\n",
      "  14.034 ns (1 allocation: 16 bytes)\n",
      "Optimized:\n",
      "  1.575 ns (0 allocations: 0 bytes)\n",
      "RefValue:\n",
      "  1.793 ns (0 allocations: 0 bytes)\n",
      "\n",
      "### Benchmark: WRITE mutation ###\n",
      "Original:\n",
      "  14.417 ns (1 allocation: 16 bytes)\n",
      "Optimized:\n",
      "  2.013 ns (0 allocations: 0 bytes)\n",
      "RefValue:\n",
      "  2.013 ns (0 allocations: 0 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500503.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"BenchmarkTools\")\n",
    "using BenchmarkTools\n",
    "\n",
    "abstract type GraphNode end\n",
    "abstract type Operator <: GraphNode end\n",
    "\n",
    "# Original implementation\n",
    "mutable struct VariableOriginal <: GraphNode\n",
    "    output :: Any\n",
    "    grad :: Any\n",
    "    name :: String\n",
    "    VariableOriginal(output; name=\"?\") = new(output, nothing, name)\n",
    "end\n",
    "\n",
    "# Optimized implementation\n",
    "mutable struct VariableOptimized{T<:Float64} <: GraphNode\n",
    "    output :: T\n",
    "    grad :: Union{Nothing, T}\n",
    "    name :: String\n",
    "    VariableOptimized(output::T; name=\"?\") where T<:Float64 = new{T}(output, nothing, name)\n",
    "end\n",
    "\n",
    "# RefValue-based immutable implementation\n",
    "struct VariableRef <: GraphNode\n",
    "    output :: Base.RefValue{Float64}\n",
    "    grad   :: Union{Nothing, Base.RefValue{Float64}}\n",
    "    name   :: String\n",
    "    function VariableRef(output::Float64; name=\"?\")\n",
    "        new(Base.RefValue(output), nothing, name)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Functions to benchmark: reading\n",
    "function read_output(v::VariableOriginal)\n",
    "    v.output + 1.0\n",
    "end\n",
    "\n",
    "function read_output(v::VariableOptimized)\n",
    "    v.output + 1.0\n",
    "end\n",
    "\n",
    "function read_output(v::VariableRef)\n",
    "    v.output[] + 1.0\n",
    "end\n",
    "\n",
    "# Functions to benchmark: writing\n",
    "function write_output!(v::VariableOriginal)\n",
    "    v.output = v.output + 1.0\n",
    "end\n",
    "\n",
    "function write_output!(v::VariableOptimized)\n",
    "    v.output = v.output + 1.0\n",
    "end\n",
    "\n",
    "function write_output!(v::VariableRef)\n",
    "    v.output[] = v.output[] + 1.0\n",
    "end\n",
    "\n",
    "# Create instances\n",
    "v1 = VariableOriginal(1.0)\n",
    "v2 = VariableOptimized(1.0)\n",
    "v3 = VariableRef(1.0)\n",
    "\n",
    "# Benchmark READ\n",
    "println(\"### Benchmark: READ access ###\")\n",
    "println(\"Original:\")\n",
    "@btime read_output($v1)\n",
    "\n",
    "println(\"Optimized:\")\n",
    "@btime read_output($v2)\n",
    "\n",
    "println(\"RefValue:\")\n",
    "@btime read_output($v3)\n",
    "\n",
    "# Benchmark WRITE\n",
    "println(\"\\n### Benchmark: WRITE mutation ###\")\n",
    "println(\"Original:\")\n",
    "@btime write_output!($v1)\n",
    "\n",
    "println(\"Optimized:\")\n",
    "@btime write_output!($v2)\n",
    "\n",
    "println(\"RefValue:\")\n",
    "@btime write_output!($v3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4694cbbe",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c212c68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 5 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Base: *, +, clamp, log, exp\n",
    "import LinearAlgebra: mul!\n",
    "import Statistics: sum\n",
    "\n",
    "abstract type GraphNode end\n",
    "abstract type Operator <: GraphNode end\n",
    "\n",
    "# Definition of basic structures for computational graph\n",
    "mutable struct Constant{T<:Matrix{Float64}} <: GraphNode\n",
    "    output :: T\n",
    "end\n",
    "\n",
    "mutable struct Variable{T<:Matrix{Float64}} <: GraphNode\n",
    "    output :: T\n",
    "    gradient :: Union{Nothing, T}\n",
    "    name :: String\n",
    "    \n",
    "    Variable(output::T; name=\"?\") where {T<:Matrix{Float64}} = new{T}(output, nothing, name)\n",
    "end\n",
    "\n",
    "mutable struct ScalarOperator{F} <: Operator\n",
    "    inputs :: Union{Nothing, Tuple{GraphNode, GraphNode}}\n",
    "    output :: Union{Nothing, Float64}\n",
    "    gradient :: Union{Nothing, Float64}\n",
    "    name :: String\n",
    "    ScalarOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "end\n",
    "\n",
    "mutable struct BroadcastedOperator{F} <: Operator\n",
    "    inputs :: Union{Nothing, Tuple{GraphNode, GraphNode}, Tuple{GraphNode}}\n",
    "    output :: Union{Nothing, Matrix{Float64}}\n",
    "    gradient :: Union{Nothing, Matrix{Float64}}\n",
    "    name :: String\n",
    "    BroadcastedOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "end\n",
    "\n",
    "\n",
    "import Base: show, summary\n",
    "show(io::IO, x::ScalarOperator{F}) where {F} = print(io, \"op \", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::BroadcastedOperator{F}) where {F} = print(io, \"op.\", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::Constant) = print(io, \"const \", x.output)\n",
    "show(io::IO, x::Variable) = begin\n",
    "    print(io, \"var \", x.name);\n",
    "    print(io, \"\\n ┣━ ^ \"); summary(io, x.output)\n",
    "    print(io, \"\\n ┗━ ∇ \");  summary(io, x.gradient)\n",
    "end\n",
    "\n",
    "\n",
    "function visit(node::GraphNode, visited, order)\n",
    "    if node ∈ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function visit(node::Operator, visited, order)\n",
    "    if node ∈ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        for input in node.inputs\n",
    "            visit(input, visited, order)\n",
    "        end\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function topological_sort(head::GraphNode)\n",
    "    visited = Set()\n",
    "    order = Vector()\n",
    "    visit(head, visited, order)\n",
    "    return order\n",
    "end\n",
    "\n",
    "\n",
    "# x * y (aka matrix multiplication)\n",
    "*(A::GraphNode, x::GraphNode) = BroadcastedOperator(mul!, A, x)\n",
    "forward(::BroadcastedOperator{typeof(mul!)}, A, x) = return A * x\n",
    "backward(::BroadcastedOperator{typeof(mul!)}, A, x, g) = tuple(g * x', A' * g)\n",
    "\n",
    "# relu activation\n",
    "relu(x::GraphNode) = BroadcastedOperator(relu, x)\n",
    "forward(::BroadcastedOperator{typeof(relu)}, x) = return x .* (x .> 0.0)\n",
    "backward(::BroadcastedOperator{typeof(relu)}, x, g) = tuple(g .* (x .> 0.0), nothing)\n",
    "\n",
    "# add operation (for bias)\n",
    "+(x::GraphNode, y::GraphNode) = BroadcastedOperator(+, x, y)\n",
    "forward(::BroadcastedOperator{typeof(+)}, x, y) = return x .+ y\n",
    "backward(::BroadcastedOperator{typeof(+)}, x, y, g) = begin\n",
    "    grad_wrt_x = g\n",
    "    # Gradient biasu: sumowanie gradientu g wzdłuż wymiaru batcha (dims=1)\n",
    "    # i przekształcenie wyniku 1xN do wektora N-elementowego.\n",
    "    grad_wrt_y = sum(g, dims=2)\n",
    "    return (grad_wrt_x, grad_wrt_y)\n",
    "end\n",
    "\n",
    "# sigmoid activation\n",
    "σ(x::GraphNode) = BroadcastedOperator(σ, x)\n",
    "forward(::BroadcastedOperator{typeof(σ)}, x) = return 1.0 ./ (1.0 .+ exp.(-x))\n",
    "backward(node::BroadcastedOperator{typeof(σ)}, x, g) = begin\n",
    "    y = node.output\n",
    "    local_derivative = y .* (1.0 .- y)\n",
    "    grad_wrt_x = g .* local_derivative\n",
    "    return (grad_wrt_x, nothing)\n",
    "end\n",
    "\n",
    "# # binarycrossentropy\n",
    "# binarycrossentropy(y::GraphNode, ŷ::GraphNode) = ScalarOperator(binarycrossentropy, y, ŷ)\n",
    "# forward(::ScalarOperator{typeof(binarycrossentropy)}, ŷ, y) = begin\n",
    "#     return -mean(y .* log.(ŷ) + (1.0 .- y) .* log.(1.0 .- ŷ))\n",
    "# end\n",
    "# backward(::ScalarOperator{typeof(binarycrossentropy)}, ŷ, y, g) = begin\n",
    "#     J = (ŷ .- y) ./ (ŷ .* (1.0 .- ŷ))\n",
    "#     return (J * g, nothing)\n",
    "# end\n",
    "\n",
    "function binary_cross_entropy_loss_impl(ŷ, y_true; epsilon=1e-10)\n",
    "    ŷ_clamped = clamp.(ŷ, epsilon, 1.0 - epsilon)\n",
    "    loss_elements = -y_true .* log.(ŷ_clamped) .- (1.0 .- y_true) .* log.(1.0 .- ŷ_clamped)\n",
    "    return mean(loss_elements)\n",
    "end\n",
    "\n",
    "binarycrossentropy(ŷ::GraphNode, y::GraphNode) = ScalarOperator(binary_cross_entropy_loss_impl, ŷ, y)\n",
    "\n",
    "forward(::ScalarOperator{typeof(binary_cross_entropy_loss_impl)}, ŷ_value, y_value) = begin\n",
    "    loss_value = binary_cross_entropy_loss_impl(ŷ_value, y_value)\n",
    "    return loss_value\n",
    "end\n",
    "\n",
    "backward(::ScalarOperator{typeof(binary_cross_entropy_loss_impl)}, ŷ_value, y_value, g) = begin\n",
    "    epsilon = 1e-10\n",
    "    ŷ_clamped_for_grad = clamp.(ŷ_value, epsilon, 1.0 - epsilon)\n",
    "    local_grad_per_sample = (ŷ_clamped_for_grad .- y_value) ./ (ŷ_clamped_for_grad .* (1.0 .- ŷ_clamped_for_grad))\n",
    "    batch_size = size(y_value, 2)\n",
    "    grad_wrt_ŷ = local_grad_per_sample ./ batch_size\n",
    "    return (grad_wrt_ŷ, nothing)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3f27fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reset!(node::Constant) = nothing\n",
    "\n",
    "reset!(node::Variable) = node.gradient = nothing\n",
    "reset!(node::Operator) = node.gradient = nothing\n",
    "\n",
    "compute!(node::Constant) = nothing\n",
    "compute!(node::Variable) = nothing\n",
    "\n",
    "compute!(node::Operator) =\n",
    "    node.output = forward(node, [input.output for input in node.inputs]...)\n",
    "\n",
    "function forward!(order::Vector)\n",
    "    #   Iteruje przez każdy węzeł w order.\n",
    "    for node in order\n",
    "        compute!(node)\n",
    "        reset!(node)\n",
    "    end\n",
    "    return last(order).output\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c495933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "update!(node::Constant, gradient) = nothing\n",
    "\n",
    "update!(node::GraphNode, gradient) = if isnothing(node.gradient)\n",
    "    node.gradient = gradient else node.gradient .+= gradient\n",
    "end\n",
    "\n",
    "\n",
    "function backward!(order::Vector; seed=1.0)\n",
    "    result = last(order)   #   The output node\n",
    "\n",
    "    if isa(result.output, Matrix{Float64})\n",
    "        result.gradient = ones(Float64, size(result.output))\n",
    "    else\n",
    "        result.gradient = seed\n",
    "        @assert length(result.output) == 1 \"Gradient is defined only for scalar functions\"\n",
    "    end\n",
    "\n",
    "    for node in reverse(order)   #   Iterate through nodes in reverse topological order.\n",
    "        backward!(node)   #   Compute and propagate gradients backwards.\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function backward!(node::Constant) end\n",
    "function backward!(node::Variable) end\n",
    "\n",
    "function backward!(node::Operator)\n",
    "    inputs = node.inputs\n",
    "\n",
    "    gradients = backward(node, [input.output for input in inputs]..., node.gradient)\n",
    "\n",
    "    for (input, gradient) in zip(inputs, gradients)\n",
    "        update!(input, gradient)\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52f5b6e",
   "metadata": {},
   "source": [
    "## Funkcja Xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af4c532e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2 Matrix{Float64}:\n",
       "  0.134939   0.115107\n",
       " -0.610532  -0.211263\n",
       " -0.423269  -0.988555"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function xavier_uniform(size::Tuple{Int, Int})\n",
    "    limit = sqrt(6.0 / (size[1] + size[2]))\n",
    "    return rand(Uniform(-limit, limit), size)\n",
    "end\n",
    "\n",
    "function xavier_normal(size::Tuple{Int, Int})\n",
    "    limit = sqrt(2.0 / (size[1] + size[2]))\n",
    "    return rand(Normal(0.0, limit), size)\n",
    "end\n",
    "\n",
    "function xavier_uniform!(w::Matrix{Float64})\n",
    "    fan_out, fan_in = size(w)\n",
    "    limit = sqrt(6.0 / (fan_in + fan_out))\n",
    "    rand!(Uniform(-limit, limit), w)\n",
    "end\n",
    "\n",
    "function xavier_normal!(w::Matrix{Float64})\n",
    "    fan_out, fan_in = size(w)\n",
    "    limit = sqrt(2.0 / (fan_in + fan_out))\n",
    "    rand!(Normal(0.0, limit), w)\n",
    "end\n",
    "\n",
    "xavier_uniform((3, 2))  # Example usage\n",
    "w1 = Variable(xavier_normal((3, 2)); name=\"w1\")\n",
    "\n",
    "w2_mat = zeros(Float64, (3, 2))\n",
    "w2 = Variable(w2_mat; name=\"w2\")\n",
    "xavier_uniform!(w2.output)\n",
    "xavier_normal!(w2.output)\n",
    "w2.output  # Check the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3218f4",
   "metadata": {},
   "source": [
    "##  Funkcje dostępowe do wag, biasów oraz ich gradientów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bfecadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_gradients (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function get_weights(order::Vector)\n",
    "    weights = Vector{Tuple{String, Variable}}()\n",
    "    for node in order\n",
    "        if isa(node, Variable)\n",
    "            if occursin(\"w\", node.name)\n",
    "                push!(weights, (node.name, node))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return weights\n",
    "end\n",
    "\n",
    "function get_biases(order::Vector)\n",
    "    biases = Vector{Tuple{String, Variable}}()\n",
    "    for node in order\n",
    "        if isa(node, Variable)\n",
    "            if occursin(\"b\", node.name)\n",
    "                push!(biases, (node.name, node))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return biases\n",
    "end\n",
    "\n",
    "function get_weights_and_biases(order::Vector)\n",
    "    parameters = Vector{Tuple{String, Variable}}()\n",
    "    for node in order\n",
    "        if isa(node, Variable)\n",
    "            if occursin(\"w\", node.name) || occursin(\"b\", node.name)\n",
    "                push!(parameters, (node.name, node))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return parameters\n",
    "end\n",
    "\n",
    "function get_gradients(order::Vector)\n",
    "    gradients = Vector{Tuple{String, Variable}}()\n",
    "    for node in order\n",
    "        if isa(node, Variable)\n",
    "            if occursin(\"w\", node.name) || occursin(\"b\", node.name)\n",
    "                push!(gradients, (node.name, node))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return gradients\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c977923f",
   "metadata": {},
   "source": [
    "## Optymalizator ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0456313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set_state!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mutable struct Adam\n",
    "    α :: Float64    # learning rate\n",
    "    β1 :: Float64   # First moment decay rate\n",
    "    β2 :: Float64   # Second moment decay rate\n",
    "    ε :: Float64    # Epsilon for numerical stability\n",
    "\n",
    "    #   Stan optymalizatora\n",
    "    m :: Dict{String, Matrix{Float64}}  # First moment estimate\n",
    "    v :: Dict{String, Matrix{Float64}}  # Second moment estimate\n",
    "    t :: Int  # Time step\n",
    "\n",
    "    #   Parametry optymalizatora\n",
    "    parameters :: Vector{Tuple{String, Variable}}   #   Wszystkie parametry (wagi i biasy)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    init!(network_order::Vector; α=0.001, β1=0.9, β2=0.999, ϵ=1e-8)\n",
    "\n",
    "Inicjalizuje optymalizator Adam dla sieci neuronowej.\n",
    "\n",
    "## Argumenty:\n",
    "- `network_order`: wektor węzłów sieci (z funkcji forward)\n",
    "- `α`: współczynnik uczenia (domyślnie 0.001)\n",
    "- `β1`: współczynnik zaniku dla pierwszego momentu (domyślnie 0.9)\n",
    "- `β2`: współczynnik zaniku dla drugiego momentu (domyślnie 0.999)\n",
    "- `ϵ`: epsilon dla stabilności numerycznej (domyślnie 1e-8)\n",
    "\n",
    "## Zwraca:\n",
    "- Zainicjalizowany obiekt optymalizatora Adam\n",
    "\"\"\"\n",
    "\n",
    "function init!(order::Vector{Any}, α=0.001, β1=0.9, β2=0.999, ε=1e-8)\n",
    "    parameters = get_weights_and_biases(order)\n",
    "    \n",
    "    m = Dict{String, Matrix{Float64}}()\n",
    "    v = Dict{String, Matrix{Float64}}()\n",
    "    for (name, var) in parameters\n",
    "        m[name] = zeros(Float64, size(var.output))\n",
    "        v[name] = zeros(Float64, size(var.output))\n",
    "    end\n",
    "    return Adam(α, β1, β2, ε, m, v, 0, parameters)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    step!(optimizer::Adam)\n",
    "\n",
    "Wykonuje krok optymalizacji dla wszystkich parametrów sieci.\n",
    "\n",
    "## Argumenty:\n",
    "- `optimizer`: zainicjalizowany optymalizator Adam\n",
    "\n",
    "## Zwraca:\n",
    "- nic (modyfikuje parametry sieci in-place)\n",
    "\"\"\"\n",
    "\n",
    "function step!(optimizer::Adam)\n",
    "    optimizer.t += 1\n",
    "\n",
    "    for (name, var) in optimizer.parameters\n",
    "        g = var.gradient\n",
    "\n",
    "        #   Aktualizuj momenty\n",
    "        optimizer.m[name] = optimizer.β1 * optimizer.m[name] + (1 - optimizer.β1) * g\n",
    "        optimizer.v[name] = optimizer.β2 * optimizer.v[name] + (1 - optimizer.β2) * (g .^ 2)\n",
    "\n",
    "        #   Popraw momenty\n",
    "        m_corrected = optimizer.m[name] / (1 - optimizer.β1 ^ optimizer.t)\n",
    "        v_corrected = optimizer.v[name] / (1 - optimizer.β2 ^ optimizer.t)\n",
    "\n",
    "        #   Aktualizuj parametry\n",
    "        var.output .-= optimizer.α .* m_corrected ./ (sqrt.(v_corrected) .+ optimizer.ε)\n",
    "    end\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    reset!(optimizer::Adam)\n",
    "\n",
    "Resetuje stan optymalizatora (momenty i licznik czasu).\n",
    "\n",
    "## Argumenty:\n",
    "- `optimizer`: optymalizator Adam do zresetowania\n",
    "\n",
    "## Zwraca:\n",
    "- nic (modyfikuje optymalizator in-place)\n",
    "\"\"\"\n",
    "\n",
    "function reset!(optimizer::Adam)\n",
    "    optimizer.t = 0\n",
    "    #  Reset momentów\n",
    "    for (name, var) in optimizer.parameters\n",
    "        optimizer.m[name] .= zeros(size(var.output))\n",
    "        optimizer.v[name] .= zeros(size(var.output))\n",
    "    end\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    get_state(optimizer::Adam)\n",
    "\n",
    "Zwraca aktualny stan optymalizatora (przydatne do zapisywania checkpointów).\n",
    "\n",
    "## Argumenty:\n",
    "- `optimizer`: optymalizator Adam\n",
    "\n",
    "## Zwraca:\n",
    "- Dict zawierający stan optymializatora (licznik czasu i momenty)\n",
    "\"\"\"\n",
    "function get_state(optimizer::Adam)\n",
    "    return Dict(\n",
    "        \"t\" => optimizer.t,\n",
    "        \"m\" => deepcopy(optimizer.m),\n",
    "        \"v\" => deepcopy(optimizer.v)\n",
    "    )\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    set_state!(optimizer::Adam, state::Dict)\n",
    "\n",
    "Ustawia stan optymalizatora z zapisanego stanu.\n",
    "\n",
    "## Argumenty:\n",
    "- `optimizer`: optymalizator Adam\n",
    "- `state`: stan optymalizatora (z funkcji get_state)\n",
    "\n",
    "## Zwraca:\n",
    "- nic (modyfikuje optymalizator in-place)\n",
    "\"\"\"\n",
    "function set_state!(optimizer::Adam, state::Dict)\n",
    "    optimizer.t = state[\"t\"]\n",
    "    optimizer.m = state[\"m\"]\n",
    "    optimizer.v = state[\"v\"]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3686905b",
   "metadata": {},
   "source": [
    "## Test wejścia do neuronu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c1d7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2 Matrix{Float64}:\n",
       " 1.0  1.0\n",
       " 2.0  2.0\n",
       " 3.0  3.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = Variable(reshape([1.0, 2.0, 3.0, 1.0, 2.0, 3.0], 3, 2), name=\"x\")\n",
    "x.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb994942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3 Matrix{Float64}:\n",
       " 1.0  2.0  3.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = Variable([1.0 2.0 3.0], name=\"w\")\n",
    "w.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2e83b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"z\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = w * x\n",
    "z.name = \"z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6fc88b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topological order:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1-element Vector{Tuple{String, Matrix{Float64}}}:\n",
       " (\"w\", [1.0 2.0 3.0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "order = topological_sort(z)\n",
    "println(\"Topological order:\")\n",
    "order\n",
    "weights = get_weights(order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3fe1ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Tuple{String, Matrix{Float64}}}:\n",
       " (\"w\", [11.0 12.0 13.0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w.output .+= 1.0\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63689cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×2 Matrix{Float64}:\n",
       " 14.0  14.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = forward!(order)\n",
    "z.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6544db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3 Matrix{Float64}:\n",
       " 2.0  4.0  6.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "backward!(order)\n",
    "w.gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7746c8e",
   "metadata": {},
   "source": [
    "## Test 2 szeregowych Neuronów - 1 warstwa + bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ca98db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable([1.0 1.0; 2.0 2.0; 3.0 3.0], name=\"x\")\n",
    "w = Variable([2.0 4.0 6.0; 3.0 5.0 7.0], name=\"w\")\n",
    "y = Constant(reshape([1.0, 1.0], 1, 2))\n",
    "z = w * x\n",
    "z.name = \"z\"\n",
    "# c = Constant(1.0)\n",
    "# d = z + c\n",
    "# dense_layer_2 = σ(z)\n",
    "# dense_layer_2.name = \"σ(z)\"\n",
    "dense_layer_2 = relu(z)\n",
    "dense_layer_2.name = \"relu(z)\"\n",
    "loss = binarycrossentropy(dense_layer_2, y)\n",
    "loss.name = \"binarycrossentropy\"\n",
    "order = topological_sort(loss)\n",
    "y = forward!(order)\n",
    "backward!(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34a9a0",
   "metadata": {},
   "source": [
    "## Test 2. warstw neuronów 2-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff6a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Vector{Any}:\n",
       " var w2\n",
       " ┣━ ^ 4×2 Matrix{Float64}\n",
       " ┗━ ∇ Nothing\n",
       " var w1\n",
       " ┣━ ^ 2×3 Matrix{Float64}\n",
       " ┗━ ∇ Nothing\n",
       " var x\n",
       " ┣━ ^ 3×1 Matrix{Float64}\n",
       " ┗━ ∇ Nothing\n",
       " op.a(typeof(mul!))\n",
       " op.b(typeof(relu))\n",
       " op.c(typeof(mul!))\n",
       " op.d(typeof(relu))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#   Pierwsza warstwa\n",
    "x = Variable(reshape([1.0, 2.0, 3.0], 3, 1), name=\"x\")\n",
    "w = Variable(reshape([2.0, 3.0, 4.0, 5.0, 6.0, 7.0], 2, 3), name=\"w1\")\n",
    "a = w * x\n",
    "a.name = \"a\"\n",
    "b = relu(a)\n",
    "b.name = \"b\"\n",
    "\n",
    "#   Druga warstwa\n",
    "w2 = Variable(reshape([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0], 4,2), name=\"w2\")\n",
    "c = w2 * b\n",
    "c.name = \"c\"\n",
    "d = relu(c)\n",
    "d.name = \"d\"\n",
    "order = topological_sort(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8583b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ = forward!(order)\n",
    "backward!(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4798c5",
   "metadata": {},
   "source": [
    "## Test binary cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce454e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2231435513142097"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ŷ = Variable(reshape([0.8], 1, 1), name=\"ŷ\")\n",
    "y = Variable(reshape([1.0], 1, 1), name=\"y\")\n",
    "loss = binarycrossentropy(ŷ, y)\n",
    "loss.name = \"loss\"\n",
    "order = topological_sort(loss)\n",
    "result = forward!(order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2daa76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward!(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb1e575",
   "metadata": {},
   "source": [
    "##  Test tworzenia modelu dla batch = 2 relu-sigmoid-bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4f5bb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"loss\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = Constant([1.0 1.0; 2.0 1.0; 3.0 1.0])\n",
    "w1 = Variable([0.1 0.2 0.3; 0.4 0.5 0.6], name=\"w1\")\n",
    "z1_mul = w1 * x\n",
    "z1_mul.name = \"z1_mul\"\n",
    "\n",
    "\n",
    "b1_matrix = zeros(Float64, 2, 1)\n",
    "b1_matrix[1,1] = 0.1\n",
    "b1_matrix[2,1] = 0.2\n",
    "b1 = Variable(b1_matrix, name=\"b1\")\n",
    "z1 = z1_mul + b1\n",
    "z1.name = \"z1\"\n",
    "\n",
    "a1 = relu(z1)\n",
    "a1.name = \"a1\"\n",
    "\n",
    "w2_matrix = zeros(Float64, 1, 2)\n",
    "w2_matrix[1,1] = 0.5\n",
    "w2_matrix[1,2] = -0.5\n",
    "w2 = Variable(w2_matrix, name=\"w2\")\n",
    "z2_mul = w2 * a1\n",
    "z2_mul.name = \"z2_mul\"\n",
    "\n",
    "b2_matrix = zeros(Float64, 1, 1)\n",
    "b2_matrix[1,1] = 0.0\n",
    "b2 = Variable(b2_matrix, name=\"b2\")\n",
    "z2 = z2_mul + b2\n",
    "\n",
    "ŷ = σ(z2)\n",
    "ŷ.name = \"ŷ\"\n",
    "\n",
    "y_matrix = zeros(Float64, 1, 2)\n",
    "y_matrix[1,1] = 1.0\n",
    "y_matrix[1,2] = 0.0\n",
    "y = Constant(y_matrix)\n",
    "\n",
    "loss = binarycrossentropy(ŷ, y)\n",
    "loss.name = \"loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c14403ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13-element Vector{Any}:\n",
       " var w2\n",
       " ┣━ ^ 1×2 Matrix{Float64}\n",
       " ┗━ ∇ Nothing\n",
       " var w1\n",
       " ┣━ ^ 2×3 Matrix{Float64}\n",
       " ┗━ ∇ Nothing\n",
       " const [1.0 1.0; 2.0 1.0; 3.0 1.0]\n",
       " op.z1_mul(typeof(mul!))\n",
       " var b1\n",
       " ┣━ ^ 2×1 Matrix{Float64}\n",
       " ┗━ ∇ Nothing\n",
       " op.z1(typeof(+))\n",
       " op.a1(typeof(relu))\n",
       " op.z2_mul(typeof(mul!))\n",
       " var b2\n",
       " ┣━ ^ 1×1 Matrix{Float64}\n",
       " ┗━ ∇ Nothing\n",
       " op.?(typeof(+))\n",
       " op.ŷ(typeof(σ))\n",
       " const [1.0 0.0]\n",
       " op loss(typeof(binary_cross_entropy_loss_impl))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "order = topological_sort(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7aa7dc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8755166955155294"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = forward!(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac6789bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×2 Matrix{Float64}:\n",
       " 0.5  -0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w2.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01fa666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward!(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c193818c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Tuple{String, Variable}}:\n",
       " (\"w2\", var w2\n",
       " ┣━ ^ 1×2 Matrix{Float64}\n",
       " ┗━ ∇ 1×2 Matrix{Float64})\n",
       " (\"w1\", var w1\n",
       " ┣━ ^ 2×3 Matrix{Float64}\n",
       " ┗━ ∇ 2×3 Matrix{Float64})\n",
       " (\"b1\", var b1\n",
       " ┣━ ^ 2×1 Matrix{Float64}\n",
       " ┗━ ∇ 2×1 Matrix{Float64})\n",
       " (\"b2\", var b2\n",
       " ┣━ ^ 1×1 Matrix{Float64}\n",
       " ┗━ ∇ 1×1 Matrix{Float64})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_weights(order)\n",
    "get_biases(order)\n",
    "get_gradients(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bbb5f4",
   "metadata": {},
   "source": [
    "## Iris Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10024aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(features = [5.1 4.9 … 6.2 5.9; 3.5 3.0 … 3.4 3.0; 1.4 1.4 … 5.4 5.1; 0.2 0.2 … 2.3 1.8], targets = InlineStrings.String15[\"Iris-setosa\" \"Iris-setosa\" … \"Iris-virginica\" \"Iris-virginica\"])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using MLDatasets\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using Plots\n",
    "using DataFrames # Dodajmy pakiet DataFrame, bo wygląda na to, że jest używany\n",
    "using MLDataUtils\n",
    "\n",
    "# Załaduj zbiór danych Iris\n",
    "iris_features, iris_targets = Iris(as_df=false)[:]\n",
    "class1_name = \"Iris-setosa\"\n",
    "class2_name = \"Iris-versicolor\"\n",
    "iris_features_cut = iris_features[:, 1:100];\n",
    "iris_targets_cut = iris_targets[:, 1:100];\n",
    "\n",
    "label_mapping = Dict(\"Iris-setosa\" => 0.0, \"Iris-versicolor\" => 1.0)\n",
    "iris_targets_cut_classes = [label_mapping[class_name] for class_name in iris_targets_cut]\n",
    "iris_shuffled_x, iris_shuffled_y = shuffleobs((iris_features_cut, iris_targets_cut_classes));\n",
    "batch_size = 10\n",
    "num_observations = size(iris_shuffled_x, 2)\n",
    "\n",
    "batches = []\n",
    "\n",
    "for i = 1:batch_size:num_observations\n",
    "    end_index = min(i+batch_size-1, num_observations)\n",
    "    features_batch = iris_shuffled_x[:, i:end_index]\n",
    "    targets_batch = iris_shuffled_y[i:end_index]\n",
    "    targets_batch = reshape(targets_batch, 1, length(targets_batch))\n",
    "    push!(batches, (features_batch, targets_batch))\n",
    "end\n",
    "\n",
    "iris_train = batches[1:8]\n",
    "iris_test = batches[9:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcdbc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9-element Vector{Any}:\n",
       " var w2\n",
       " ┣━ ^ 1×8 Matrix{Float64}\n",
       " ┗━ ∇ Nothing\n",
       " var w1\n",
       " ┣━ ^ 8×4 Matrix{Float64}\n",
       " ┗━ ∇ Nothing\n",
       " const [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " op.z1_mul(typeof(mul!))\n",
       " op.d1(typeof(relu))\n",
       " op.z2_mul(typeof(mul!))\n",
       " op.ŷ(typeof(σ))\n",
       " const [0.0 0.0 … 0.0 0.0]\n",
       " op loss(typeof(binary_cross_entropy_loss_impl))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#   Ustawienia sieci neuronowej\n",
    "features = 4\n",
    "hidden = 8\n",
    "out = 1\n",
    "epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "x = Constant(zeros(Float64, features, batch_size))\n",
    "w1 = Variable(xavier_uniform((hidden, features)); name=\"w1\")\n",
    "z1_mul = w1 * x\n",
    "z1_mul.name = \"z1_mul\"\n",
    "d1 = relu(z1_mul)\n",
    "d1.name = \"d1\"\n",
    "w2 = Variable(xavier_uniform((out, hidden)); name=\"w2\")\n",
    "z2_mul = w2 * d1\n",
    "z2_mul.name = \"z2_mul\"\n",
    "ŷ = σ(z2_mul)\n",
    "ŷ.name = \"ŷ\"\n",
    "y = Constant(zeros(Float64, out, batch_size))\n",
    "loss = binarycrossentropy(ŷ, y)\n",
    "loss.name = \"loss\"\n",
    "order = topological_sort(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8ed70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state of optimizer:\n",
      "Dict{String, Any}(\"v\" => Dict(\"w2\" => [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0], \"w1\" => [0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0]), \"m\" => Dict(\"w2\" => [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0], \"w1\" => [0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0]), \"t\" => 0)\n"
     ]
    }
   ],
   "source": [
    "#   Start ADAM   \n",
    "optimizer = init!(order)\n",
    "println(\"Initial state of optimizer:\")\n",
    "println(get_state(optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "456ef0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "ename": "ErrorException",
     "evalue": "setfield!: immutable struct of type Adam cannot be changed",
     "output_type": "error",
     "traceback": [
      "setfield!: immutable struct of type Adam cannot be changed\n",
      "\n",
      "Stacktrace:\n",
      " [1] setproperty!(x::Adam, f::Symbol, v::Int64)\n",
      "   @ Base ./Base.jl:53\n",
      " [2] step!(optimizer::Adam)\n",
      "   @ Main ~/Repos/AWiD/MyMlp/src/notebooks/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X16sZmlsZQ==.jl:57\n",
      " [3] top-level scope\n",
      "   @ ~/Repos/AWiD/MyMlp/src/notebooks/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X62sZmlsZQ==.jl:10"
     ]
    }
   ],
   "source": [
    "#   Run training and visualize results\n",
    "for epoch in 1:epochs\n",
    "    println(\"Epoch $epoch\")\n",
    "    for (features_batch, targets_batch) in iris_train\n",
    "        x.output = features_batch\n",
    "        y.output = targets_batch\n",
    "        forward!(order)\n",
    "        loss_value = loss.output\n",
    "        backward!(order)\n",
    "        step!(optimizer)\n",
    "    end\n",
    "    println(\"Loss after epoch $epoch: \", loss_value)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.0",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
