{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f1ba62",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3221369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Repos/AWiD/MyMlp`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[92m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "   6144.9 ms\u001b[32m  ✓ \u001b[39mMyMlp\n",
      "  1 dependency successfully precompiled in 8 seconds. 337 already precompiled.\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for UUIDs\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "  \u001b[90m[cf7118a7] \u001b[39m\u001b[92m+ UUIDs v1.11.0\u001b[39m\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mProject\u001b[22m\u001b[39m MyMlp v0.1.0\n",
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "  \u001b[90m[6e4b80f9] \u001b[39mBenchmarkTools v1.6.0\n",
      "  \u001b[90m[a93c6f00] \u001b[39mDataFrames v1.7.0\n",
      "  \u001b[90m[31c24e10] \u001b[39mDistributions v0.25.120\n",
      "  \u001b[90m[033835bb] \u001b[39mJLD2 v0.5.13\n",
      "  \u001b[90m[cc2ba9b6] \u001b[39mMLDataUtils v0.5.4\n",
      "  \u001b[90m[eb30cadb] \u001b[39mMLDatasets v0.7.18\n",
      "  \u001b[90m[91a5bcdd] \u001b[39mPlots v1.40.13\n",
      "  \u001b[90m[10745b16] \u001b[39mStatistics v1.11.1\n",
      "  \u001b[90m[37e2e46d] \u001b[39mLinearAlgebra v1.11.0\n",
      "  \u001b[90m[9a3f8284] \u001b[39mRandom v1.11.0\n",
      "  \u001b[90m[cf7118a7] \u001b[39mUUIDs v1.11.0\n"
     ]
    }
   ],
   "source": [
    "# Make sure we're using the latest version\n",
    "import Pkg\n",
    "Pkg.activate(\"../..\")  # Activate the project environment\n",
    "\n",
    "Pkg.add(\"BenchmarkTools\")\n",
    "Pkg.add(\"LinearAlgebra\")\n",
    "Pkg.add(\"Statistics\")\n",
    "Pkg.add(\"Distributions\")\n",
    "Pkg.add(\"Random\")\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"MLDatasets\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"MLDataUtils\")\n",
    "Pkg.add(\"JLD2\")\n",
    "Pkg.add(\"UUIDs\")\n",
    "Pkg.instantiate()      # Install any missing dependencies\n",
    "Pkg.status()          # Check if MyMlp is listed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bfe5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try importing\n",
    "using BenchmarkTools\n",
    "using LinearAlgebra\n",
    "using Distributions\n",
    "using Random\n",
    "using MLDatasets\n",
    "using Plots\n",
    "using Statistics\n",
    "using DataFrames\n",
    "using JLD2\n",
    "using UUIDs\n",
    "using Printf\n",
    "using MLDataUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb89e6c",
   "metadata": {},
   "source": [
    "## Comparison of different optimizations to Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc8f4feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Benchmark: READ access ###\n",
      "Original:\n",
      "  14.034 ns (1 allocation: 16 bytes)\n",
      "Optimized:\n",
      "  1.575 ns (0 allocations: 0 bytes)\n",
      "RefValue:\n",
      "  1.793 ns (0 allocations: 0 bytes)\n",
      "\n",
      "### Benchmark: WRITE mutation ###\n",
      "Original:\n",
      "  14.417 ns (1 allocation: 16 bytes)\n",
      "Optimized:\n",
      "  2.013 ns (0 allocations: 0 bytes)\n",
      "RefValue:\n",
      "  2.013 ns (0 allocations: 0 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500503.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"BenchmarkTools\")\n",
    "using BenchmarkTools\n",
    "\n",
    "abstract type GraphNode end\n",
    "abstract type Operator <: GraphNode end\n",
    "\n",
    "# Original implementation\n",
    "mutable struct VariableOriginal <: GraphNode\n",
    "    output :: Any\n",
    "    grad :: Any\n",
    "    name :: String\n",
    "    VariableOriginal(output; name=\"?\") = new(output, nothing, name)\n",
    "end\n",
    "\n",
    "# Optimized implementation\n",
    "mutable struct VariableOptimized{T<:Float64} <: GraphNode\n",
    "    output :: T\n",
    "    grad :: Union{Nothing, T}\n",
    "    name :: String\n",
    "    VariableOptimized(output::T; name=\"?\") where T<:Float64 = new{T}(output, nothing, name)\n",
    "end\n",
    "\n",
    "# RefValue-based immutable implementation\n",
    "struct VariableRef <: GraphNode\n",
    "    output :: Base.RefValue{Float64}\n",
    "    grad   :: Union{Nothing, Base.RefValue{Float64}}\n",
    "    name   :: String\n",
    "    function VariableRef(output::Float64; name=\"?\")\n",
    "        new(Base.RefValue(output), nothing, name)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Functions to benchmark: reading\n",
    "function read_output(v::VariableOriginal)\n",
    "    v.output + 1.0\n",
    "end\n",
    "\n",
    "function read_output(v::VariableOptimized)\n",
    "    v.output + 1.0\n",
    "end\n",
    "\n",
    "function read_output(v::VariableRef)\n",
    "    v.output[] + 1.0\n",
    "end\n",
    "\n",
    "# Functions to benchmark: writing\n",
    "function write_output!(v::VariableOriginal)\n",
    "    v.output = v.output + 1.0\n",
    "end\n",
    "\n",
    "function write_output!(v::VariableOptimized)\n",
    "    v.output = v.output + 1.0\n",
    "end\n",
    "\n",
    "function write_output!(v::VariableRef)\n",
    "    v.output[] = v.output[] + 1.0\n",
    "end\n",
    "\n",
    "# Create instances\n",
    "v1 = VariableOriginal(1.0)\n",
    "v2 = VariableOptimized(1.0)\n",
    "v3 = VariableRef(1.0)\n",
    "\n",
    "# Benchmark READ\n",
    "println(\"### Benchmark: READ access ###\")\n",
    "println(\"Original:\")\n",
    "@btime read_output($v1)\n",
    "\n",
    "println(\"Optimized:\")\n",
    "@btime read_output($v2)\n",
    "\n",
    "println(\"RefValue:\")\n",
    "@btime read_output($v3)\n",
    "\n",
    "# Benchmark WRITE\n",
    "println(\"\\n### Benchmark: WRITE mutation ###\")\n",
    "println(\"Original:\")\n",
    "@btime write_output!($v1)\n",
    "\n",
    "println(\"Optimized:\")\n",
    "@btime write_output!($v2)\n",
    "\n",
    "println(\"RefValue:\")\n",
    "@btime write_output!($v3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4694cbbe",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c212c68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 5 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Base: *, +, clamp, log, exp\n",
    "import LinearAlgebra: mul!\n",
    "import Statistics: sum\n",
    "\n",
    "abstract type GraphNode end\n",
    "abstract type Operator <: GraphNode end\n",
    "\n",
    "# Definition of basic structures for computational graph\n",
    "mutable struct Constant{T<:Matrix{Float32}} <: GraphNode\n",
    "    output :: T\n",
    "end\n",
    "\n",
    "mutable struct Variable{T<:Matrix{Float32}} <: GraphNode\n",
    "    output :: T\n",
    "    gradient :: T\n",
    "    name :: String\n",
    "    \n",
    "    Variable(output::T; name=\"?\") where {T<:Matrix{Float32}} = new{T}(output, zeros(Float32, size(output)), name)\n",
    "end\n",
    "\n",
    "mutable struct ScalarOperator{F} <: Operator\n",
    "    inputs :: Tuple{GraphNode, GraphNode}\n",
    "    output :: Float32\n",
    "    gradient :: Float32\n",
    "    name :: String\n",
    "    ScalarOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, 0.0f0, 0.0f0, name)\n",
    "end\n",
    "\n",
    "mutable struct BroadcastedOperator{F} <: Operator\n",
    "    inputs :: Union{Tuple{GraphNode, GraphNode}, Tuple{GraphNode}}\n",
    "    output :: Matrix{Float32}\n",
    "    gradient :: Matrix{Float32}\n",
    "    name :: String\n",
    "    BroadcastedOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, zeros(Float32, 1, 1), zeros(Float32, 1, 1), name)\n",
    "end\n",
    "\n",
    "\n",
    "import Base: show, summary\n",
    "show(io::IO, x::ScalarOperator{F}) where {F} = print(io, \"op \", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::BroadcastedOperator{F}) where {F} = print(io, \"op.\", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::Constant) = print(io, \"const \", x.output)\n",
    "show(io::IO, x::Variable) = begin\n",
    "    print(io, \"var \", x.name);\n",
    "    print(io, \"\\n ┣━ ^ \"); summary(io, x.output)\n",
    "    print(io, \"\\n ┗━ ∇ \");  summary(io, x.gradient)\n",
    "end\n",
    "\n",
    "\n",
    "function visit(node::GraphNode, visited, order)\n",
    "    if node ∈ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return zeros(Float32, 1, 1)\n",
    "end\n",
    "\n",
    "function visit(node::Operator, visited, order)\n",
    "    if node ∈ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        for input in node.inputs\n",
    "            visit(input, visited, order)\n",
    "        end\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return zeros(Float32, 1, 1)\n",
    "end\n",
    "\n",
    "function topological_sort(head::GraphNode)\n",
    "    visited = Set()\n",
    "    order = Vector()\n",
    "    visit(head, visited, order)\n",
    "    return order\n",
    "end\n",
    "\n",
    "\n",
    "# x * y (aka matrix multiplication)\n",
    "*(A::GraphNode, x::GraphNode; name=\"mul\") = BroadcastedOperator(mul!, A, x, name=name)\n",
    "forward(::BroadcastedOperator{typeof(mul!)}, A, x) = return A * x\n",
    "backward(::BroadcastedOperator{typeof(mul!)}, A, x, g) = tuple(g * x', A' * g)\n",
    "\n",
    "# relu activation\n",
    "relu(x::GraphNode; name=\"relu\") = BroadcastedOperator(relu, x, name=name)\n",
    "forward(::BroadcastedOperator{typeof(relu)}, x) = return x .* (x .> 0.0f0)\n",
    "backward(::BroadcastedOperator{typeof(relu)}, x, g) = tuple(g .* (x .> 0.0f0), zeros(Float32, 1, 1))\n",
    "\n",
    "# add operation (for bias)\n",
    "+(x::GraphNode, y::GraphNode; name=\"sum\") = BroadcastedOperator(+, x, y, name=name)\n",
    "forward(::BroadcastedOperator{typeof(+)}, x, y) = return x .+ y\n",
    "backward(::BroadcastedOperator{typeof(+)}, x, y, g) = begin\n",
    "    grad_wrt_x = g\n",
    "    grad_wrt_y = sum(g, dims=2)\n",
    "    return (grad_wrt_x, grad_wrt_y)\n",
    "end\n",
    "\n",
    "# sigmoid activation\n",
    "σ(x::GraphNode; name=\"sigmoid\") = BroadcastedOperator(σ, x, name=name)\n",
    "forward(::BroadcastedOperator{typeof(σ)}, x) = return 1.0f0 ./ (1.0f0 .+ exp.(-x))\n",
    "backward(node::BroadcastedOperator{typeof(σ)}, x, g) = begin\n",
    "    y = node.output\n",
    "    local_derivative = y .* (1.0f0 .- y)\n",
    "    grad_wrt_x = g .* local_derivative\n",
    "    return (grad_wrt_x, zeros(Float32, 1, 1))\n",
    "end\n",
    "\n",
    "function binary_cross_entropy_loss_impl(ŷ, y_true; epsilon=1e-10)\n",
    "    ŷ_clamped = clamp.(ŷ, epsilon, 1.0f0 - epsilon)\n",
    "    loss_elements = -y_true .* log.(ŷ_clamped) .- (1.0f0 .- y_true) .* log.(1.0f0 .- ŷ_clamped)\n",
    "    return mean(loss_elements)\n",
    "end\n",
    "\n",
    "binarycrossentropy(ŷ::GraphNode, y::GraphNode; name=\"bce_loss\") = ScalarOperator(binary_cross_entropy_loss_impl, ŷ, y, name=name)\n",
    "\n",
    "forward(::ScalarOperator{typeof(binary_cross_entropy_loss_impl)}, ŷ_value, y_value) = begin\n",
    "    loss_value = binary_cross_entropy_loss_impl(ŷ_value, y_value)\n",
    "    return loss_value\n",
    "end\n",
    "\n",
    "backward(::ScalarOperator{typeof(binary_cross_entropy_loss_impl)}, ŷ_value, y_value, g) = begin\n",
    "    epsilon = 1e-10\n",
    "    ŷ_clamped_for_grad = clamp.(ŷ_value, epsilon, 1.0f0 - epsilon)\n",
    "    local_grad_per_sample = (ŷ_clamped_for_grad .- y_value) ./ (ŷ_clamped_for_grad .* (1.0f0 .- ŷ_clamped_for_grad))\n",
    "    batch_size = size(y_value, 2)\n",
    "    grad_wrt_ŷ = local_grad_per_sample ./ batch_size\n",
    "    return (grad_wrt_ŷ, zeros(Float32, 1, 1))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f27fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reset!(node::Constant) = nothing\n",
    "reset!(node::Variable) = node.gradient = zeros(Float32, size(node.output))\n",
    "\n",
    "function reset!(node::Operator)\n",
    "    if isa(node.output, Matrix{Float32})\n",
    "        node.gradient = zeros(Float32, size(node.output))\n",
    "    else\n",
    "        node.gradient = 0.0f0\n",
    "    end\n",
    "end\n",
    "#reset!(node::Operator) = node.gradient = zeros(Float32, size(node.output))\n",
    "\n",
    "compute!(node::Constant) = nothing\n",
    "compute!(node::Variable) = nothing\n",
    "\n",
    "function compute!(node::Operator)\n",
    "    node.output = forward(node, [input.output for input in node.inputs]...)\n",
    "    if isa(node.output, Matrix{Float32})\n",
    "        node.gradient = zeros(Float32, size(node.output))\n",
    "    end\n",
    "end\n",
    "# compute!(node::Operator) =\n",
    "#     node.output = forward(node, [input.output for input in node.inputs]...)\n",
    "\n",
    "function forward!(order::Vector)\n",
    "    #   Iteruje przez każdy węzeł w order.\n",
    "    for node in order\n",
    "        compute!(node)\n",
    "        reset!(node)\n",
    "    end\n",
    "    return last(order).output\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c495933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "update!(node::Constant, gradient) = nothing\n",
    "\n",
    "update!(node::GraphNode, gradient) = if isnothing(node.gradient)\n",
    "    node.gradient = gradient else node.gradient .+= gradient\n",
    "end\n",
    "\n",
    "function backward!(order::Vector; seed=1.0)\n",
    "    result = last(order)   #   The output node\n",
    "    if all(iszero, result.gradient)\n",
    "        if isa(result.output, Matrix{Float32})\n",
    "            result.gradient = ones(Float32, size(result.output))\n",
    "        else\n",
    "            result.gradient = seed\n",
    "            @assert length(result.output) == 1 \"Gradient is defined only for scalar functions\"\n",
    "        end\n",
    "    end\n",
    "\n",
    "    for node in reverse(order)   #   Iterate through nodes in reverse topological order.\n",
    "        backward!(node)   #   Compute and propagate gradients backwards.\n",
    "    end\n",
    "    return zeros(Float32, 1, 1)\n",
    "end\n",
    "\n",
    "function backward!(node::Constant) end\n",
    "function backward!(node::Variable) end\n",
    "\n",
    "function backward!(node::Operator)\n",
    "    inputs = node.inputs\n",
    "\n",
    "    gradients = backward(node, [input.output for input in inputs]..., node.gradient)\n",
    "\n",
    "    for (input, gradient) in zip(inputs, gradients)\n",
    "        update!(input, gradient)\n",
    "    end\n",
    "    return zeros(Float32, 1, 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52f5b6e",
   "metadata": {},
   "source": [
    "## Funkcja Xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af4c532e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xavier_normal! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function xavier_uniform(size::Tuple{Int, Int})\n",
    "    limit = sqrt(6.0f0 / (size[1] + size[2]))\n",
    "    return Float32.(rand(Uniform(-limit, limit), size))\n",
    "end\n",
    "\n",
    "function xavier_normal(size::Tuple{Int, Int})\n",
    "    limit = sqrt(2.0f0 / (size[1] + size[2]))\n",
    "    return Float32.(rand(Normal(0.0f0, limit), size))\n",
    "end\n",
    "\n",
    "function xavier_uniform!(w::Matrix{Float32})\n",
    "    fan_out, fan_in = size(w)\n",
    "    limit = sqrt(6.0f0 / (fan_in + fan_out))\n",
    "    Float32.(rand!(Uniform(-limit, limit), w))\n",
    "end\n",
    "\n",
    "function xavier_normal!(w::Matrix{Float32})\n",
    "    fan_out, fan_in = size(w)\n",
    "    limit = sqrt(2.0f0 / (fan_in + fan_out))\n",
    "    Float32.(rand!(Normal(0.0f0, limit), w))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3218f4",
   "metadata": {},
   "source": [
    "##  Funkcje dostępowe do wag, biasów oraz ich gradientów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bfecadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_gradients (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function get_weights(order::Vector)\n",
    "    weights = Vector{Tuple{String, Variable}}()\n",
    "    for node in order\n",
    "        if isa(node, Variable)\n",
    "            if occursin(\"w\", node.name)\n",
    "                push!(weights, (node.name, node))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return weights\n",
    "end\n",
    "\n",
    "function get_biases(order::Vector)\n",
    "    biases = Vector{Tuple{String, Variable}}()\n",
    "    for node in order\n",
    "        if isa(node, Variable)\n",
    "            if occursin(\"b\", node.name)\n",
    "                push!(biases, (node.name, node))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return biases\n",
    "end\n",
    "\n",
    "function get_weights_and_biases(order::Vector)\n",
    "    parameters = Vector{Tuple{String, Variable}}()\n",
    "    for node in order\n",
    "        if isa(node, Variable)\n",
    "            if occursin(\"w\", node.name) || occursin(\"b\", node.name)\n",
    "                push!(parameters, (node.name, node))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return parameters\n",
    "end\n",
    "\n",
    "function get_gradients(order::Vector)\n",
    "    gradients = Vector{Tuple{String, Variable}}()\n",
    "    for node in order\n",
    "        if isa(node, Variable)\n",
    "            if occursin(\"w\", node.name) || occursin(\"b\", node.name)\n",
    "                push!(gradients, (node.name, node))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return gradients\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c977923f",
   "metadata": {},
   "source": [
    "## Optymalizator ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0456313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reset! (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "abstract type AbstractOptimizer end\n",
    "\n",
    "struct Adam <: AbstractOptimizer\n",
    "    α :: Float32    # learning rate\n",
    "    β1 :: Float32   # First moment decay rate\n",
    "    β2 :: Float32   # Second moment decay rate\n",
    "    ε :: Float32    # Epsilon for numerical stability\n",
    "end\n",
    "\n",
    "Adam() = Adam(0.001f0, 0.9f0, 0.999f0, 1e-8)\n",
    "\n",
    "mutable struct AdamState\n",
    "    hyperparams :: Adam # Przechowuje konfigurację optymalizatora\n",
    "    m :: Dict{String, Matrix{Float32}}\n",
    "    v :: Dict{String, Matrix{Float32}}\n",
    "    t :: Int\n",
    "    parameters :: Vector{Tuple{String, Variable}}\n",
    "end\n",
    "\n",
    "function setup_optimizer(optimizer_config::AbstractOptimizer, model::Chain)\n",
    "    trainable_vars = collect_model_parameters(model)\n",
    "    m = Dict{String, Matrix{Float32}}()\n",
    "    v = Dict{String, Matrix{Float32}}()\n",
    "    for (name, var) in trainable_vars\n",
    "        m[name] = zeros(Float32, size(var.output))\n",
    "        v[name] = zeros(Float32, size(var.output))\n",
    "    end\n",
    "    return AdamState(optimizer_config, m, v, 0, trainable_vars)\n",
    "end\n",
    "\n",
    "function collect_model_parameters(model::Chain)\n",
    "    all_params = Vector{Tuple{String, Variable}}()\n",
    "    for layer in model.layers\n",
    "        append!(all_params, collect_model_parameters(layer))\n",
    "    end\n",
    "    return all_params\n",
    "end\n",
    "\n",
    "function collect_model_parameters(layer::Dense)\n",
    "    return [(layer.W.name, layer.W), (layer.b.name, layer.b)]\n",
    "end\n",
    "\n",
    "function step!(optimizer_state::AdamState)\n",
    "    optimizer_state.t += 1\n",
    "\n",
    "    config = optimizer_state.hyperparams # Dostęp do hyperparametrów z konfiguracji\n",
    "\n",
    "    for (name, var) in optimizer_state.parameters\n",
    "        g = var.gradient\n",
    "\n",
    "        optimizer_state.m[name] = config.β1 * optimizer_state.m[name] + (1 - config.β1) * g\n",
    "        optimizer_state.v[name] = config.β2 * optimizer_state.v[name] + (1 - config.β2) * (g .^ 2)\n",
    "\n",
    "        m_corrected = optimizer_state.m[name] / (1 - config.β1 ^ optimizer_state.t)\n",
    "        v_corrected = optimizer_state.v[name] / (1 - config.β2 ^ optimizer_state.t)\n",
    "\n",
    "        var.output .-= config.α .* m_corrected ./ (sqrt.(v_corrected) .+ config.ε)\n",
    "    end\n",
    "end\n",
    "\n",
    "function reset!(optimizer_state::AdamState)\n",
    "    optimizer_state.t = 0\n",
    "    #  Reset momentów\n",
    "    for (name, var) in optimizer_state.parameters\n",
    "        optimizer_state.m[name] .= zeros(size(var.output))\n",
    "        optimizer_state.v[name] .= zeros(size(var.output))\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1e541e",
   "metadata": {},
   "source": [
    "## Higher level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cce85c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "build_graph! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "abstract type Layer end\n",
    "\n",
    "mutable struct Dense <: Layer\n",
    "    W::Variable\n",
    "    b::Variable\n",
    "    activation\n",
    "    name::String\n",
    "end\n",
    "\n",
    "function Dense(in_features::Int, out_features::Int, activation=identity; \n",
    "    weight_init = xavier_uniform,\n",
    "    bias_init = (dims) -> zeros(Float32, dims),\n",
    "    name=\"dense\")\n",
    "\n",
    "    W = Variable(weight_init((out_features, in_features)); name=\"$(name)_w\")\n",
    "\n",
    "    b = Variable(bias_init((out_features, 1)); name=\"$(name)_b\")\n",
    "\n",
    "    return Dense(W, b, activation, name)\n",
    "end\n",
    "\n",
    "function (d::Dense)(x::GraphNode)\n",
    "\n",
    "    multiplication_code = *(d.W, x, name=\"$(d.name)_mul\")\n",
    "    #   Dodanie biasu\n",
    "    linear_output = +(multiplication_code, d.b, name=\"$(d.name)_add\")\n",
    "    # Przekazanie nazwy operatorowi aktywacji\n",
    "    if d.activation == relu\n",
    "        return relu(linear_output, name=\"$(d.name)_relu\")\n",
    "    elseif d.activation == σ\n",
    "        return σ(linear_output, name=\"$(d.name)_sigmoid\")\n",
    "    else\n",
    "        #   Użyj domyślnej nazwy\n",
    "        try\n",
    "             return d.activation(linear_output, name=\"$(d.name)_$(string(nameof(d.activation)))\")\n",
    "        catch\n",
    "             return d.activation(linear_output)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "mutable struct Chain\n",
    "    layers::Vector{<:Layer}\n",
    "end\n",
    "\n",
    "Chain(layers...) = Chain([layers...])\n",
    "\n",
    "function (c::Chain)(x::GraphNode)\n",
    "    input = x\n",
    "    for layer in c.layers\n",
    "        input = layer(input)\n",
    "    end\n",
    "    return input\n",
    "end\n",
    "\n",
    "function build_graph!(model::Chain, loss_fn, input_node::GraphNode, label_node::GraphNode; loss_name=\"loss\")\n",
    "\n",
    "    model_output_node = model(input_node)\n",
    "    loss_node = loss_fn(model_output_node, label_node; name=loss_name)\n",
    "\n",
    "    if hasproperty(loss_node, :name)\n",
    "        loss_node.name = loss_name\n",
    "    end\n",
    "\n",
    "    order = topological_sort(loss_node)\n",
    "\n",
    "    return (loss_node, model_output_node, order)\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3686905b",
   "metadata": {},
   "source": [
    "## Test wejścia do neuronu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c1d7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2 Matrix{Float32}:\n",
       " 1.0  1.0\n",
       " 2.0  2.0\n",
       " 3.0  3.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = Variable(Float32.(reshape([1.0, 2.0, 3.0, 1.0, 2.0, 3.0], 3, 2)), name=\"x\")\n",
    "x.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb994942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3 Matrix{Float32}:\n",
       " 1.0  2.0  3.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = Variable(Float32.([1.0 2.0 3.0]), name=\"w\")\n",
    "w.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2e83b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"z\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = w * x\n",
    "z.name = \"z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6fc88b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topological order:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1-element Vector{Tuple{String, Variable}}:\n",
       " (\"w\", var w\n",
       " ┣━ ^ 1×3 Matrix{Float32}\n",
       " ┗━ ∇ 1×3 Matrix{Float32})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "order = topological_sort(z)\n",
    "println(\"Topological order:\")\n",
    "order\n",
    "weights = get_weights(order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63689cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×2 Matrix{Float32}:\n",
       " 14.0  14.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = forward!(order)\n",
    "z.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6544db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×1 Matrix{Float32}:\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "backward!(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7746c8e",
   "metadata": {},
   "source": [
    "## Test 2 szeregowych Neuronów - 1 warstwa + bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ca98db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×1 Matrix{Float32}:\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = Variable(Float32.([1.0 1.0; 2.0 2.0; 3.0 3.0]), name=\"x\")\n",
    "w = Variable(Float32.([2.0 4.0 6.0; 3.0 5.0 7.0]), name=\"w\")\n",
    "y = Constant(Float32.(reshape([1.0, 1.0], 1, 2)))\n",
    "z = w * x\n",
    "z.name = \"z\"\n",
    "# c = Constant(1.0)\n",
    "# d = z + c\n",
    "# dense_layer_2 = σ(z)\n",
    "# dense_layer_2.name = \"σ(z)\"\n",
    "dense_layer_2 = relu(z)\n",
    "dense_layer_2.name = \"relu(z)\"\n",
    "loss = binarycrossentropy(dense_layer_2, y)\n",
    "loss.name = \"binarycrossentropy\"\n",
    "order = topological_sort(loss)\n",
    "y = forward!(order)\n",
    "backward!(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34a9a0",
   "metadata": {},
   "source": [
    "## Test 2. warstw neuronów 2-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03ff6a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Vector{Any}:\n",
       " var w2\n",
       " ┣━ ^ 4×2 Matrix{Float32}\n",
       " ┗━ ∇ 4×2 Matrix{Float32}\n",
       " var w1\n",
       " ┣━ ^ 2×3 Matrix{Float32}\n",
       " ┗━ ∇ 2×3 Matrix{Float32}\n",
       " var x\n",
       " ┣━ ^ 3×1 Matrix{Float32}\n",
       " ┗━ ∇ 3×1 Matrix{Float32}\n",
       " op.a(typeof(mul!))\n",
       " op.b(typeof(relu))\n",
       " op.c(typeof(mul!))\n",
       " op.d(typeof(relu))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#   Pierwsza warstwa\n",
    "x = Variable(Float32.(reshape([1.0, 2.0, 3.0], 3, 1)), name=\"x\")\n",
    "w = Variable(Float32.(reshape([2.0, 3.0, 4.0, 5.0, 6.0, 7.0], 2, 3)), name=\"w1\")\n",
    "a = w * x\n",
    "a.name = \"a\"\n",
    "b = relu(a)\n",
    "b.name = \"b\"\n",
    "\n",
    "#   Druga warstwa\n",
    "w2 = Variable(Float32.(reshape([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0], 4,2)), name=\"w2\")\n",
    "c = w2 * b\n",
    "c.name = \"c\"\n",
    "d = relu(c)\n",
    "d.name = \"d\"\n",
    "order = topological_sort(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8583b5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×1 Matrix{Float32}:\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ŷ = forward!(order)\n",
    "backward!(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4798c5",
   "metadata": {},
   "source": [
    "## Test binary cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce454e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22314353f0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ŷ = Variable(Float32.(reshape([0.8], 1, 1)), name=\"ŷ\")\n",
    "y = Variable(Float32.(reshape([1.0], 1, 1)), name=\"y\")\n",
    "loss = binarycrossentropy(ŷ, y)\n",
    "loss.name = \"loss\"\n",
    "order = topological_sort(loss)\n",
    "result = forward!(order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2daa76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×1 Matrix{Float32}:\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "backward!(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb1e575",
   "metadata": {},
   "source": [
    "##  Test tworzenia modelu dla batch = 2 relu-sigmoid-bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4f5bb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"loss\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = Constant(Float32.([1.0 1.0; 2.0 1.0; 3.0 1.0]))\n",
    "w1 = Variable(Float32.([0.1 0.2 0.3; 0.4 0.5 0.6]), name=\"w1\")\n",
    "z1_mul = w1 * x\n",
    "z1_mul.name = \"z1_mul\"\n",
    "\n",
    "\n",
    "b1_matrix = zeros(Float32, 2, 1)\n",
    "b1_matrix[1,1] = 0.1f0\n",
    "b1_matrix[2,1] = 0.2f0\n",
    "b1 = Variable(b1_matrix, name=\"b1\")\n",
    "z1 = z1_mul + b1\n",
    "z1.name = \"z1\"\n",
    "\n",
    "a1 = relu(z1)\n",
    "a1.name = \"a1\"\n",
    "\n",
    "w2_matrix = zeros(Float32, 1, 2)\n",
    "w2_matrix[1,1] = 0.5f0\n",
    "w2_matrix[1,2] = -0.5f0\n",
    "w2 = Variable(w2_matrix, name=\"w2\")\n",
    "z2_mul = w2 * a1\n",
    "z2_mul.name = \"z2_mul\"\n",
    "\n",
    "b2_matrix = zeros(Float32, 1, 1)\n",
    "b2_matrix[1,1] = 0.0f0\n",
    "b2 = Variable(b2_matrix, name=\"b2\")\n",
    "z2 = z2_mul + b2\n",
    "\n",
    "ŷ = σ(z2)\n",
    "ŷ.name = \"ŷ\"\n",
    "\n",
    "y_matrix = zeros(Float32, 1, 2)\n",
    "y_matrix[1,1] = 1.0f0\n",
    "y_matrix[1,2] = 0.0f0\n",
    "y = Constant(y_matrix)\n",
    "\n",
    "loss = binarycrossentropy(ŷ, y)\n",
    "loss.name = \"loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c14403ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13-element Vector{Any}:\n",
       " var w2\n",
       " ┣━ ^ 1×2 Matrix{Float32}\n",
       " ┗━ ∇ Nothing\n",
       " var w1\n",
       " ┣━ ^ 2×3 Matrix{Float32}\n",
       " ┗━ ∇ Nothing\n",
       " const Float32[1.0 1.0; 2.0 1.0; 3.0 1.0]\n",
       " op.z1_mul(typeof(mul!))\n",
       " var b1\n",
       " ┣━ ^ 2×1 Matrix{Float32}\n",
       " ┗━ ∇ Nothing\n",
       " op.z1(typeof(+))\n",
       " op.a1(typeof(relu))\n",
       " op.z2_mul(typeof(mul!))\n",
       " var b2\n",
       " ┣━ ^ 1×1 Matrix{Float32}\n",
       " ┗━ ∇ Nothing\n",
       " op.?(typeof(+))\n",
       " op.ŷ(typeof(σ))\n",
       " const Float32[1.0 0.0]\n",
       " op loss(typeof(binary_cross_entropy_loss_impl))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "order = topological_sort(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7aa7dc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8755167f0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = forward!(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac6789bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×2 Matrix{Float32}:\n",
       " 0.5  -0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w2.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01fa666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward!(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c193818c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Tuple{String, Variable}}:\n",
       " (\"w2\", var w2\n",
       " ┣━ ^ 1×2 Matrix{Float32}\n",
       " ┗━ ∇ 1×2 Matrix{Float32})\n",
       " (\"w1\", var w1\n",
       " ┣━ ^ 2×3 Matrix{Float32}\n",
       " ┗━ ∇ 2×3 Matrix{Float32})\n",
       " (\"b1\", var b1\n",
       " ┣━ ^ 2×1 Matrix{Float32}\n",
       " ┗━ ∇ 2×1 Matrix{Float32})\n",
       " (\"b2\", var b2\n",
       " ┣━ ^ 1×1 Matrix{Float32}\n",
       " ┗━ ∇ 1×1 Matrix{Float32})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_weights(order)\n",
    "get_biases(order)\n",
    "get_gradients(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bbb5f4",
   "metadata": {},
   "source": [
    "## Iris Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10024aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×20 Matrix{Float32}:\n",
       " 0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  …  0.0  1.0  1.0  1.0  1.0  0.0  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using MLDatasets\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using Plots\n",
    "using DataFrames # Dodajmy pakiet DataFrame, bo wygląda na to, że jest używany\n",
    "using MLDataUtils\n",
    "\n",
    "# Załaduj zbiór danych Iris\n",
    "iris_features, iris_targets = Iris(as_df=false)[:]\n",
    "class1_name = \"Iris-setosa\"\n",
    "class2_name = \"Iris-versicolor\"\n",
    "iris_features_cut = iris_features[:, 1:100];\n",
    "iris_targets_cut = iris_targets[:, 1:100];\n",
    "\n",
    "label_mapping = Dict(\"Iris-setosa\" => 0.0, \"Iris-versicolor\" => 1.0)\n",
    "iris_targets_cut_classes = [label_mapping[class_name] for class_name in iris_targets_cut]\n",
    "iris_shuffled_all_x, iris_shuffled_all_y = shuffleobs((iris_features_cut, iris_targets_cut_classes));\n",
    "# Podział na zbiór treningowy i testowy (np. 80% train, 20% test)\n",
    "train_ratio = 0.8\n",
    "num_all_obs = size(iris_shuffled_all_x, 2)\n",
    "num_train_obs = floor(Int, num_all_obs * train_ratio)\n",
    "\n",
    "X_train = Float32.(iris_shuffled_all_x[:, 1:num_train_obs])\n",
    "y_train = Float32.(iris_shuffled_all_y[:, 1:num_train_obs]) # Zakładając, że y_train ma kształt (out, num_obs)\n",
    "\n",
    "X_test = Float32.(iris_shuffled_all_x[:, num_train_obs+1:end])\n",
    "y_test = Float32.(iris_shuffled_all_y[:, num_train_obs+1:end]) # Podobnie dla y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebcdbc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13-element Vector{Any}:\n",
       " var w2\n",
       " ┣━ ^ 1×8 Matrix{Float32}\n",
       " ┗━ ∇ 1×8 Matrix{Float32}\n",
       " var w1\n",
       " ┣━ ^ 8×4 Matrix{Float32}\n",
       " ┗━ ∇ 8×4 Matrix{Float32}\n",
       " const Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " op.z1_mul(typeof(mul!))\n",
       " var b1\n",
       " ┣━ ^ 8×1 Matrix{Float32}\n",
       " ┗━ ∇ 8×1 Matrix{Float32}\n",
       " op.z1(typeof(+))\n",
       " op.d1(typeof(relu))\n",
       " op.z2_mul(typeof(mul!))\n",
       " var b2\n",
       " ┣━ ^ 1×1 Matrix{Float32}\n",
       " ┗━ ∇ 1×1 Matrix{Float32}\n",
       " op.z2(typeof(+))\n",
       " op.ŷ(typeof(σ))\n",
       " const Float32[0.0 0.0 … 0.0 0.0]\n",
       " op loss(typeof(binary_cross_entropy_loss_impl))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#   Ustawienia sieci neuronowej\n",
    "features = 4\n",
    "hidden = 8\n",
    "out = 1\n",
    "epochs = 30\n",
    "batch_size = 10\n",
    "\n",
    "x = Constant(zeros(Float32, features, batch_size))\n",
    "w1 = Variable(xavier_uniform((hidden, features)); name=\"w1\")\n",
    "z1_mul = w1 * x\n",
    "z1_mul.name = \"z1_mul\"\n",
    "b1 = Variable(xavier_uniform((hidden, 1)); name=\"b1\")\n",
    "z1 = z1_mul + b1\n",
    "z1.name = \"z1\"\n",
    "d1 = relu(z1)\n",
    "d1.name = \"d1\"\n",
    "w2 = Variable(xavier_uniform((out, hidden)); name=\"w2\")\n",
    "z2_mul = w2 * d1\n",
    "z2_mul.name = \"z2_mul\"\n",
    "b2 = Variable(xavier_uniform((out, 1)); name=\"b2\")\n",
    "z2 = z2_mul + b2\n",
    "z2.name = \"z2\"\n",
    "ŷ = σ(z2)\n",
    "ŷ.name = \"ŷ\"\n",
    "y = Constant(zeros(Float32, out, batch_size))\n",
    "loss = binarycrossentropy(ŷ, y)\n",
    "loss.name = \"loss\"\n",
    "order = topological_sort(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceb8ed70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state of optimizer:\n",
      "Dict{String, Any}(\"v\" => Dict{String, Matrix{Float32}}(\"b2\" => [0.0;;], \"w2\" => [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0], \"w1\" => [0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0], \"b1\" => [0.0; 0.0; 0.0; 0.0; 0.0; 0.0; 0.0; 0.0;;]), \"m\" => Dict{String, Matrix{Float32}}(\"b2\" => [0.0;;], \"w2\" => [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0], \"w1\" => [0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0], \"b1\" => [0.0; 0.0; 0.0; 0.0; 0.0; 0.0; 0.0; 0.0;;]), \"t\" => 0)\n"
     ]
    }
   ],
   "source": [
    "#   Start ADAM   \n",
    "optimizer = init!(order)\n",
    "println(\"Initial state of optimizer:\")\n",
    "println(get_state(optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "456ef0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss after epoch 1: 0.39245194\n",
      "Epoch 2\n",
      "Loss after epoch 2: 0.36798656\n",
      "Epoch 3\n",
      "Loss after epoch 3: 0.393301\n",
      "Epoch 4\n",
      "Loss after epoch 4: 0.365246\n",
      "Epoch 5\n",
      "Loss after epoch 5: 0.36020437\n",
      "Epoch 6\n",
      "Loss after epoch 6: 0.3220604\n",
      "Epoch 7\n",
      "Loss after epoch 7: 0.35850725\n",
      "Epoch 8\n",
      "Loss after epoch 8: 0.3821807\n",
      "Epoch 9\n",
      "Loss after epoch 9: 0.32759959\n",
      "Epoch 10\n",
      "Loss after epoch 10: 0.31206957\n",
      "Epoch 11\n",
      "Loss after epoch 11: 0.3031282\n",
      "Epoch 12\n",
      "Loss after epoch 12: 0.31560922\n",
      "Epoch 13\n",
      "Loss after epoch 13: 0.26405737\n",
      "Epoch 14\n",
      "Loss after epoch 14: 0.3200249\n",
      "Epoch 15\n",
      "Loss after epoch 15: 0.28471595\n",
      "Epoch 16\n",
      "Loss after epoch 16: 0.2944382\n",
      "Epoch 17\n",
      "Loss after epoch 17: 0.24712132\n",
      "Epoch 18\n",
      "Loss after epoch 18: 0.25892696\n",
      "Epoch 19\n",
      "Loss after epoch 19: 0.25536734\n",
      "Epoch 20\n",
      "Loss after epoch 20: 0.2164693\n",
      "Epoch 21\n",
      "Loss after epoch 21: 0.24449864\n",
      "Epoch 22\n",
      "Loss after epoch 22: 0.21425393\n",
      "Epoch 23\n",
      "Loss after epoch 23: 0.21903259\n",
      "Epoch 24\n",
      "Loss after epoch 24: 0.20544605\n",
      "Epoch 25\n",
      "Loss after epoch 25: 0.21182384\n",
      "Epoch 26\n",
      "Loss after epoch 26: 0.20673756\n",
      "Epoch 27\n",
      "Loss after epoch 27: 0.18612152\n",
      "Epoch 28\n",
      "Loss after epoch 28: 0.21863167\n",
      "Epoch 29\n",
      "Loss after epoch 29: 0.19841632\n",
      "Epoch 30\n",
      "Loss after epoch 30: 0.1572816\n"
     ]
    }
   ],
   "source": [
    "num_training_samples = size(X_train, 2) # Liczba próbek w zbiorze treningowym\n",
    "\n",
    "\n",
    "#   Run training and visualize results\n",
    "loss_value = 0.0\n",
    "for epoch in 1:epochs\n",
    "    # --- Tasowanie zbioru treningowego NA NOWO w każdej epoce ---\n",
    "    permutation = randperm(num_training_samples)\n",
    "    X_train_shuffled_epoch = X_train[:, permutation]\n",
    "    y_train_shuffled_epoch = y_train[:, permutation]\n",
    "    num_batches = ceil(Int, num_training_samples / batch_size)\n",
    "\n",
    "    println(\"Epoch $epoch\")\n",
    "    for i in 1:num_batches\n",
    "        # Wybierz batch z POTASOWANYCH W TEJ EPOCE danych\n",
    "        start_idx = (i - 1) * batch_size + 1\n",
    "        end_idx = min(i * batch_size, num_training_samples)\n",
    "        x_batch = X_train_shuffled_epoch[:, start_idx:end_idx]\n",
    "        y_batch = y_train_shuffled_epoch[:, start_idx:end_idx]\n",
    "\n",
    "        x.output = x_batch\n",
    "        y.output = y_batch\n",
    "\n",
    "        forward!(order)\n",
    "\n",
    "        #println(\"ŷ: \", ŷ.output)\n",
    "        #println(\"y: \", y.output)\n",
    "    \n",
    "        loss_value = loss.output\n",
    "        backward!(order)\n",
    "        step!(optimizer)\n",
    "\n",
    "    end\n",
    "\n",
    "    println(\"Loss after epoch $epoch: \", loss_value)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b43eeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Test Evaluation ---\n",
      "Test Accuracy: 100.0 %\n",
      "\n",
      "Confusion Matrix:\n",
      "TP: 12, TN: 8, FP: 0, FN: 0\n",
      "Sum of CM components: 20 (Should be 20)\n",
      "Precision (for class 1): 1.0\n",
      "Recall (for class 1): 1.0\n",
      "F1 Score (for class 1): 1.0\n",
      "\n",
      "--- Test Evaluation Finished ---\n"
     ]
    }
   ],
   "source": [
    "println(\"\\n--- Starting Test Evaluation ---\")\n",
    "\n",
    "x.output = X_test\n",
    "y.output = y_test\n",
    "\n",
    "forward!(order)\n",
    "\n",
    "predictions_prob = ŷ.output\n",
    "\n",
    "predicted_classes = (predictions_prob .> 0.5) # Wynik to BitMatrix (1, liczba_próbek_testowych)\n",
    "\n",
    "true_classes = convert.(Bool, y_test) # Wynik to BitMatrix (1, liczba_próbek_testowych)\n",
    "\n",
    "# --- Obliczanie Dokładności ---\n",
    "# Najprostsze obliczenie powinno działać poprawnie na BitMatrix\n",
    "correct_predictions = sum(predicted_classes .== true_classes) # Sumuje true w macierzy wynikowej porównania element-wise\n",
    "\n",
    "total_test_samples = size(X_test, 2)\n",
    "accuracy = correct_predictions / total_test_samples\n",
    "\n",
    "println(\"Test Accuracy: $(accuracy * 100.0) %\")\n",
    "\n",
    "# --- Obliczanie Macierzy Pomyłek (Confusion Matrix) - POPRAWIONE ---\n",
    "# Używamy standardowych operatorów logicznych na macierzach Boolowskich\n",
    "# TP: predicted = true AND true = true\n",
    "TP = sum(predicted_classes .& true_classes)\n",
    "\n",
    "# TN: predicted = false AND true = false\n",
    "TN = sum(.!predicted_classes .& .!true_classes)\n",
    "\n",
    "# FP: predicted = true AND true = false\n",
    "FP = sum(predicted_classes .& .!true_classes)\n",
    "\n",
    "# FN: predicted = false AND true = true\n",
    "FN = sum(.!predicted_classes .& true_classes)\n",
    "\n",
    "println(\"\\nConfusion Matrix:\")\n",
    "println(\"TP: $(TP), TN: $(TN), FP: $(FP), FN: $(FN)\")\n",
    "\n",
    "# Ważna weryfikacja: Sprawdź, czy suma komponentów CM równa się liczbie próbek testowych\n",
    "println(\"Sum of CM components: $(TP + TN + FP + FN) (Should be $(total_test_samples))\")\n",
    "\n",
    "# Możesz teraz bezpiecznie obliczyć Precision, Recall, F1, używając tych (poprawionych) wartości TP, TN, FP, FN.\n",
    "if (TP + FP) > 0\n",
    "    precision = TP / (TP + FP)\n",
    "    println(\"Precision (for class 1): $(precision)\")\n",
    "else\n",
    "    println(\"Precision (for class 1): N/A (No positive predictions)\")\n",
    "end\n",
    "\n",
    "if (TP + FN) > 0\n",
    "    recall = TP / (TP + FN)\n",
    "     println(\"Recall (for class 1): $(recall)\")\n",
    "else\n",
    "     println(\"Recall (for class 1): N/A (No actual positive samples of class 1)\")\n",
    "end\n",
    "\n",
    "if (precision + recall) > 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    println(\"F1 Score (for class 1): $(f1_score)\")\n",
    "else\n",
    "    println(\"F1 Score (for class 1): N/A\")\n",
    "end\n",
    "\n",
    "\n",
    "println(\"\\n--- Test Evaluation Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7988cf76",
   "metadata": {},
   "source": [
    "## IMDB Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6679a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Matrix(load(\"../../dataset/imdb_dataset_prepared.jld2\", \"X_train\"));\n",
    "y_train = Matrix(load(\"../../dataset/imdb_dataset_prepared.jld2\", \"y_train\"));\n",
    "X_test = Matrix(load(\"../../dataset/imdb_dataset_prepared.jld2\", \"X_test\"));\n",
    "y_test = Matrix(load(\"../../dataset/imdb_dataset_prepared.jld2\", \"y_test\"));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01eb07d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `X_train` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `X_train` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Repos/AWiD/MyMlp/src/notebooks/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X65sZmlsZQ==.jl:2"
     ]
    }
   ],
   "source": [
    "#   Ustawienia sieci neuronowej\n",
    "features = size(X_train, 1)\n",
    "hidden = 32\n",
    "out = 1\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "x = Constant(zeros(Float32, features, batch_size))\n",
    "w1 = Variable(xavier_uniform((hidden, features)); name=\"w1\")\n",
    "z1_mul = w1 * x\n",
    "z1_mul.name = \"z1_mul\"\n",
    "b1 = Variable(xavier_uniform((hidden, 1)); name=\"b1\")\n",
    "z1 = z1_mul + b1\n",
    "z1.name = \"z1\"\n",
    "d1 = relu(z1)\n",
    "d1.name = \"d1\"\n",
    "w2 = Variable(xavier_uniform((out, hidden)); name=\"w2\")\n",
    "z2_mul = w2 * d1\n",
    "z2_mul.name = \"z2_mul\"\n",
    "b2 = Variable(xavier_uniform((out, 1)); name=\"b2\")\n",
    "z2 = z2_mul + b2\n",
    "z2.name = \"z2\"\n",
    "ŷ = σ(z2)\n",
    "ŷ.name = \"ŷ\"\n",
    "y = Constant(zeros(Float32, out, batch_size))\n",
    "loss = binarycrossentropy(ŷ, y)\n",
    "loss.name = \"loss\"\n",
    "order = topological_sort(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed535b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam(0.001f0, 0.9f0, 0.999f0, 1.0f-8, Dict{String, Matrix{Float32}}(\"b2\" => [0.0;;], \"w2\" => [0.0 0.0 … 0.0 0.0], \"w1\" => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], \"b1\" => [0.0; 0.0; … ; 0.0; 0.0;;]), Dict{String, Matrix{Float32}}(\"b2\" => [0.0;;], \"w2\" => [0.0 0.0 … 0.0 0.0], \"w1\" => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], \"b1\" => [0.0; 0.0; … ; 0.0; 0.0;;]), 0, Tuple{String, Variable}[(\"w2\", var w2\n",
       " ┣━ ^ 1×32 Matrix{Float32}\n",
       " ┗━ ∇ 1×32 Matrix{Float32}), (\"w1\", var w1\n",
       " ┣━ ^ 32×17703 Matrix{Float32}\n",
       " ┗━ ∇ 32×17703 Matrix{Float32}), (\"b1\", var b1\n",
       " ┣━ ^ 32×1 Matrix{Float32}\n",
       " ┗━ ∇ 32×1 Matrix{Float32}), (\"b2\", var b2\n",
       " ┣━ ^ 1×1 Matrix{Float32}\n",
       " ┗━ ∇ 1×1 Matrix{Float32})])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#   Start ADAM   \n",
    "optimizer = init!(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b10799a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss after epoch 1 (4.87s): 0.64\n",
      "Epoch 2\n",
      "Loss after epoch 2 (1.42s): 0.55\n",
      "Epoch 3\n",
      "Loss after epoch 3 (1.39s): 0.48\n",
      "Epoch 4\n",
      "Loss after epoch 4 (1.45s): 0.30\n",
      "Epoch 5\n",
      "Loss after epoch 5 (1.40s): 0.26\n"
     ]
    }
   ],
   "source": [
    "using Printf\n",
    "num_training_samples = size(X_train, 2) # Liczba próbek w zbiorze treningowym\n",
    "\n",
    "\n",
    "#   Run training and visualize results\n",
    "loss_value = 0.0\n",
    "for epoch in 1:epochs\n",
    "    # --- Tasowanie zbioru treningowego NA NOWO w każdej epoce ---\n",
    "    permutation = randperm(num_training_samples)\n",
    "    X_train_shuffled_epoch = X_train[:, permutation]\n",
    "    y_train_shuffled_epoch = y_train[:, permutation]\n",
    "    num_batches = ceil(Int, num_training_samples / batch_size)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    t = @elapsed begin\n",
    "        println(\"Epoch $epoch\")\n",
    "        for i in 1:num_batches\n",
    "            # Wybierz batch z POTASOWANYCH W TEJ EPOCE danych\n",
    "            start_idx = (i - 1) * batch_size + 1\n",
    "            end_idx = min(i * batch_size, num_training_samples)\n",
    "            x_batch = X_train_shuffled_epoch[:, start_idx:end_idx]\n",
    "            y_batch = y_train_shuffled_epoch[:, start_idx:end_idx]\n",
    "    \n",
    "            x.output = x_batch\n",
    "            y.output = y_batch\n",
    "    \n",
    "            forward!(order)\n",
    "            backward!(order)\n",
    "            step!(optimizer)\n",
    "\n",
    "            num_samples = i\n",
    "        end\n",
    "    end\n",
    "    loss_value = loss.output\n",
    "\n",
    "    println(@sprintf(\"Loss after epoch %d (%.2fs): %.2f\", epoch, t, loss_value))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49c632fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Test Evaluation ---\n",
      "Test Accuracy: 86.45 %\n",
      "\n",
      "Confusion Matrix:\n",
      "TP: 924, TN: 805, FP: 170, FN: 101\n",
      "Sum of CM components: 2000 (Should be 2000)\n",
      "Precision (for class 1): 0.8446069469835467\n",
      "Recall (for class 1): 0.9014634146341464\n",
      "F1 Score (for class 1): 0.8721094856064181\n",
      "\n",
      "--- Test Evaluation Finished ---\n"
     ]
    }
   ],
   "source": [
    "println(\"\\n--- Starting Test Evaluation ---\")\n",
    "\n",
    "x.output = X_test\n",
    "y.output = y_test\n",
    "\n",
    "forward!(order)\n",
    "\n",
    "predictions_prob = ŷ.output\n",
    "\n",
    "predicted_classes = (predictions_prob .> 0.5) # Wynik to BitMatrix (1, liczba_próbek_testowych)\n",
    "\n",
    "true_classes = convert.(Bool, y_test) # Wynik to BitMatrix (1, liczba_próbek_testowych)\n",
    "\n",
    "# --- Obliczanie Dokładności ---\n",
    "# Najprostsze obliczenie powinno działać poprawnie na BitMatrix\n",
    "correct_predictions = sum(predicted_classes .== true_classes) # Sumuje true w macierzy wynikowej porównania element-wise\n",
    "\n",
    "total_test_samples = size(X_test, 2)\n",
    "accuracy = correct_predictions / total_test_samples\n",
    "\n",
    "println(\"Test Accuracy: $(accuracy * 100.0) %\")\n",
    "\n",
    "# --- Obliczanie Macierzy Pomyłek (Confusion Matrix) - POPRAWIONE ---\n",
    "# Używamy standardowych operatorów logicznych na macierzach Boolowskich\n",
    "# TP: predicted = true AND true = true\n",
    "TP = sum(predicted_classes .& true_classes)\n",
    "\n",
    "# TN: predicted = false AND true = false\n",
    "TN = sum(.!predicted_classes .& .!true_classes)\n",
    "\n",
    "# FP: predicted = true AND true = false\n",
    "FP = sum(predicted_classes .& .!true_classes)\n",
    "\n",
    "# FN: predicted = false AND true = true\n",
    "FN = sum(.!predicted_classes .& true_classes)\n",
    "\n",
    "println(\"\\nConfusion Matrix:\")\n",
    "println(\"TP: $(TP), TN: $(TN), FP: $(FP), FN: $(FN)\")\n",
    "\n",
    "# Ważna weryfikacja: Sprawdź, czy suma komponentów CM równa się liczbie próbek testowych\n",
    "println(\"Sum of CM components: $(TP + TN + FP + FN) (Should be $(total_test_samples))\")\n",
    "\n",
    "# Możesz teraz bezpiecznie obliczyć Precision, Recall, F1, używając tych (poprawionych) wartości TP, TN, FP, FN.\n",
    "if (TP + FP) > 0\n",
    "    precision = TP / (TP + FP)\n",
    "    println(\"Precision (for class 1): $(precision)\")\n",
    "else\n",
    "    println(\"Precision (for class 1): N/A (No positive predictions)\")\n",
    "end\n",
    "\n",
    "if (TP + FN) > 0\n",
    "    recall = TP / (TP + FN)\n",
    "     println(\"Recall (for class 1): $(recall)\")\n",
    "else\n",
    "     println(\"Recall (for class 1): N/A (No actual positive samples of class 1)\")\n",
    "end\n",
    "\n",
    "if (precision + recall) > 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    println(\"F1 Score (for class 1): $(f1_score)\")\n",
    "else\n",
    "    println(\"F1 Score (for class 1): N/A\")\n",
    "end\n",
    "\n",
    "\n",
    "println(\"\\n--- Test Evaluation Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b40666f",
   "metadata": {},
   "source": [
    "##  API Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05ab6b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Matrix(load(\"../../dataset/imdb_dataset_prepared.jld2\", \"X_train\"));\n",
    "y_train = Matrix(load(\"../../dataset/imdb_dataset_prepared.jld2\", \"y_train\"));\n",
    "X_test = Matrix(load(\"../../dataset/imdb_dataset_prepared.jld2\", \"X_test\"));\n",
    "y_test = Matrix(load(\"../../dataset/imdb_dataset_prepared.jld2\", \"y_test\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6490883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (1.22s) \tTrain: (l: 0.67)\n",
      "Epoch: 2 (1.22s) \tTrain: (l: 0.58)\n",
      "Epoch: 3 (1.22s) \tTrain: (l: 0.47)\n",
      "Epoch: 4 (1.22s) \tTrain: (l: 0.38)\n",
      "Epoch: 5 (1.22s) \tTrain: (l: 0.30)\n"
     ]
    }
   ],
   "source": [
    "# 1. Definicja rozmiarów modelu\n",
    "input_size = size(X_train, 1) # Liczba cech\n",
    "hidden_size = 8\n",
    "output_size = 1\n",
    "batch_size = 64\n",
    "\n",
    "# 2. Inicjalizacja modelu (Chain) (raz)\n",
    "model = Chain(\n",
    "    Dense(input_size, hidden_size, relu; weight_init=xavier_uniform,  name=\"layer1\"),\n",
    "    Dense(hidden_size, output_size, σ; weight_init=xavier_uniform, name=\"layer2\")\n",
    ")\n",
    "\n",
    "# 3. Utworzenie początkowych węzłów Constant dla danych wejściowych i etykiet (raz)\n",
    "x_input_node = Constant(zeros(Float32, input_size, batch_size))\n",
    "y_label_node = Constant(zeros(Float32, output_size, batch_size))\n",
    "\n",
    "# 4. Budowanie grafu treningowego (raz)\n",
    "loss_node, model_output_node, order = build_graph!(model, binarycrossentropy, x_input_node, y_label_node; loss_name=\"loss\")\n",
    "\n",
    "optimizer_state = setup_optimizer(Adam(), model)\n",
    "\n",
    "# 5. Początek treningu\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "for epoch in 1:epochs\n",
    "    # --- Tasowanie zbioru treningowego NA NOWO w każdej epoce ---\n",
    "    permutation = randperm(size(X_train, 2))\n",
    "    X_train_shuffled_epoch = X_train[:, permutation]\n",
    "    y_train_shuffled_epoch = y_train[:, permutation]\n",
    "    num_batches = ceil(Int, size(X_train, 2) / batch_size)\n",
    "\n",
    "    loss_value = 0.0\n",
    "\n",
    "    t = @elapsed begin\n",
    "\n",
    "    for i in 1:num_batches\n",
    "\n",
    "        start_idx = (i - 1) * batch_size + 1\n",
    "        end_idx = min(i * batch_size, size(X_train, 2))\n",
    "        x_batch = X_train_shuffled_epoch[:, start_idx:end_idx]\n",
    "        y_batch = y_train_shuffled_epoch[:, start_idx:end_idx]\n",
    "\n",
    "        current_batch_size = size(x_batch, 2)\n",
    "        view(x_input_node.output, :, 1:current_batch_size) .= x_batch\n",
    "        view(y_label_node.output, :, 1:current_batch_size) .= y_batch\n",
    "\n",
    "\n",
    "        forward!(order)\n",
    "\n",
    "        backward!(order)\n",
    "\n",
    "        step!(optimizer_state)\n",
    "        loss_value += loss_node.output\n",
    "\n",
    "    end\n",
    "end\n",
    "    avg_loss_epoch = loss_value / num_batches\n",
    "\n",
    "    println(@sprintf(\"Epoch: %d (%.2fs) \\tTrain: (l: %.2f)\", epoch, t, avg_loss_epoch))\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "955329f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss (czas: 0.39s): 0.3952\n",
      "Test Accuracy: 86.45 %\n"
     ]
    }
   ],
   "source": [
    "# --- Test Evaluation ---\n",
    "\n",
    "batch_size = 64\n",
    "num_test_samples = size(X_test, 2)\n",
    "num_batches = ceil(Int, num_test_samples / batch_size)\n",
    "total_test_loss_sum = 0.0\n",
    "total_correct_predictions = 0.0\n",
    "\n",
    "t_test = @elapsed begin\n",
    "    for i in 1:num_batches\n",
    "\n",
    "        start_idx = (i - 1) * batch_size + 1\n",
    "        end_idx = min(i * batch_size, num_test_samples)\n",
    "        x_batch_test = X_test[:, start_idx:end_idx]\n",
    "        y_batch_test = y_test[:, start_idx:end_idx]\n",
    "\n",
    "        # Aktualna liczba próbek w bieżącym batchu (może być mniejsza dla ostatniego batcha)\n",
    "        current_test_batch_size = size(x_batch_test, 2)\n",
    "\n",
    "        view(x_input_node.output, :, 1:current_test_batch_size) .= x_batch_test\n",
    "        view(y_label_node.output, :, 1:current_test_batch_size) .= y_batch_test\n",
    "\n",
    "        forward!(order)\n",
    "\n",
    "        predictions = view(model_output_node.output, :, 1:current_test_batch_size)\n",
    "\n",
    "\n",
    "        batch_loss = loss_node.output\n",
    "        \n",
    "        total_test_loss_sum += batch_loss * current_test_batch_size # Sumuj stratę, uwzględniając rozmiar batcha\n",
    "\n",
    "        # --- Oblicz dokładność na bieżącym batchu testowym ---\n",
    "        # Dla klasyfikacji binarnej z progiem 0.5 (lub innym, w zależności od problemu)\n",
    "        batch_accuracy = sum((predictions .> 0.5f0) .== y_batch_test) / current_test_batch_size\n",
    "        total_correct_predictions += batch_accuracy * current_test_batch_size # Sumuj poprawne predykcje\n",
    "    end\n",
    "end\n",
    "\n",
    "# --- Oblicz średnią stratę i średnią dokładność na całym zbiorze testowym ---\n",
    "avg_test_loss = total_test_loss_sum / num_test_samples\n",
    "avg_test_accuracy = total_correct_predictions / num_test_samples * 100.0\n",
    "\n",
    "println(@sprintf(\"Test Loss (czas: %.2fs): %.4f\", t_test, avg_test_loss))\n",
    "println(\"Test Accuracy: $avg_test_accuracy %\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.0",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
