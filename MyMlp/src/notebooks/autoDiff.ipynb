{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f1ba62",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3221369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Repos/AWiD/MyMlp`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mProject\u001b[22m\u001b[39m MyMlp v0.1.0\n",
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "  \u001b[90m[6e4b80f9] \u001b[39mBenchmarkTools v1.6.0\n",
      "  \u001b[90m[a93c6f00] \u001b[39mDataFrames v1.7.0\n",
      "  \u001b[90m[31c24e10] \u001b[39mDistributions v0.25.120\n",
      "  \u001b[90m[033835bb] \u001b[39mJLD2 v0.5.13\n",
      "  \u001b[90m[cc2ba9b6] \u001b[39mMLDataUtils v0.5.4\n",
      "  \u001b[90m[eb30cadb] \u001b[39mMLDatasets v0.7.18\n",
      "  \u001b[90m[91a5bcdd] \u001b[39mPlots v1.40.13\n",
      "  \u001b[90m[10745b16] \u001b[39mStatistics v1.11.1\n",
      "  \u001b[90m[37e2e46d] \u001b[39mLinearAlgebra v1.11.0\n",
      "  \u001b[90m[9a3f8284] \u001b[39mRandom v1.11.0\n"
     ]
    }
   ],
   "source": [
    "# Make sure we're using the latest version\n",
    "import Pkg\n",
    "Pkg.activate(\"../..\")  # Activate the project environment\n",
    "\n",
    "Pkg.add(\"BenchmarkTools\")\n",
    "Pkg.add(\"LinearAlgebra\")\n",
    "Pkg.add(\"Statistics\")\n",
    "Pkg.add(\"Distributions\")\n",
    "Pkg.add(\"Random\")\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"MLDatasets\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"MLDataUtils\")\n",
    "Pkg.add(\"JLD2\")\n",
    "Pkg.instantiate()      # Install any missing dependencies\n",
    "Pkg.status()          # Check if MyMlp is listed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bfe5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try importing\n",
    "using BenchmarkTools\n",
    "using LinearAlgebra\n",
    "using Distributions\n",
    "using Random\n",
    "using MLDatasets\n",
    "using Plots\n",
    "using Statistics\n",
    "using DataFrames\n",
    "using JLD2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb89e6c",
   "metadata": {},
   "source": [
    "## Comparison of different optimizations to Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc8f4feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Benchmark: READ access ###\n",
      "Original:\n",
      "  14.034 ns (1 allocation: 16 bytes)\n",
      "Optimized:\n",
      "  1.575 ns (0 allocations: 0 bytes)\n",
      "RefValue:\n",
      "  1.793 ns (0 allocations: 0 bytes)\n",
      "\n",
      "### Benchmark: WRITE mutation ###\n",
      "Original:\n",
      "  14.417 ns (1 allocation: 16 bytes)\n",
      "Optimized:\n",
      "  2.013 ns (0 allocations: 0 bytes)\n",
      "RefValue:\n",
      "  2.013 ns (0 allocations: 0 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500503.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"BenchmarkTools\")\n",
    "using BenchmarkTools\n",
    "\n",
    "abstract type GraphNode end\n",
    "abstract type Operator <: GraphNode end\n",
    "\n",
    "# Original implementation\n",
    "mutable struct VariableOriginal <: GraphNode\n",
    "    output :: Any\n",
    "    grad :: Any\n",
    "    name :: String\n",
    "    VariableOriginal(output; name=\"?\") = new(output, nothing, name)\n",
    "end\n",
    "\n",
    "# Optimized implementation\n",
    "mutable struct VariableOptimized{T<:Float64} <: GraphNode\n",
    "    output :: T\n",
    "    grad :: Union{Nothing, T}\n",
    "    name :: String\n",
    "    VariableOptimized(output::T; name=\"?\") where T<:Float64 = new{T}(output, nothing, name)\n",
    "end\n",
    "\n",
    "# RefValue-based immutable implementation\n",
    "struct VariableRef <: GraphNode\n",
    "    output :: Base.RefValue{Float64}\n",
    "    grad   :: Union{Nothing, Base.RefValue{Float64}}\n",
    "    name   :: String\n",
    "    function VariableRef(output::Float64; name=\"?\")\n",
    "        new(Base.RefValue(output), nothing, name)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Functions to benchmark: reading\n",
    "function read_output(v::VariableOriginal)\n",
    "    v.output + 1.0\n",
    "end\n",
    "\n",
    "function read_output(v::VariableOptimized)\n",
    "    v.output + 1.0\n",
    "end\n",
    "\n",
    "function read_output(v::VariableRef)\n",
    "    v.output[] + 1.0\n",
    "end\n",
    "\n",
    "# Functions to benchmark: writing\n",
    "function write_output!(v::VariableOriginal)\n",
    "    v.output = v.output + 1.0\n",
    "end\n",
    "\n",
    "function write_output!(v::VariableOptimized)\n",
    "    v.output = v.output + 1.0\n",
    "end\n",
    "\n",
    "function write_output!(v::VariableRef)\n",
    "    v.output[] = v.output[] + 1.0\n",
    "end\n",
    "\n",
    "# Create instances\n",
    "v1 = VariableOriginal(1.0)\n",
    "v2 = VariableOptimized(1.0)\n",
    "v3 = VariableRef(1.0)\n",
    "\n",
    "# Benchmark READ\n",
    "println(\"### Benchmark: READ access ###\")\n",
    "println(\"Original:\")\n",
    "@btime read_output($v1)\n",
    "\n",
    "println(\"Optimized:\")\n",
    "@btime read_output($v2)\n",
    "\n",
    "println(\"RefValue:\")\n",
    "@btime read_output($v3)\n",
    "\n",
    "# Benchmark WRITE\n",
    "println(\"\\n### Benchmark: WRITE mutation ###\")\n",
    "println(\"Original:\")\n",
    "@btime write_output!($v1)\n",
    "\n",
    "println(\"Optimized:\")\n",
    "@btime write_output!($v2)\n",
    "\n",
    "println(\"RefValue:\")\n",
    "@btime write_output!($v3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4694cbbe",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c212c68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 5 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Base: *, +, clamp, log, exp\n",
    "import LinearAlgebra: mul!\n",
    "import Statistics: sum\n",
    "\n",
    "abstract type GraphNode end\n",
    "abstract type Operator <: GraphNode end\n",
    "\n",
    "# Definition of basic structures for computational graph\n",
    "mutable struct Constant{T<:Matrix{Float32}} <: GraphNode\n",
    "    output :: T\n",
    "end\n",
    "\n",
    "mutable struct Variable{T<:Matrix{Float32}} <: GraphNode\n",
    "    output :: T\n",
    "    gradient :: T\n",
    "    name :: String\n",
    "    \n",
    "    Variable(output::T; name=\"?\") where {T<:Matrix{Float32}} = new{T}(output, zeros(Float32, size(output)), name)\n",
    "end\n",
    "\n",
    "mutable struct ScalarOperator{F} <: Operator\n",
    "    inputs :: Tuple{GraphNode, GraphNode}\n",
    "    output :: Float32\n",
    "    gradient :: Float32\n",
    "    name :: String\n",
    "    ScalarOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, 0.0f0, 0.0f0, name)\n",
    "end\n",
    "\n",
    "mutable struct BroadcastedOperator{F} <: Operator\n",
    "    inputs :: Union{Tuple{GraphNode, GraphNode}, Tuple{GraphNode}}\n",
    "    output :: Matrix{Float32}\n",
    "    gradient :: Matrix{Float32}\n",
    "    name :: String\n",
    "    BroadcastedOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, zeros(Float32, 1, 1), zeros(Float32, 1, 1), name)\n",
    "end\n",
    "\n",
    "\n",
    "import Base: show, summary\n",
    "show(io::IO, x::ScalarOperator{F}) where {F} = print(io, \"op \", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::BroadcastedOperator{F}) where {F} = print(io, \"op.\", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::Constant) = print(io, \"const \", x.output)\n",
    "show(io::IO, x::Variable) = begin\n",
    "    print(io, \"var \", x.name);\n",
    "    print(io, \"\\n ┣━ ^ \"); summary(io, x.output)\n",
    "    print(io, \"\\n ┗━ ∇ \");  summary(io, x.gradient)\n",
    "end\n",
    "\n",
    "\n",
    "function visit(node::GraphNode, visited, order)\n",
    "    if node ∈ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return zeros(Float32, 1, 1)\n",
    "end\n",
    "\n",
    "function visit(node::Operator, visited, order)\n",
    "    if node ∈ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        for input in node.inputs\n",
    "            visit(input, visited, order)\n",
    "        end\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return zeros(Float32, 1, 1)\n",
    "end\n",
    "\n",
    "function topological_sort(head::GraphNode)\n",
    "    visited = Set()\n",
    "    order = Vector()\n",
    "    visit(head, visited, order)\n",
    "    return order\n",
    "end\n",
    "\n",
    "\n",
    "# x * y (aka matrix multiplication)\n",
    "*(A::GraphNode, x::GraphNode) = BroadcastedOperator(mul!, A, x)\n",
    "forward(::BroadcastedOperator{typeof(mul!)}, A, x) = return A * x\n",
    "backward(::BroadcastedOperator{typeof(mul!)}, A, x, g) = tuple(g * x', A' * g)\n",
    "\n",
    "# relu activation\n",
    "relu(x::GraphNode) = BroadcastedOperator(relu, x)\n",
    "forward(::BroadcastedOperator{typeof(relu)}, x) = return x .* (x .> 0.0f0)\n",
    "backward(::BroadcastedOperator{typeof(relu)}, x, g) = tuple(g .* (x .> 0.0f0), zeros(Float32, 1, 1))\n",
    "\n",
    "# add operation (for bias)\n",
    "+(x::GraphNode, y::GraphNode) = BroadcastedOperator(+, x, y)\n",
    "forward(::BroadcastedOperator{typeof(+)}, x, y) = return x .+ y\n",
    "backward(::BroadcastedOperator{typeof(+)}, x, y, g) = begin\n",
    "    grad_wrt_x = g\n",
    "    grad_wrt_y = sum(g, dims=2)\n",
    "    return (grad_wrt_x, grad_wrt_y)\n",
    "end\n",
    "\n",
    "# sigmoid activation\n",
    "σ(x::GraphNode) = BroadcastedOperator(σ, x)\n",
    "forward(::BroadcastedOperator{typeof(σ)}, x) = return 1.0f0 ./ (1.0f0 .+ exp.(-x))\n",
    "backward(node::BroadcastedOperator{typeof(σ)}, x, g) = begin\n",
    "    y = node.output\n",
    "    local_derivative = y .* (1.0f0 .- y)\n",
    "    grad_wrt_x = g .* local_derivative\n",
    "    return (grad_wrt_x, zeros(Float32, 1, 1))\n",
    "end\n",
    "\n",
    "function binary_cross_entropy_loss_impl(ŷ, y_true; epsilon=1e-10)\n",
    "    ŷ_clamped = clamp.(ŷ, epsilon, 1.0f0 - epsilon)\n",
    "    loss_elements = -y_true .* log.(ŷ_clamped) .- (1.0f0 .- y_true) .* log.(1.0f0 .- ŷ_clamped)\n",
    "    return mean(loss_elements)\n",
    "end\n",
    "\n",
    "binarycrossentropy(ŷ::GraphNode, y::GraphNode) = ScalarOperator(binary_cross_entropy_loss_impl, ŷ, y)\n",
    "\n",
    "forward(::ScalarOperator{typeof(binary_cross_entropy_loss_impl)}, ŷ_value, y_value) = begin\n",
    "    loss_value = binary_cross_entropy_loss_impl(ŷ_value, y_value)\n",
    "    return loss_value\n",
    "end\n",
    "\n",
    "backward(::ScalarOperator{typeof(binary_cross_entropy_loss_impl)}, ŷ_value, y_value, g) = begin\n",
    "    epsilon = 1e-10\n",
    "    ŷ_clamped_for_grad = clamp.(ŷ_value, epsilon, 1.0f0 - epsilon)\n",
    "    local_grad_per_sample = (ŷ_clamped_for_grad .- y_value) ./ (ŷ_clamped_for_grad .* (1.0f0 .- ŷ_clamped_for_grad))\n",
    "    batch_size = size(y_value, 2)\n",
    "    grad_wrt_ŷ = local_grad_per_sample ./ batch_size\n",
    "    return (grad_wrt_ŷ, zeros(Float32, 1, 1))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f27fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reset!(node::Constant) = nothing\n",
    "reset!(node::Variable) = node.gradient = zeros(Float32, size(node.output))\n",
    "\n",
    "function reset!(node::Operator)\n",
    "    if isa(node.output, Matrix{Float32})\n",
    "        node.gradient = zeros(Float32, size(node.output))\n",
    "    else\n",
    "        node.gradient = 0.0f0\n",
    "    end\n",
    "end\n",
    "#reset!(node::Operator) = node.gradient = zeros(Float32, size(node.output))\n",
    "\n",
    "compute!(node::Constant) = nothing\n",
    "compute!(node::Variable) = nothing\n",
    "\n",
    "function compute!(node::Operator)\n",
    "    node.output = forward(node, [input.output for input in node.inputs]...)\n",
    "    if isa(node.output, Matrix{Float32})\n",
    "        node.gradient = zeros(Float32, size(node.output))\n",
    "    end\n",
    "end\n",
    "# compute!(node::Operator) =\n",
    "#     node.output = forward(node, [input.output for input in node.inputs]...)\n",
    "\n",
    "function forward!(order::Vector)\n",
    "    #   Iteruje przez każdy węzeł w order.\n",
    "    for node in order\n",
    "        compute!(node)\n",
    "        reset!(node)\n",
    "    end\n",
    "    return last(order).output\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c495933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "update!(node::Constant, gradient) = nothing\n",
    "\n",
    "update!(node::GraphNode, gradient) = if isnothing(node.gradient)\n",
    "    node.gradient = gradient else node.gradient .+= gradient\n",
    "end\n",
    "\n",
    "function backward!(order::Vector; seed=1.0)\n",
    "    result = last(order)   #   The output node\n",
    "    if all(iszero, result.gradient)\n",
    "        if isa(result.output, Matrix{Float32})\n",
    "            result.gradient = ones(Float64, size(result.output))\n",
    "        else\n",
    "            result.gradient = seed\n",
    "            @assert length(result.output) == 1 \"Gradient is defined only for scalar functions\"\n",
    "        end\n",
    "    end\n",
    "\n",
    "    for node in reverse(order)   #   Iterate through nodes in reverse topological order.\n",
    "        backward!(node)   #   Compute and propagate gradients backwards.\n",
    "    end\n",
    "    return zeros(Float32, 1, 1)\n",
    "end\n",
    "\n",
    "function backward!(node::Constant) end\n",
    "function backward!(node::Variable) end\n",
    "\n",
    "function backward!(node::Operator)\n",
    "    inputs = node.inputs\n",
    "\n",
    "    gradients = backward(node, [input.output for input in inputs]..., node.gradient)\n",
    "\n",
    "    for (input, gradient) in zip(inputs, gradients)\n",
    "        update!(input, gradient)\n",
    "    end\n",
    "    return zeros(Float32, 1, 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52f5b6e",
   "metadata": {},
   "source": [
    "## Funkcja Xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af4c532e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xavier_normal! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function xavier_uniform(size::Tuple{Int, Int})\n",
    "    limit = sqrt(6.0f0 / (size[1] + size[2]))\n",
    "    return Float32.(rand(Uniform(-limit, limit), size))\n",
    "end\n",
    "\n",
    "function xavier_normal(size::Tuple{Int, Int})\n",
    "    limit = sqrt(2.0f0 / (size[1] + size[2]))\n",
    "    return Float32.(rand(Normal(0.0f0, limit), size))\n",
    "end\n",
    "\n",
    "function xavier_uniform!(w::Matrix{Float32})\n",
    "    fan_out, fan_in = size(w)\n",
    "    limit = sqrt(6.0f0 / (fan_in + fan_out))\n",
    "    Float32.(rand!(Uniform(-limit, limit), w))\n",
    "end\n",
    "\n",
    "function xavier_normal!(w::Matrix{Float32})\n",
    "    fan_out, fan_in = size(w)\n",
    "    limit = sqrt(2.0f0 / (fan_in + fan_out))\n",
    "    Float32.(rand!(Normal(0.0f0, limit), w))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3218f4",
   "metadata": {},
   "source": [
    "##  Funkcje dostępowe do wag, biasów oraz ich gradientów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bfecadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_gradients (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function get_weights(order::Vector)\n",
    "    weights = Vector{Tuple{String, Variable}}()\n",
    "    for node in order\n",
    "        if isa(node, Variable)\n",
    "            if occursin(\"w\", node.name)\n",
    "                push!(weights, (node.name, node))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return weights\n",
    "end\n",
    "\n",
    "function get_biases(order::Vector)\n",
    "    biases = Vector{Tuple{String, Variable}}()\n",
    "    for node in order\n",
    "        if isa(node, Variable)\n",
    "            if occursin(\"b\", node.name)\n",
    "                push!(biases, (node.name, node))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return biases\n",
    "end\n",
    "\n",
    "function get_weights_and_biases(order::Vector)\n",
    "    parameters = Vector{Tuple{String, Variable}}()\n",
    "    for node in order\n",
    "        if isa(node, Variable)\n",
    "            if occursin(\"w\", node.name) || occursin(\"b\", node.name)\n",
    "                push!(parameters, (node.name, node))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return parameters\n",
    "end\n",
    "\n",
    "function get_gradients(order::Vector)\n",
    "    gradients = Vector{Tuple{String, Variable}}()\n",
    "    for node in order\n",
    "        if isa(node, Variable)\n",
    "            if occursin(\"w\", node.name) || occursin(\"b\", node.name)\n",
    "                push!(gradients, (node.name, node))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return gradients\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c977923f",
   "metadata": {},
   "source": [
    "## Optymalizator ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0456313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set_state! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mutable struct Adam\n",
    "    α :: Float32    # learning rate\n",
    "    β1 :: Float32   # First moment decay rate\n",
    "    β2 :: Float32   # Second moment decay rate\n",
    "    ε :: Float32    # Epsilon for numerical stability\n",
    "\n",
    "    #   Stan optymalizatora\n",
    "    m :: Dict{String, Matrix{Float32}}  # First moment estimate\n",
    "    v :: Dict{String, Matrix{Float32}}  # Second moment estimate\n",
    "    t :: Int  # Time step\n",
    "\n",
    "    #   Parametry optymalizatora\n",
    "    parameters :: Vector{Tuple{String, Variable}}   #   Wszystkie parametry (wagi i biasy)\n",
    "end\n",
    "\n",
    "function init!(order::Vector{Any}, α=0.001f0, β1=0.9f0, β2=0.999f0, ε=1e-8)\n",
    "    parameters = get_weights_and_biases(order)\n",
    "    \n",
    "    m = Dict{String, Matrix{Float32}}()\n",
    "    v = Dict{String, Matrix{Float32}}()\n",
    "    for (name, var) in parameters\n",
    "        m[name] = zeros(Float32, size(var.output))\n",
    "        v[name] = zeros(Float32, size(var.output))\n",
    "    end\n",
    "    return Adam(α, β1, β2, ε, m, v, 0, parameters)\n",
    "end\n",
    "\n",
    "function step!(optimizer::Adam)\n",
    "    optimizer.t += 1\n",
    "\n",
    "    for (name, var) in optimizer.parameters\n",
    "        g = var.gradient\n",
    "\n",
    "        #   Aktualizuj momenty\n",
    "        optimizer.m[name] = optimizer.β1 * optimizer.m[name] + (1 - optimizer.β1) * g\n",
    "        optimizer.v[name] = optimizer.β2 * optimizer.v[name] + (1 - optimizer.β2) * (g .^ 2)\n",
    "\n",
    "        #   Popraw momenty\n",
    "        m_corrected = optimizer.m[name] / (1 - optimizer.β1 ^ optimizer.t)\n",
    "        v_corrected = optimizer.v[name] / (1 - optimizer.β2 ^ optimizer.t)\n",
    "\n",
    "        #   Aktualizuj parametry\n",
    "        var.output .-= optimizer.α .* m_corrected ./ (sqrt.(v_corrected) .+ optimizer.ε)\n",
    "    end\n",
    "end\n",
    "\n",
    "function reset!(optimizer::Adam)\n",
    "    optimizer.t = 0\n",
    "    #  Reset momentów\n",
    "    for (name, var) in optimizer.parameters\n",
    "        optimizer.m[name] .= zeros(size(var.output))\n",
    "        optimizer.v[name] .= zeros(size(var.output))\n",
    "    end\n",
    "end\n",
    "\n",
    "function get_state(optimizer::Adam)\n",
    "    return Dict(\n",
    "        \"t\" => optimizer.t,\n",
    "        \"m\" => deepcopy(optimizer.m),\n",
    "        \"v\" => deepcopy(optimizer.v)\n",
    "    )\n",
    "end\n",
    "\n",
    "function set_state!(optimizer::Adam, state::Dict)\n",
    "    optimizer.t = state[\"t\"]\n",
    "    optimizer.m = state[\"m\"]\n",
    "    optimizer.v = state[\"v\"]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3686905b",
   "metadata": {},
   "source": [
    "## Test wejścia do neuronu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c1d7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2 Matrix{Float32}:\n",
       " 1.0  1.0\n",
       " 2.0  2.0\n",
       " 3.0  3.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = Variable(Float32.(reshape([1.0, 2.0, 3.0, 1.0, 2.0, 3.0], 3, 2)), name=\"x\")\n",
    "x.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb994942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3 Matrix{Float32}:\n",
       " 1.0  2.0  3.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = Variable(Float32.([1.0 2.0 3.0]), name=\"w\")\n",
    "w.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2e83b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"z\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = w * x\n",
    "z.name = \"z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6fc88b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topological order:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1-element Vector{Tuple{String, Variable}}:\n",
       " (\"w\", var w\n",
       " ┣━ ^ 1×3 Matrix{Float32}\n",
       " ┗━ ∇ 1×3 Matrix{Float32})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "order = topological_sort(z)\n",
    "println(\"Topological order:\")\n",
    "order\n",
    "weights = get_weights(order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63689cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×2 Matrix{Float32}:\n",
       " 14.0  14.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = forward!(order)\n",
    "z.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6544db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×1 Matrix{Float32}:\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "backward!(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7746c8e",
   "metadata": {},
   "source": [
    "## Test 2 szeregowych Neuronów - 1 warstwa + bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ca98db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×1 Matrix{Float32}:\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = Variable(Float32.([1.0 1.0; 2.0 2.0; 3.0 3.0]), name=\"x\")\n",
    "w = Variable(Float32.([2.0 4.0 6.0; 3.0 5.0 7.0]), name=\"w\")\n",
    "y = Constant(Float32.(reshape([1.0, 1.0], 1, 2)))\n",
    "z = w * x\n",
    "z.name = \"z\"\n",
    "# c = Constant(1.0)\n",
    "# d = z + c\n",
    "# dense_layer_2 = σ(z)\n",
    "# dense_layer_2.name = \"σ(z)\"\n",
    "dense_layer_2 = relu(z)\n",
    "dense_layer_2.name = \"relu(z)\"\n",
    "loss = binarycrossentropy(dense_layer_2, y)\n",
    "loss.name = \"binarycrossentropy\"\n",
    "order = topological_sort(loss)\n",
    "y = forward!(order)\n",
    "backward!(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34a9a0",
   "metadata": {},
   "source": [
    "## Test 2. warstw neuronów 2-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03ff6a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Vector{Any}:\n",
       " var w2\n",
       " ┣━ ^ 4×2 Matrix{Float32}\n",
       " ┗━ ∇ 4×2 Matrix{Float32}\n",
       " var w1\n",
       " ┣━ ^ 2×3 Matrix{Float32}\n",
       " ┗━ ∇ 2×3 Matrix{Float32}\n",
       " var x\n",
       " ┣━ ^ 3×1 Matrix{Float32}\n",
       " ┗━ ∇ 3×1 Matrix{Float32}\n",
       " op.a(typeof(mul!))\n",
       " op.b(typeof(relu))\n",
       " op.c(typeof(mul!))\n",
       " op.d(typeof(relu))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#   Pierwsza warstwa\n",
    "x = Variable(Float32.(reshape([1.0, 2.0, 3.0], 3, 1)), name=\"x\")\n",
    "w = Variable(Float32.(reshape([2.0, 3.0, 4.0, 5.0, 6.0, 7.0], 2, 3)), name=\"w1\")\n",
    "a = w * x\n",
    "a.name = \"a\"\n",
    "b = relu(a)\n",
    "b.name = \"b\"\n",
    "\n",
    "#   Druga warstwa\n",
    "w2 = Variable(Float32.(reshape([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0], 4,2)), name=\"w2\")\n",
    "c = w2 * b\n",
    "c.name = \"c\"\n",
    "d = relu(c)\n",
    "d.name = \"d\"\n",
    "order = topological_sort(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8583b5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×1 Matrix{Float32}:\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ŷ = forward!(order)\n",
    "backward!(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4798c5",
   "metadata": {},
   "source": [
    "## Test binary cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce454e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22314353f0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ŷ = Variable(Float32.(reshape([0.8], 1, 1)), name=\"ŷ\")\n",
    "y = Variable(Float32.(reshape([1.0], 1, 1)), name=\"y\")\n",
    "loss = binarycrossentropy(ŷ, y)\n",
    "loss.name = \"loss\"\n",
    "order = topological_sort(loss)\n",
    "result = forward!(order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2daa76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×1 Matrix{Float32}:\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "backward!(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb1e575",
   "metadata": {},
   "source": [
    "##  Test tworzenia modelu dla batch = 2 relu-sigmoid-bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4f5bb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"loss\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = Constant(Float32.([1.0 1.0; 2.0 1.0; 3.0 1.0]))\n",
    "w1 = Variable(Float32.([0.1 0.2 0.3; 0.4 0.5 0.6]), name=\"w1\")\n",
    "z1_mul = w1 * x\n",
    "z1_mul.name = \"z1_mul\"\n",
    "\n",
    "\n",
    "b1_matrix = zeros(Float32, 2, 1)\n",
    "b1_matrix[1,1] = 0.1f0\n",
    "b1_matrix[2,1] = 0.2f0\n",
    "b1 = Variable(b1_matrix, name=\"b1\")\n",
    "z1 = z1_mul + b1\n",
    "z1.name = \"z1\"\n",
    "\n",
    "a1 = relu(z1)\n",
    "a1.name = \"a1\"\n",
    "\n",
    "w2_matrix = zeros(Float32, 1, 2)\n",
    "w2_matrix[1,1] = 0.5f0\n",
    "w2_matrix[1,2] = -0.5f0\n",
    "w2 = Variable(w2_matrix, name=\"w2\")\n",
    "z2_mul = w2 * a1\n",
    "z2_mul.name = \"z2_mul\"\n",
    "\n",
    "b2_matrix = zeros(Float32, 1, 1)\n",
    "b2_matrix[1,1] = 0.0f0\n",
    "b2 = Variable(b2_matrix, name=\"b2\")\n",
    "z2 = z2_mul + b2\n",
    "\n",
    "ŷ = σ(z2)\n",
    "ŷ.name = \"ŷ\"\n",
    "\n",
    "y_matrix = zeros(Float32, 1, 2)\n",
    "y_matrix[1,1] = 1.0f0\n",
    "y_matrix[1,2] = 0.0f0\n",
    "y = Constant(y_matrix)\n",
    "\n",
    "loss = binarycrossentropy(ŷ, y)\n",
    "loss.name = \"loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c14403ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13-element Vector{Any}:\n",
       " var w2\n",
       " ┣━ ^ 1×2 Matrix{Float32}\n",
       " ┗━ ∇ Nothing\n",
       " var w1\n",
       " ┣━ ^ 2×3 Matrix{Float32}\n",
       " ┗━ ∇ Nothing\n",
       " const Float32[1.0 1.0; 2.0 1.0; 3.0 1.0]\n",
       " op.z1_mul(typeof(mul!))\n",
       " var b1\n",
       " ┣━ ^ 2×1 Matrix{Float32}\n",
       " ┗━ ∇ Nothing\n",
       " op.z1(typeof(+))\n",
       " op.a1(typeof(relu))\n",
       " op.z2_mul(typeof(mul!))\n",
       " var b2\n",
       " ┣━ ^ 1×1 Matrix{Float32}\n",
       " ┗━ ∇ Nothing\n",
       " op.?(typeof(+))\n",
       " op.ŷ(typeof(σ))\n",
       " const Float32[1.0 0.0]\n",
       " op loss(typeof(binary_cross_entropy_loss_impl))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "order = topological_sort(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7aa7dc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8755167f0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = forward!(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac6789bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×2 Matrix{Float32}:\n",
       " 0.5  -0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w2.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01fa666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward!(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c193818c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Tuple{String, Variable}}:\n",
       " (\"w2\", var w2\n",
       " ┣━ ^ 1×2 Matrix{Float32}\n",
       " ┗━ ∇ 1×2 Matrix{Float32})\n",
       " (\"w1\", var w1\n",
       " ┣━ ^ 2×3 Matrix{Float32}\n",
       " ┗━ ∇ 2×3 Matrix{Float32})\n",
       " (\"b1\", var b1\n",
       " ┣━ ^ 2×1 Matrix{Float32}\n",
       " ┗━ ∇ 2×1 Matrix{Float32})\n",
       " (\"b2\", var b2\n",
       " ┣━ ^ 1×1 Matrix{Float32}\n",
       " ┗━ ∇ 1×1 Matrix{Float32})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_weights(order)\n",
    "get_biases(order)\n",
    "get_gradients(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bbb5f4",
   "metadata": {},
   "source": [
    "## Iris Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10024aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×20 Matrix{Float32}:\n",
       " 0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  …  0.0  1.0  1.0  1.0  1.0  0.0  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using MLDatasets\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using Plots\n",
    "using DataFrames # Dodajmy pakiet DataFrame, bo wygląda na to, że jest używany\n",
    "using MLDataUtils\n",
    "\n",
    "# Załaduj zbiór danych Iris\n",
    "iris_features, iris_targets = Iris(as_df=false)[:]\n",
    "class1_name = \"Iris-setosa\"\n",
    "class2_name = \"Iris-versicolor\"\n",
    "iris_features_cut = iris_features[:, 1:100];\n",
    "iris_targets_cut = iris_targets[:, 1:100];\n",
    "\n",
    "label_mapping = Dict(\"Iris-setosa\" => 0.0, \"Iris-versicolor\" => 1.0)\n",
    "iris_targets_cut_classes = [label_mapping[class_name] for class_name in iris_targets_cut]\n",
    "iris_shuffled_all_x, iris_shuffled_all_y = shuffleobs((iris_features_cut, iris_targets_cut_classes));\n",
    "# Podział na zbiór treningowy i testowy (np. 80% train, 20% test)\n",
    "train_ratio = 0.8\n",
    "num_all_obs = size(iris_shuffled_all_x, 2)\n",
    "num_train_obs = floor(Int, num_all_obs * train_ratio)\n",
    "\n",
    "X_train = Float32.(iris_shuffled_all_x[:, 1:num_train_obs])\n",
    "y_train = Float32.(iris_shuffled_all_y[:, 1:num_train_obs]) # Zakładając, że y_train ma kształt (out, num_obs)\n",
    "\n",
    "X_test = Float32.(iris_shuffled_all_x[:, num_train_obs+1:end])\n",
    "y_test = Float32.(iris_shuffled_all_y[:, num_train_obs+1:end]) # Podobnie dla y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebcdbc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13-element Vector{Any}:\n",
       " var w2\n",
       " ┣━ ^ 1×8 Matrix{Float32}\n",
       " ┗━ ∇ 1×8 Matrix{Float32}\n",
       " var w1\n",
       " ┣━ ^ 8×4 Matrix{Float32}\n",
       " ┗━ ∇ 8×4 Matrix{Float32}\n",
       " const Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " op.z1_mul(typeof(mul!))\n",
       " var b1\n",
       " ┣━ ^ 8×1 Matrix{Float32}\n",
       " ┗━ ∇ 8×1 Matrix{Float32}\n",
       " op.z1(typeof(+))\n",
       " op.d1(typeof(relu))\n",
       " op.z2_mul(typeof(mul!))\n",
       " var b2\n",
       " ┣━ ^ 1×1 Matrix{Float32}\n",
       " ┗━ ∇ 1×1 Matrix{Float32}\n",
       " op.z2(typeof(+))\n",
       " op.ŷ(typeof(σ))\n",
       " const Float32[0.0 0.0 … 0.0 0.0]\n",
       " op loss(typeof(binary_cross_entropy_loss_impl))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#   Ustawienia sieci neuronowej\n",
    "features = 4\n",
    "hidden = 8\n",
    "out = 1\n",
    "epochs = 30\n",
    "batch_size = 10\n",
    "\n",
    "x = Constant(zeros(Float32, features, batch_size))\n",
    "w1 = Variable(xavier_uniform((hidden, features)); name=\"w1\")\n",
    "z1_mul = w1 * x\n",
    "z1_mul.name = \"z1_mul\"\n",
    "b1 = Variable(xavier_uniform((hidden, 1)); name=\"b1\")\n",
    "z1 = z1_mul + b1\n",
    "z1.name = \"z1\"\n",
    "d1 = relu(z1)\n",
    "d1.name = \"d1\"\n",
    "w2 = Variable(xavier_uniform((out, hidden)); name=\"w2\")\n",
    "z2_mul = w2 * d1\n",
    "z2_mul.name = \"z2_mul\"\n",
    "b2 = Variable(xavier_uniform((out, 1)); name=\"b2\")\n",
    "z2 = z2_mul + b2\n",
    "z2.name = \"z2\"\n",
    "ŷ = σ(z2)\n",
    "ŷ.name = \"ŷ\"\n",
    "y = Constant(zeros(Float32, out, batch_size))\n",
    "loss = binarycrossentropy(ŷ, y)\n",
    "loss.name = \"loss\"\n",
    "order = topological_sort(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceb8ed70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state of optimizer:\n",
      "Dict{String, Any}(\"v\" => Dict{String, Matrix{Float32}}(\"b2\" => [0.0;;], \"w2\" => [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0], \"w1\" => [0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0], \"b1\" => [0.0; 0.0; 0.0; 0.0; 0.0; 0.0; 0.0; 0.0;;]), \"m\" => Dict{String, Matrix{Float32}}(\"b2\" => [0.0;;], \"w2\" => [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0], \"w1\" => [0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0], \"b1\" => [0.0; 0.0; 0.0; 0.0; 0.0; 0.0; 0.0; 0.0;;]), \"t\" => 0)\n"
     ]
    }
   ],
   "source": [
    "#   Start ADAM   \n",
    "optimizer = init!(order)\n",
    "println(\"Initial state of optimizer:\")\n",
    "println(get_state(optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "456ef0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss after epoch 1: 0.39245194\n",
      "Epoch 2\n",
      "Loss after epoch 2: 0.36798656\n",
      "Epoch 3\n",
      "Loss after epoch 3: 0.393301\n",
      "Epoch 4\n",
      "Loss after epoch 4: 0.365246\n",
      "Epoch 5\n",
      "Loss after epoch 5: 0.36020437\n",
      "Epoch 6\n",
      "Loss after epoch 6: 0.3220604\n",
      "Epoch 7\n",
      "Loss after epoch 7: 0.35850725\n",
      "Epoch 8\n",
      "Loss after epoch 8: 0.3821807\n",
      "Epoch 9\n",
      "Loss after epoch 9: 0.32759959\n",
      "Epoch 10\n",
      "Loss after epoch 10: 0.31206957\n",
      "Epoch 11\n",
      "Loss after epoch 11: 0.3031282\n",
      "Epoch 12\n",
      "Loss after epoch 12: 0.31560922\n",
      "Epoch 13\n",
      "Loss after epoch 13: 0.26405737\n",
      "Epoch 14\n",
      "Loss after epoch 14: 0.3200249\n",
      "Epoch 15\n",
      "Loss after epoch 15: 0.28471595\n",
      "Epoch 16\n",
      "Loss after epoch 16: 0.2944382\n",
      "Epoch 17\n",
      "Loss after epoch 17: 0.24712132\n",
      "Epoch 18\n",
      "Loss after epoch 18: 0.25892696\n",
      "Epoch 19\n",
      "Loss after epoch 19: 0.25536734\n",
      "Epoch 20\n",
      "Loss after epoch 20: 0.2164693\n",
      "Epoch 21\n",
      "Loss after epoch 21: 0.24449864\n",
      "Epoch 22\n",
      "Loss after epoch 22: 0.21425393\n",
      "Epoch 23\n",
      "Loss after epoch 23: 0.21903259\n",
      "Epoch 24\n",
      "Loss after epoch 24: 0.20544605\n",
      "Epoch 25\n",
      "Loss after epoch 25: 0.21182384\n",
      "Epoch 26\n",
      "Loss after epoch 26: 0.20673756\n",
      "Epoch 27\n",
      "Loss after epoch 27: 0.18612152\n",
      "Epoch 28\n",
      "Loss after epoch 28: 0.21863167\n",
      "Epoch 29\n",
      "Loss after epoch 29: 0.19841632\n",
      "Epoch 30\n",
      "Loss after epoch 30: 0.1572816\n"
     ]
    }
   ],
   "source": [
    "num_training_samples = size(X_train, 2) # Liczba próbek w zbiorze treningowym\n",
    "\n",
    "\n",
    "#   Run training and visualize results\n",
    "loss_value = 0.0\n",
    "for epoch in 1:epochs\n",
    "    # --- Tasowanie zbioru treningowego NA NOWO w każdej epoce ---\n",
    "    permutation = randperm(num_training_samples)\n",
    "    X_train_shuffled_epoch = X_train[:, permutation]\n",
    "    y_train_shuffled_epoch = y_train[:, permutation]\n",
    "    num_batches = ceil(Int, num_training_samples / batch_size)\n",
    "\n",
    "    println(\"Epoch $epoch\")\n",
    "    for i in 1:num_batches\n",
    "        # Wybierz batch z POTASOWANYCH W TEJ EPOCE danych\n",
    "        start_idx = (i - 1) * batch_size + 1\n",
    "        end_idx = min(i * batch_size, num_training_samples)\n",
    "        x_batch = X_train_shuffled_epoch[:, start_idx:end_idx]\n",
    "        y_batch = y_train_shuffled_epoch[:, start_idx:end_idx]\n",
    "\n",
    "        x.output = x_batch\n",
    "        y.output = y_batch\n",
    "\n",
    "        forward!(order)\n",
    "\n",
    "        #println(\"ŷ: \", ŷ.output)\n",
    "        #println(\"y: \", y.output)\n",
    "    \n",
    "        loss_value = loss.output\n",
    "        backward!(order)\n",
    "        step!(optimizer)\n",
    "\n",
    "    end\n",
    "\n",
    "    println(\"Loss after epoch $epoch: \", loss_value)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b43eeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Test Evaluation ---\n",
      "Test Accuracy: 100.0 %\n",
      "\n",
      "Confusion Matrix:\n",
      "TP: 12, TN: 8, FP: 0, FN: 0\n",
      "Sum of CM components: 20 (Should be 20)\n",
      "Precision (for class 1): 1.0\n",
      "Recall (for class 1): 1.0\n",
      "F1 Score (for class 1): 1.0\n",
      "\n",
      "--- Test Evaluation Finished ---\n"
     ]
    }
   ],
   "source": [
    "println(\"\\n--- Starting Test Evaluation ---\")\n",
    "\n",
    "x.output = X_test\n",
    "y.output = y_test\n",
    "\n",
    "forward!(order)\n",
    "\n",
    "predictions_prob = ŷ.output\n",
    "\n",
    "predicted_classes = (predictions_prob .> 0.5) # Wynik to BitMatrix (1, liczba_próbek_testowych)\n",
    "\n",
    "true_classes = convert.(Bool, y_test) # Wynik to BitMatrix (1, liczba_próbek_testowych)\n",
    "\n",
    "# --- Obliczanie Dokładności ---\n",
    "# Najprostsze obliczenie powinno działać poprawnie na BitMatrix\n",
    "correct_predictions = sum(predicted_classes .== true_classes) # Sumuje true w macierzy wynikowej porównania element-wise\n",
    "\n",
    "total_test_samples = size(X_test, 2)\n",
    "accuracy = correct_predictions / total_test_samples\n",
    "\n",
    "println(\"Test Accuracy: $(accuracy * 100.0) %\")\n",
    "\n",
    "# --- Obliczanie Macierzy Pomyłek (Confusion Matrix) - POPRAWIONE ---\n",
    "# Używamy standardowych operatorów logicznych na macierzach Boolowskich\n",
    "# TP: predicted = true AND true = true\n",
    "TP = sum(predicted_classes .& true_classes)\n",
    "\n",
    "# TN: predicted = false AND true = false\n",
    "TN = sum(.!predicted_classes .& .!true_classes)\n",
    "\n",
    "# FP: predicted = true AND true = false\n",
    "FP = sum(predicted_classes .& .!true_classes)\n",
    "\n",
    "# FN: predicted = false AND true = true\n",
    "FN = sum(.!predicted_classes .& true_classes)\n",
    "\n",
    "println(\"\\nConfusion Matrix:\")\n",
    "println(\"TP: $(TP), TN: $(TN), FP: $(FP), FN: $(FN)\")\n",
    "\n",
    "# Ważna weryfikacja: Sprawdź, czy suma komponentów CM równa się liczbie próbek testowych\n",
    "println(\"Sum of CM components: $(TP + TN + FP + FN) (Should be $(total_test_samples))\")\n",
    "\n",
    "# Możesz teraz bezpiecznie obliczyć Precision, Recall, F1, używając tych (poprawionych) wartości TP, TN, FP, FN.\n",
    "if (TP + FP) > 0\n",
    "    precision = TP / (TP + FP)\n",
    "    println(\"Precision (for class 1): $(precision)\")\n",
    "else\n",
    "    println(\"Precision (for class 1): N/A (No positive predictions)\")\n",
    "end\n",
    "\n",
    "if (TP + FN) > 0\n",
    "    recall = TP / (TP + FN)\n",
    "     println(\"Recall (for class 1): $(recall)\")\n",
    "else\n",
    "     println(\"Recall (for class 1): N/A (No actual positive samples of class 1)\")\n",
    "end\n",
    "\n",
    "if (precision + recall) > 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    println(\"F1 Score (for class 1): $(f1_score)\")\n",
    "else\n",
    "    println(\"F1 Score (for class 1): N/A\")\n",
    "end\n",
    "\n",
    "\n",
    "println(\"\\n--- Test Evaluation Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7988cf76",
   "metadata": {},
   "source": [
    "## IMDB Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6679a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Matrix(load(\"../../dataset/imdb_dataset_prepared.jld2\", \"X_train\"));\n",
    "y_train = Matrix(load(\"../../dataset/imdb_dataset_prepared.jld2\", \"y_train\"));\n",
    "X_test = Matrix(load(\"../../dataset/imdb_dataset_prepared.jld2\", \"X_test\"));\n",
    "y_test = Matrix(load(\"../../dataset/imdb_dataset_prepared.jld2\", \"y_test\"));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01eb07d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13-element Vector{Any}:\n",
       " var w2\n",
       " ┣━ ^ 1×32 Matrix{Float32}\n",
       " ┗━ ∇ 1×32 Matrix{Float32}\n",
       " var w1\n",
       " ┣━ ^ 32×17703 Matrix{Float32}\n",
       " ┗━ ∇ 32×17703 Matrix{Float32}\n",
       " const Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n",
       " op.z1_mul(typeof(mul!))\n",
       " var b1\n",
       " ┣━ ^ 32×1 Matrix{Float32}\n",
       " ┗━ ∇ 32×1 Matrix{Float32}\n",
       " op.z1(typeof(+))\n",
       " op.d1(typeof(relu))\n",
       " op.z2_mul(typeof(mul!))\n",
       " var b2\n",
       " ┣━ ^ 1×1 Matrix{Float32}\n",
       " ┗━ ∇ 1×1 Matrix{Float32}\n",
       " op.z2(typeof(+))\n",
       " op.ŷ(typeof(σ))\n",
       " const Float32[0.0 0.0 … 0.0 0.0]\n",
       " op loss(typeof(binary_cross_entropy_loss_impl))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#   Ustawienia sieci neuronowej\n",
    "features = size(X_train, 1)\n",
    "hidden = 32\n",
    "out = 1\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "x = Constant(zeros(Float32, features, batch_size))\n",
    "w1 = Variable(xavier_uniform((hidden, features)); name=\"w1\")\n",
    "z1_mul = w1 * x\n",
    "z1_mul.name = \"z1_mul\"\n",
    "b1 = Variable(xavier_uniform((hidden, 1)); name=\"b1\")\n",
    "z1 = z1_mul + b1\n",
    "z1.name = \"z1\"\n",
    "d1 = relu(z1)\n",
    "d1.name = \"d1\"\n",
    "w2 = Variable(xavier_uniform((out, hidden)); name=\"w2\")\n",
    "z2_mul = w2 * d1\n",
    "z2_mul.name = \"z2_mul\"\n",
    "b2 = Variable(xavier_uniform((out, 1)); name=\"b2\")\n",
    "z2 = z2_mul + b2\n",
    "z2.name = \"z2\"\n",
    "ŷ = σ(z2)\n",
    "ŷ.name = \"ŷ\"\n",
    "y = Constant(zeros(Float32, out, batch_size))\n",
    "loss = binarycrossentropy(ŷ, y)\n",
    "loss.name = \"loss\"\n",
    "order = topological_sort(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed535b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam(0.001f0, 0.9f0, 0.999f0, 1.0f-8, Dict{String, Matrix{Float32}}(\"b2\" => [0.0;;], \"w2\" => [0.0 0.0 … 0.0 0.0], \"w1\" => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], \"b1\" => [0.0; 0.0; … ; 0.0; 0.0;;]), Dict{String, Matrix{Float32}}(\"b2\" => [0.0;;], \"w2\" => [0.0 0.0 … 0.0 0.0], \"w1\" => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], \"b1\" => [0.0; 0.0; … ; 0.0; 0.0;;]), 0, Tuple{String, Variable}[(\"w2\", var w2\n",
       " ┣━ ^ 1×32 Matrix{Float32}\n",
       " ┗━ ∇ 1×32 Matrix{Float32}), (\"w1\", var w1\n",
       " ┣━ ^ 32×17703 Matrix{Float32}\n",
       " ┗━ ∇ 32×17703 Matrix{Float32}), (\"b1\", var b1\n",
       " ┣━ ^ 32×1 Matrix{Float32}\n",
       " ┗━ ∇ 32×1 Matrix{Float32}), (\"b2\", var b2\n",
       " ┣━ ^ 1×1 Matrix{Float32}\n",
       " ┗━ ∇ 1×1 Matrix{Float32})])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#   Start ADAM   \n",
    "optimizer = init!(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b10799a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss after epoch 1 (4.87s): 0.64\n",
      "Epoch 2\n",
      "Loss after epoch 2 (1.42s): 0.55\n",
      "Epoch 3\n",
      "Loss after epoch 3 (1.39s): 0.48\n",
      "Epoch 4\n",
      "Loss after epoch 4 (1.45s): 0.30\n",
      "Epoch 5\n",
      "Loss after epoch 5 (1.40s): 0.26\n"
     ]
    }
   ],
   "source": [
    "using Printf\n",
    "num_training_samples = size(X_train, 2) # Liczba próbek w zbiorze treningowym\n",
    "\n",
    "\n",
    "#   Run training and visualize results\n",
    "loss_value = 0.0\n",
    "for epoch in 1:epochs\n",
    "    # --- Tasowanie zbioru treningowego NA NOWO w każdej epoce ---\n",
    "    permutation = randperm(num_training_samples)\n",
    "    X_train_shuffled_epoch = X_train[:, permutation]\n",
    "    y_train_shuffled_epoch = y_train[:, permutation]\n",
    "    num_batches = ceil(Int, num_training_samples / batch_size)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    t = @elapsed begin\n",
    "        println(\"Epoch $epoch\")\n",
    "        for i in 1:num_batches\n",
    "            # Wybierz batch z POTASOWANYCH W TEJ EPOCE danych\n",
    "            start_idx = (i - 1) * batch_size + 1\n",
    "            end_idx = min(i * batch_size, num_training_samples)\n",
    "            x_batch = X_train_shuffled_epoch[:, start_idx:end_idx]\n",
    "            y_batch = y_train_shuffled_epoch[:, start_idx:end_idx]\n",
    "    \n",
    "            x.output = x_batch\n",
    "            y.output = y_batch\n",
    "    \n",
    "            forward!(order)\n",
    "            backward!(order)\n",
    "            step!(optimizer)\n",
    "\n",
    "            num_samples = i\n",
    "        end\n",
    "    end\n",
    "    loss_value = loss.output\n",
    "\n",
    "    println(@sprintf(\"Loss after epoch %d (%.2fs): %.2f\", epoch, t, loss_value))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49c632fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Test Evaluation ---\n",
      "Test Accuracy: 86.45 %\n",
      "\n",
      "Confusion Matrix:\n",
      "TP: 924, TN: 805, FP: 170, FN: 101\n",
      "Sum of CM components: 2000 (Should be 2000)\n",
      "Precision (for class 1): 0.8446069469835467\n",
      "Recall (for class 1): 0.9014634146341464\n",
      "F1 Score (for class 1): 0.8721094856064181\n",
      "\n",
      "--- Test Evaluation Finished ---\n"
     ]
    }
   ],
   "source": [
    "println(\"\\n--- Starting Test Evaluation ---\")\n",
    "\n",
    "x.output = X_test\n",
    "y.output = y_test\n",
    "\n",
    "forward!(order)\n",
    "\n",
    "predictions_prob = ŷ.output\n",
    "\n",
    "predicted_classes = (predictions_prob .> 0.5) # Wynik to BitMatrix (1, liczba_próbek_testowych)\n",
    "\n",
    "true_classes = convert.(Bool, y_test) # Wynik to BitMatrix (1, liczba_próbek_testowych)\n",
    "\n",
    "# --- Obliczanie Dokładności ---\n",
    "# Najprostsze obliczenie powinno działać poprawnie na BitMatrix\n",
    "correct_predictions = sum(predicted_classes .== true_classes) # Sumuje true w macierzy wynikowej porównania element-wise\n",
    "\n",
    "total_test_samples = size(X_test, 2)\n",
    "accuracy = correct_predictions / total_test_samples\n",
    "\n",
    "println(\"Test Accuracy: $(accuracy * 100.0) %\")\n",
    "\n",
    "# --- Obliczanie Macierzy Pomyłek (Confusion Matrix) - POPRAWIONE ---\n",
    "# Używamy standardowych operatorów logicznych na macierzach Boolowskich\n",
    "# TP: predicted = true AND true = true\n",
    "TP = sum(predicted_classes .& true_classes)\n",
    "\n",
    "# TN: predicted = false AND true = false\n",
    "TN = sum(.!predicted_classes .& .!true_classes)\n",
    "\n",
    "# FP: predicted = true AND true = false\n",
    "FP = sum(predicted_classes .& .!true_classes)\n",
    "\n",
    "# FN: predicted = false AND true = true\n",
    "FN = sum(.!predicted_classes .& true_classes)\n",
    "\n",
    "println(\"\\nConfusion Matrix:\")\n",
    "println(\"TP: $(TP), TN: $(TN), FP: $(FP), FN: $(FN)\")\n",
    "\n",
    "# Ważna weryfikacja: Sprawdź, czy suma komponentów CM równa się liczbie próbek testowych\n",
    "println(\"Sum of CM components: $(TP + TN + FP + FN) (Should be $(total_test_samples))\")\n",
    "\n",
    "# Możesz teraz bezpiecznie obliczyć Precision, Recall, F1, używając tych (poprawionych) wartości TP, TN, FP, FN.\n",
    "if (TP + FP) > 0\n",
    "    precision = TP / (TP + FP)\n",
    "    println(\"Precision (for class 1): $(precision)\")\n",
    "else\n",
    "    println(\"Precision (for class 1): N/A (No positive predictions)\")\n",
    "end\n",
    "\n",
    "if (TP + FN) > 0\n",
    "    recall = TP / (TP + FN)\n",
    "     println(\"Recall (for class 1): $(recall)\")\n",
    "else\n",
    "     println(\"Recall (for class 1): N/A (No actual positive samples of class 1)\")\n",
    "end\n",
    "\n",
    "if (precision + recall) > 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    println(\"F1 Score (for class 1): $(f1_score)\")\n",
    "else\n",
    "    println(\"F1 Score (for class 1): N/A\")\n",
    "end\n",
    "\n",
    "\n",
    "println(\"\\n--- Test Evaluation Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.0",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
