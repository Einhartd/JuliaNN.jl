{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f1ba62",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3221369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m      Compat\u001b[22m\u001b[39m entries added for \n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Repos/AWiD/MyMlp/Manifest.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mProject\u001b[22m\u001b[39m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Repos/AWiD/MyMlp`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyMlp v0.1.0\n",
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/Repos/AWiD/MyMlp/Project.toml`\n",
      "  \u001b[90m[6e4b80f9] \u001b[39mBenchmarkTools v1.6.0\n"
     ]
    }
   ],
   "source": [
    "# Make sure we're using the latest version\n",
    "import Pkg\n",
    "Pkg.add(\"BenchmarkTools\")\n",
    "Pkg.activate(\"../..\")  # Activate the project environment\n",
    "Pkg.instantiate()      # Install any missing dependencies\n",
    "Pkg.status()          # Check if MyMlp is listed\n",
    "\n",
    "# Now try importing\n",
    "using BenchmarkTools\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb89e6c",
   "metadata": {},
   "source": [
    "## Comparison of different optimizations to Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc8f4feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.11/Manifest.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Benchmark: READ access ###\n",
      "Original:\n",
      "  14.034 ns (1 allocation: 16 bytes)\n",
      "Optimized:\n",
      "  1.575 ns (0 allocations: 0 bytes)\n",
      "RefValue:\n",
      "  1.793 ns (0 allocations: 0 bytes)\n",
      "\n",
      "### Benchmark: WRITE mutation ###\n",
      "Original:\n",
      "  14.417 ns (1 allocation: 16 bytes)\n",
      "Optimized:\n",
      "  2.013 ns (0 allocations: 0 bytes)\n",
      "RefValue:\n",
      "  2.013 ns (0 allocations: 0 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500503.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"BenchmarkTools\")\n",
    "using BenchmarkTools\n",
    "\n",
    "abstract type GraphNode end\n",
    "abstract type Operator <: GraphNode end\n",
    "\n",
    "# Original implementation\n",
    "mutable struct VariableOriginal <: GraphNode\n",
    "    output :: Any\n",
    "    grad :: Any\n",
    "    name :: String\n",
    "    VariableOriginal(output; name=\"?\") = new(output, nothing, name)\n",
    "end\n",
    "\n",
    "# Optimized implementation\n",
    "mutable struct VariableOptimized{T<:Float64} <: GraphNode\n",
    "    output :: T\n",
    "    grad :: Union{Nothing, T}\n",
    "    name :: String\n",
    "    VariableOptimized(output::T; name=\"?\") where T<:Float64 = new{T}(output, nothing, name)\n",
    "end\n",
    "\n",
    "# RefValue-based immutable implementation\n",
    "struct VariableRef <: GraphNode\n",
    "    output :: Base.RefValue{Float64}\n",
    "    grad   :: Union{Nothing, Base.RefValue{Float64}}\n",
    "    name   :: String\n",
    "    function VariableRef(output::Float64; name=\"?\")\n",
    "        new(Base.RefValue(output), nothing, name)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Functions to benchmark: reading\n",
    "function read_output(v::VariableOriginal)\n",
    "    v.output + 1.0\n",
    "end\n",
    "\n",
    "function read_output(v::VariableOptimized)\n",
    "    v.output + 1.0\n",
    "end\n",
    "\n",
    "function read_output(v::VariableRef)\n",
    "    v.output[] + 1.0\n",
    "end\n",
    "\n",
    "# Functions to benchmark: writing\n",
    "function write_output!(v::VariableOriginal)\n",
    "    v.output = v.output + 1.0\n",
    "end\n",
    "\n",
    "function write_output!(v::VariableOptimized)\n",
    "    v.output = v.output + 1.0\n",
    "end\n",
    "\n",
    "function write_output!(v::VariableRef)\n",
    "    v.output[] = v.output[] + 1.0\n",
    "end\n",
    "\n",
    "# Create instances\n",
    "v1 = VariableOriginal(1.0)\n",
    "v2 = VariableOptimized(1.0)\n",
    "v3 = VariableRef(1.0)\n",
    "\n",
    "# Benchmark READ\n",
    "println(\"### Benchmark: READ access ###\")\n",
    "println(\"Original:\")\n",
    "@btime read_output($v1)\n",
    "\n",
    "println(\"Optimized:\")\n",
    "@btime read_output($v2)\n",
    "\n",
    "println(\"RefValue:\")\n",
    "@btime read_output($v3)\n",
    "\n",
    "# Benchmark WRITE\n",
    "println(\"\\n### Benchmark: WRITE mutation ###\")\n",
    "println(\"Original:\")\n",
    "@btime write_output!($v1)\n",
    "\n",
    "println(\"Optimized:\")\n",
    "@btime write_output!($v2)\n",
    "\n",
    "println(\"RefValue:\")\n",
    "@btime write_output!($v3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4694cbbe",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c212c68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 5 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Base: *, +, clamp, log, exp\n",
    "import LinearAlgebra: mul!\n",
    "import Statistics: sum\n",
    "\n",
    "abstract type GraphNode end\n",
    "abstract type Operator <: GraphNode end\n",
    "\n",
    "# Definition of basic structures for computational graph\n",
    "struct Constant{T<:AbstractVecOrMat{Float64}} <: GraphNode\n",
    "    output :: T\n",
    "end\n",
    "\n",
    "mutable struct Variable{T<:AbstractVecOrMat{Float64}} <: GraphNode\n",
    "    output :: T\n",
    "    gradient :: Union{Nothing, T}\n",
    "    name :: String\n",
    "    \n",
    "    Variable(output::T; name=\"?\") where {T<:AbstractVecOrMat{Float64}} = new{T}(output, nothing, name)\n",
    "end\n",
    "\n",
    "mutable struct ScalarOperator{F} <: Operator\n",
    "    inputs :: Union{Nothing, Tuple{GraphNode, GraphNode}}\n",
    "    output :: Union{Nothing, Float64}\n",
    "    gradient :: Union{Nothing, Float64}\n",
    "    name :: String\n",
    "    ScalarOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "end\n",
    "\n",
    "mutable struct BroadcastedOperator{F} <: Operator\n",
    "    inputs :: Union{Nothing, Tuple{GraphNode, GraphNode}, Tuple{GraphNode}}\n",
    "    output :: Union{Nothing, AbstractVecOrMat{Float64}}\n",
    "    gradient :: Union{Nothing, Matrix{Float64}}\n",
    "    name :: String\n",
    "    BroadcastedOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "end\n",
    "\n",
    "\n",
    "import Base: show, summary\n",
    "show(io::IO, x::ScalarOperator{F}) where {F} = print(io, \"op \", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::BroadcastedOperator{F}) where {F} = print(io, \"op.\", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::Constant) = print(io, \"const \", x.output)\n",
    "show(io::IO, x::Variable) = begin\n",
    "    print(io, \"var \", x.name);\n",
    "    print(io, \"\\n ┣━ ^ \"); summary(io, x.output)\n",
    "    print(io, \"\\n ┗━ ∇ \");  summary(io, x.gradient)\n",
    "end\n",
    "\n",
    "\n",
    "function visit(node::GraphNode, visited, order)\n",
    "    if node ∈ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function visit(node::Operator, visited, order)\n",
    "    if node ∈ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        for input in node.inputs\n",
    "            visit(input, visited, order)\n",
    "        end\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function topological_sort(head::GraphNode)\n",
    "    visited = Set()\n",
    "    order = Vector()\n",
    "    visit(head, visited, order)\n",
    "    return order\n",
    "end\n",
    "\n",
    "\n",
    "# x * y (aka matrix multiplication)\n",
    "*(A::GraphNode, x::GraphNode) = BroadcastedOperator(mul!, A, x)\n",
    "forward(::BroadcastedOperator{typeof(mul!)}, A, x) = return A * x\n",
    "backward(::BroadcastedOperator{typeof(mul!)}, A, x, g) = tuple(g * x', A' * g)\n",
    "\n",
    "# relu activation\n",
    "relu(x::GraphNode) = BroadcastedOperator(relu, x)\n",
    "forward(::BroadcastedOperator{typeof(relu)}, x) = return x .* (x .> 0.0)\n",
    "backward(::BroadcastedOperator{typeof(relu)}, x, g) = tuple(g .* (x .> 0.0), nothing)\n",
    "\n",
    "# add operation (for bias)\n",
    "+(x::GraphNode, y::GraphNode) = BroadcastedOperator(+, x, y)\n",
    "forward(::BroadcastedOperator{typeof(+)}, x, y) = return x .+ y\n",
    "backward(::BroadcastedOperator{typeof(+)}, x, y, g) = begin\n",
    "    grad_wrt_x = g\n",
    "    # Gradient biasu: sumowanie gradientu g wzdłuż wymiaru batcha (dims=1)\n",
    "    # i przekształcenie wyniku 1xN do wektora N-elementowego.\n",
    "    grad_wrt_y = vec(sum(g, dims=2))\n",
    "    return (grad_wrt_x, grad_wrt_y)\n",
    "end\n",
    "\n",
    "# sigmoid activation\n",
    "σ(x::GraphNode) = BroadcastedOperator(σ, x)\n",
    "forward(::BroadcastedOperator{typeof(σ)}, x) = return 1.0 ./ (1.0 .+ exp.(-x))\n",
    "backward(node::BroadcastedOperator{typeof(σ)}, x, g) = begin\n",
    "    y = node.output\n",
    "    local_derivative = y .* (1.0 .- y)\n",
    "    grad_wrt_x = g .* local_derivative\n",
    "    return (grad_wrt_x, nothing)\n",
    "end\n",
    "\n",
    "# # binarycrossentropy\n",
    "# binarycrossentropy(y::GraphNode, ŷ::GraphNode) = ScalarOperator(binarycrossentropy, y, ŷ)\n",
    "# forward(::ScalarOperator{typeof(binarycrossentropy)}, ŷ, y) = begin\n",
    "#     return -mean(y .* log.(ŷ) + (1.0 .- y) .* log.(1.0 .- ŷ))\n",
    "# end\n",
    "# backward(::ScalarOperator{typeof(binarycrossentropy)}, ŷ, y, g) = begin\n",
    "#     J = (ŷ .- y) ./ (ŷ .* (1.0 .- ŷ))\n",
    "#     return (J * g, nothing)\n",
    "# end\n",
    "\n",
    "function binary_cross_entropy_loss_impl(ŷ, y_true; epsilon=1e-10)\n",
    "    ŷ_clamped = clamp.(ŷ, epsilon, 1.0 - epsilon)\n",
    "    loss_elements = -y_true .* log.(ŷ_clamped) .- (1.0 .- y_true) .* log.(1.0 .- ŷ_clamped)\n",
    "    return mean(loss_elements)\n",
    "end\n",
    "\n",
    "binarycrossentropy(ŷ::GraphNode, y::GraphNode) = ScalarOperator(binary_cross_entropy_loss_impl, ŷ, y)\n",
    "\n",
    "forward(::ScalarOperator{typeof(binary_cross_entropy_loss_impl)}, ŷ_value, y_value) = begin\n",
    "    loss_value = binary_cross_entropy_loss_impl(ŷ_value, y_value)\n",
    "    return loss_value\n",
    "end\n",
    "\n",
    "backward(::ScalarOperator{typeof(binary_cross_entropy_loss_impl)}, ŷ_value, y_value, g) = begin\n",
    "    epsilon = 1e-10\n",
    "    ŷ_clamped_for_grad = clamp.(ŷ_value, epsilon, 1.0 - epsilon)\n",
    "    local_grad_per_sample = (ŷ_clamped_for_grad .- y_value) ./ (ŷ_clamped_for_grad .* (1.0 .- ŷ_clamped_for_grad))\n",
    "    batch_size = size(y_value, 2)\n",
    "    grad_wrt_ŷ = local_grad_per_sample ./ batch_size\n",
    "    return (grad_wrt_ŷ, nothing)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f27fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reset!(node::Constant) = nothing\n",
    "\n",
    "reset!(node::Variable) = node.gradient = nothing\n",
    "reset!(node::Operator) = node.gradient = nothing\n",
    "\n",
    "compute!(node::Constant) = nothing\n",
    "compute!(node::Variable) = nothing\n",
    "\n",
    "compute!(node::Operator) =\n",
    "    node.output = forward(node, [input.output for input in node.inputs]...)\n",
    "\n",
    "function forward!(order::Vector)\n",
    "    #   Iteruje przez każdy węzeł w order.\n",
    "    for node in order\n",
    "        compute!(node)\n",
    "        reset!(node)\n",
    "    end\n",
    "    return last(order).output\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c495933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "update!(node::Constant, gradient) = nothing\n",
    "\n",
    "update!(node::GraphNode, gradient) = if isnothing(node.gradient)\n",
    "    node.gradient = gradient else node.gradient .+= gradient\n",
    "end\n",
    "\n",
    "\n",
    "function backward!(order::Vector; seed=1.0)\n",
    "    result = last(order)   #   The output node\n",
    "\n",
    "    if isa(result.output, Matrix{Float64})\n",
    "        result.gradient = ones(Float64, size(result.output))\n",
    "    else\n",
    "        result.gradient = seed\n",
    "        @assert length(result.output) == 1 \"Gradient is defined only for scalar functions\"\n",
    "    end\n",
    "\n",
    "    for node in reverse(order)   #   Iterate through nodes in reverse topological order.\n",
    "        backward!(node)   #   Compute and propagate gradients backwards.\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function backward!(node::Constant) end\n",
    "function backward!(node::Variable) end\n",
    "\n",
    "function backward!(node::Operator)\n",
    "    inputs = node.inputs\n",
    "\n",
    "    gradients = backward(node, [input.output for input in inputs]..., node.gradient)\n",
    "\n",
    "    for (input, gradient) in zip(inputs, gradients)\n",
    "        update!(input, gradient)\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99233471",
   "metadata": {},
   "source": [
    "## Optymalizator ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2783950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct AdamOptimizer\n",
    "    α::Float64          # Learning rate\n",
    "    β1::Float64         # Decay rate for first moment\n",
    "    β2::Float64         # Decay rate for second moment\n",
    "    ϵ::Float64          # Small constant for numerical stability\n",
    "    t::Int              # Iteration counter\n",
    "\n",
    "    # Stany m i v dla każdego parametru\n",
    "    # IdDict pozwala na użycie obiektów jako kluczy (np. zmiennych parametrów)\n",
    "    m_states::IdDict{Any, Any} # Mapa: parametr -> stan m (ta sama struktura co parametr)\n",
    "    v_states::IdDict{Any, Any} # Mapa: parametr -> stan v (ta sama struktura co parametr)\n",
    "\n",
    "    # Konstruktor optymalizatora\n",
    "    function AdamOptimizer(α=0.001, β1=0.9, β2=0.999, ϵ=1e-8)\n",
    "        new(α, β1, β2, ϵ, 0, IdDict(), IdDict())\n",
    "    end\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3686905b",
   "metadata": {},
   "source": [
    "## Test wejścia do neuronu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85c1d7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2 Matrix{Float64}:\n",
       " 1.0  1.0\n",
       " 2.0  2.0\n",
       " 3.0  3.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = Variable(reshape([1.0, 2.0, 3.0, 1.0, 2.0, 3.0], 3, 2), name=\"x\")\n",
    "x.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb994942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3 Matrix{Float64}:\n",
       " 1.0  2.0  3.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = Variable([1.0 2.0 3.0], name=\"w\")\n",
    "w.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2e83b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"z\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = w * x\n",
    "z.name = \"z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6fc88b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topological order:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Vector{Any}:\n",
       " var w\n",
       " ┣━ ^ 1×3 Matrix{Float64}\n",
       " ┗━ ∇ Nothing\n",
       " var x\n",
       " ┣━ ^ 3×2 Matrix{Float64}\n",
       " ┗━ ∇ Nothing\n",
       " op.z(typeof(mul!))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "order = topological_sort(z)\n",
    "println(\"Topological order:\")\n",
    "order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63689cc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `forward!` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `forward!` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Repos/AWiD/MyMlp/src/notebooks/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X16sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "y = forward!(order)\n",
    "z.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6544db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3 Matrix{Float64}:\n",
       " 2.0  4.0  6.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "backward!(order)\n",
    "w.gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7746c8e",
   "metadata": {},
   "source": [
    "## Test 2 szeregowych Neuronów - 1 warstwa + bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ca98db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable([1.0 1.0; 2.0 2.0; 3.0 3.0], name=\"x\")\n",
    "w = Variable([2.0 4.0 6.0; 3.0 5.0 7.0], name=\"w\")\n",
    "y = Constant(reshape([1.0, 1.0], 1, 2))\n",
    "z = w * x\n",
    "z.name = \"z\"\n",
    "# c = Constant(1.0)\n",
    "# d = z + c\n",
    "# dense_layer_2 = σ(z)\n",
    "# dense_layer_2.name = \"σ(z)\"\n",
    "dense_layer_2 = relu(z)\n",
    "dense_layer_2.name = \"relu(z)\"\n",
    "loss = binarycrossentropy(dense_layer_2, y)\n",
    "loss.name = \"binarycrossentropy\"\n",
    "order = topological_sort(loss)\n",
    "y = forward!(order)\n",
    "backward!(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34a9a0",
   "metadata": {},
   "source": [
    "## Test 2. warstw neuronów 2-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff6a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Vector{Any}:\n",
       " var w2\n",
       " ┣━ ^ 4×2 Matrix{Float64}\n",
       " ┗━ ∇ Nothing\n",
       " var w1\n",
       " ┣━ ^ 2×3 Matrix{Float64}\n",
       " ┗━ ∇ Nothing\n",
       " var x\n",
       " ┣━ ^ 3×1 Matrix{Float64}\n",
       " ┗━ ∇ Nothing\n",
       " op.a(typeof(mul!))\n",
       " op.b(typeof(relu))\n",
       " op.c(typeof(mul!))\n",
       " op.d(typeof(relu))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#   Pierwsza warstwa\n",
    "x = Variable(reshape([1.0, 2.0, 3.0], 3, 1), name=\"x\")\n",
    "w = Variable(reshape([2.0, 3.0, 4.0, 5.0, 6.0, 7.0], 2, 3), name=\"w1\")\n",
    "a = w * x\n",
    "a.name = \"a\"\n",
    "b = relu(a)\n",
    "b.name = \"b\"\n",
    "\n",
    "#   Druga warstwa\n",
    "w2 = Variable(reshape([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0], 4,2), name=\"w2\")\n",
    "c = w2 * b\n",
    "c.name = \"c\"\n",
    "d = relu(c)\n",
    "d.name = \"d\"\n",
    "order = topological_sort(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8583b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ = forward!(order)\n",
    "backward!(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4798c5",
   "metadata": {},
   "source": [
    "## Test binary cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce454e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2231435513142097"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ŷ = Variable(reshape([0.8], 1, 1), name=\"ŷ\")\n",
    "y = Variable(reshape([1.0], 1, 1), name=\"y\")\n",
    "loss = binarycrossentropy(ŷ, y)\n",
    "loss.name = \"loss\"\n",
    "order = topological_sort(loss)\n",
    "result = forward!(order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2daa76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward!(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb1e575",
   "metadata": {},
   "source": [
    "##  Test tworzenia modelu dla batch = 2 relu-sigmoid-bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4f5bb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"loss\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = Constant([1.0 1.0; 2.0 1.0; 3.0 1.0])\n",
    "w1 = Variable([0.1 0.2 0.3; 0.4 0.5 0.6], name=\"w1\")\n",
    "z1_mul = w1 * x\n",
    "z1_mul.name = \"z1_mul\"\n",
    "\n",
    "b1 = Variable([0.1; 0.2], name=\"b1\")\n",
    "z1 = z1_mul + b1\n",
    "z1.name = \"z1\"\n",
    "\n",
    "a1 = relu(z1)\n",
    "a1.name = \"a1\"\n",
    "w2 = Variable([0.5 -0.5], name=\"w2\")\n",
    "z2_mul = w2 * a1\n",
    "z2_mul.name = \"z2_mul\"\n",
    "\n",
    "b2 = Variable([0.0], name=\"b2\")\n",
    "z2 = z2_mul + b2\n",
    "\n",
    "ŷ = σ(z2)\n",
    "ŷ.name = \"ŷ\"\n",
    "\n",
    "y = Constant([1.0 0.0])\n",
    "\n",
    "loss = binarycrossentropy(ŷ, y)\n",
    "loss.name = \"loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c14403ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13-element Vector{Any}:\n",
       " var w2\n",
       " ┣━ ^ 1×2 Matrix{Float64}\n",
       " ┗━ ∇ Nothing\n",
       " var w1\n",
       " ┣━ ^ 2×3 Matrix{Float64}\n",
       " ┗━ ∇ Nothing\n",
       " const [1.0 1.0; 2.0 1.0; 3.0 1.0]\n",
       " op.z1_mul(typeof(mul!))\n",
       " var b1\n",
       " ┣━ ^ 2-element Vector{Float64}\n",
       " ┗━ ∇ Nothing\n",
       " op.z1(typeof(+))\n",
       " op.a1(typeof(relu))\n",
       " op.z2_mul(typeof(mul!))\n",
       " var b2\n",
       " ┣━ ^ 1-element Vector{Float64}\n",
       " ┗━ ∇ Nothing\n",
       " op.?(typeof(+))\n",
       " op.ŷ(typeof(σ))\n",
       " const [1.0 0.0]\n",
       " op loss(typeof(binary_cross_entropy_loss_impl))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "order = topological_sort(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aa7dc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8755166955155294"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = forward!(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac6789bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×2 Matrix{Float64}:\n",
       " 0.5  -0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w2.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01fa666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward!(order)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.0",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
