{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ab3d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../MyReverseDiff.jl\")\n",
    "include(\"../MyEmbedding.jl\")\n",
    "include(\"../MyMlp.jl\")\n",
    "include(\"../TensorOperations.jl\")\n",
    "\n",
    "using .MyReverseDiff\n",
    "using .MyMlp\n",
    "using JLD2\n",
    "using Printf\n",
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "664c1218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8.460 ms (416268 allocations: 16.03 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50×130×64 Array{Float32, 3}:\n",
       "[:, :, 1] =\n",
       " -3.04443     0.504028  -1.16704   …   1.13953    -1.68675     4.42868\n",
       "  0.486429    0.89772   -2.84683       0.965314   -2.42314     6.73673\n",
       "  0.377292   -1.1309    -1.28232       2.21007    -2.58822     2.32216\n",
       "  3.48555     2.25395    0.651317      4.12646    -1.53565     2.9489\n",
       " -0.230689   -0.787347  -2.49989       3.45516     2.93407     5.79568\n",
       "  1.00348    -0.243322   1.78666   …   7.31157    -4.01358     2.08687\n",
       " -1.80243    -2.03204   -1.44316       0.75721     1.64116     2.7814\n",
       "  0.481479    1.87765   -1.94287       1.31597     3.42113     1.53179\n",
       " -1.33072     3.30427   -0.260654     -0.366261   -0.995645    1.52391\n",
       "  2.05919     0.697039  -2.45927       2.81124    -2.98661     2.66104\n",
       "  ⋮                                ⋱                          \n",
       "  2.21238    -0.697261  -3.5851       -0.373332   -3.18817    -2.49045\n",
       " -0.0881289  -0.244008  -3.77782      -1.2236      0.0500635  -2.94695\n",
       " -0.877933    0.554768  -1.35706      -1.25992    -1.82258    -4.98683\n",
       "  0.234409    0.623165   0.268131     -0.0742391   2.23548     0.0492471\n",
       " -3.36627     0.487738  -1.24282   …  -4.30228     0.311942   -0.281158\n",
       " -2.02096    -1.51675   -4.24062       2.1576      7.44937    -2.12312\n",
       "  0.165625    1.40488   -0.247946      3.26464     1.78857    -1.26619\n",
       "  0.97244    -0.631624  -2.26559       1.3976      1.45415     0.426833\n",
       "  1.15225     0.28976    0.925628      4.85474     0.353525   -3.72276\n",
       "\n",
       "[:, :, 2] =\n",
       " -0.413459   -0.0503795   0.403747  …   1.93196   -1.36469   -1.03648\n",
       " -2.64175     1.99808    -0.102343      2.59015    1.15754   -1.85733\n",
       "  0.440912   -0.486494    3.65789       1.63551   -1.52642    5.03345\n",
       " -0.0231509  -1.34292     1.71892       2.16433    5.57731    1.37039\n",
       "  3.31742    -1.99727    -1.46892       1.81122    3.23446    0.226092\n",
       " -2.14292    -2.30105    -3.16723   …  -3.35825   -0.609045  -2.42475\n",
       " -0.632329   -0.315773    1.04392      -0.777049  -0.346145  -3.66815\n",
       " -1.01255     1.1062      0.852322      0.954371  -3.81439   -4.08146\n",
       " -4.38614     1.57628    -0.688112     -0.818094   1.42179   -1.51494\n",
       " -1.68932    -0.902958    2.04471       1.63258    1.61461    0.0392834\n",
       "  ⋮                                 ⋱                        \n",
       "  0.32142    -1.40484    -3.584         2.09518    0.242928   3.71199\n",
       " -1.35258     0.315495    0.273647      1.27082   -1.00247   -0.819984\n",
       " -1.9348      1.75136    -1.66544      -0.927035   1.14334    0.4339\n",
       " -3.68734    -1.06609    -0.751552     -0.783956   3.36329   -3.2652\n",
       "  1.13491     0.181592    0.902941  …   3.25766    2.50501   -2.13154\n",
       " -0.502456    0.458112   -2.88495       4.68168    0.711842   0.507104\n",
       "  5.03042    -0.118602   -1.09515       1.97468   -0.135216   2.14147\n",
       "  0.0766872  -0.0475578   0.960431     -0.818041  -6.8378     0.287131\n",
       "  2.40068     3.33719     0.360094     -4.66811    3.64294    1.88655\n",
       "\n",
       "[:, :, 3] =\n",
       "  0.602856    2.94179   -3.0063     1.06567     …  -0.972633     3.57383\n",
       "  0.313916   -0.635136  -1.58509    1.02897         2.30108      5.43749\n",
       "  3.4716      2.85265    1.21233    1.34377        -4.01912      0.612472\n",
       " -0.0434484   5.63105    2.88898    1.78027         0.345934    -3.2402\n",
       "  2.30993     2.80725    1.38701   -1.56607        -4.23796      2.21895\n",
       " -4.27887     1.07971    1.19804   -1.1138      …   0.00061521   2.52668\n",
       " -5.91231     5.5574    -0.479999  -4.16667        -3.5472       1.58853\n",
       " -5.8946      3.06884   -1.07733    0.067256       -3.42144      1.1148\n",
       " -3.29859    -1.24213    0.899834  -3.06679        -2.72404      0.00647959\n",
       " -2.00767    -4.30339    3.54499   -2.00565         0.702156     1.61204\n",
       "  ⋮                                             ⋱               \n",
       " -4.06927    -0.937344   1.13261    2.39292         2.6955      -4.34157\n",
       " -1.02195    -3.12017    1.73434   -1.13731        -0.193694    -0.386382\n",
       " -2.38052     2.25776   -2.80449    0.970211        2.1082      -2.36864\n",
       " -1.49174     1.7128    -0.534621  -0.206764        1.51774      0.766618\n",
       " -4.59803     1.14417    0.96743   -4.50302     …   1.5972      -0.406653\n",
       " -3.01744    -2.86467    1.97425   -0.00819993      0.925416     1.93662\n",
       " -2.30154    -4.71934    2.97305    0.071313        1.9641      -1.59135\n",
       "  1.0987     -1.31716    5.08621   -1.6498          1.12017      1.83864\n",
       "  1.47535    -0.792268   2.97643   -0.744578        0.137246    -2.10824\n",
       "\n",
       ";;; … \n",
       "\n",
       "[:, :, 62] =\n",
       "  0.0333415   1.01526   -3.89462    …   2.33782   -0.421257   -2.16501\n",
       " -0.411255    3.20575    3.21937        2.47079    2.06623     4.18551\n",
       "  0.382471    3.74488    1.32315       -2.53519    1.69428    -0.370294\n",
       "  0.977561    3.39471    4.25211       -0.67859    4.53519     0.740559\n",
       "  0.503172    3.01855    2.0917        -3.50355    2.55422    -1.384\n",
       "  2.05345    -2.14926    0.999857   …   4.10624    2.08194     1.41812\n",
       "  0.064923   -2.46154    3.57905       -0.554001  -1.74823     1.62482\n",
       " -3.7379     -0.502594   0.434415       0.355166   1.37172     6.23037\n",
       " -1.07303     0.559069  -2.71543       -0.846302   1.77051     4.38668\n",
       " -0.903701   -1.22951   -2.15103        5.15444    3.4719      2.42473\n",
       "  ⋮                                 ⋱                         \n",
       " -2.58208    -1.82151   -0.129937      -2.7727     3.28187    -3.70684\n",
       " -0.902265    0.44634   -2.20652        1.33877   -3.9956     -2.0437\n",
       "  3.30288     1.13419   -0.200879      -0.314157  -4.65034     1.85434\n",
       "  2.78705     4.98102    1.7884        -0.680779   0.567018   -0.398837\n",
       "  1.60354     2.54666    0.286401   …   0.404598   1.4939      0.0667037\n",
       " -1.80553     1.78303   -0.0813379     -0.700688   0.342158   -0.426992\n",
       "  0.204029    1.95682   -3.17703       -0.859897  -2.45719     0.250596\n",
       "  1.44938     3.7525     3.45388        1.08432    0.0597261   0.562716\n",
       "  0.176985   -0.156485   0.562768       4.85093   -3.93993    -1.91106\n",
       "\n",
       "[:, :, 63] =\n",
       "  0.999346   2.8901    -1.7833    …   0.727461   -0.300512   -4.40561\n",
       "  1.54441    1.15059   -3.06815       1.48641     2.8956      0.214087\n",
       " -2.53162    3.99739   -0.663979      2.46156    -2.59899    -1.97154\n",
       " -2.04299    2.53718   -0.558862     -1.77615     2.95782     3.82045\n",
       " -0.852689  -0.133484   0.283355      0.394709   -1.1347      2.16101\n",
       "  0.970728  -0.487285  -1.59883   …   0.550083   -1.82015     2.51033\n",
       " -4.37544   -0.857121  -3.89847       2.95172    -0.119561    2.19062\n",
       "  1.28195    0.129098  -1.70738       1.41215    -0.859581   -0.184452\n",
       "  1.54038    1.80065   -0.467212     -0.87264    -0.703686   -1.5101\n",
       "  4.12593   -1.42517    3.16104       2.31694     1.99835    -1.78295\n",
       "  ⋮                               ⋱                          \n",
       " -2.75365   -3.57272   -0.607877      0.0219599  -0.910676    2.83\n",
       "  3.1854    -1.45374   -0.55261      -3.01392    -3.27687    -5.57665\n",
       "  0.382188  -0.817615  -1.29933       3.99862    -0.816787   -0.535661\n",
       "  0.101294  -0.860053  -1.54331       2.25611    -0.930616   -0.245814\n",
       " -3.93217    0.357175  -1.67072   …   8.31854     1.90243     1.71608\n",
       " -1.5153    -0.099961   1.92144       0.425953    3.68608     1.9377\n",
       " -2.6321     1.59299    0.721246      5.68731    -1.89532     0.545094\n",
       " -1.00033   -1.04179    1.82582       0.890451    0.293841    4.08521\n",
       " -5.47529    2.88426    0.433167      0.357181    0.0290646  -2.8307\n",
       "\n",
       "[:, :, 64] =\n",
       "  3.61441   -3.7361      2.62768   …  -2.74686    1.59678    -1.78933\n",
       " -2.04401    1.96131     1.10439      -3.01944    2.83826    -3.0016\n",
       "  0.342924  -2.22116     5.08556      -1.97482    4.79164    -3.16385\n",
       "  0.150125   0.0807886   5.19264      -1.02694   -5.0413     -5.75183\n",
       "  3.19661    0.208874   -2.70485      -2.00866   -1.28761     0.147182\n",
       "  0.530307   0.930668   -0.102281  …  -3.82808   -0.652651   -0.705952\n",
       " -0.773545   3.04165    -1.84798      -2.54805   -2.75838     1.37862\n",
       " -1.64055    6.06175    -0.202162      1.22955   -1.89057     0.822095\n",
       "  0.328914   0.856267   -3.39151       2.25692   -2.6759     -0.97875\n",
       "  2.42961    3.91342    -4.06456       0.526898  -8.47764    -0.663903\n",
       "  ⋮                                ⋱                         \n",
       " -1.52174    0.24095    -2.23358      -1.69481    4.29024     0.578164\n",
       "  1.58189   -0.86687    -0.516061      1.66281    1.931       3.67656\n",
       "  0.738538   0.910046   -1.57082      -1.41842    0.0477426   2.08021\n",
       "  2.32896   -1.89319    -1.68112      -0.471946  -2.31952     4.79303\n",
       "  0.798382  -2.26999    -2.13292   …  -0.598607   4.8217     -2.10389\n",
       "  2.37524   -3.90853     1.69458      -1.58303    0.322729    2.05144\n",
       "  1.47102   -1.36933    -1.30128      -0.741939  -1.20966     1.14179\n",
       "  0.631016  -3.01009    -0.47157       1.48253   -0.88978     4.65947\n",
       " -1.47256   -1.48561     2.80456      -0.547898  -1.84261    -0.38293"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = randn(Float32,50,130,64)\n",
    "m = randn(Float32,3,1)\n",
    "g = ones(Float32,size(x,1),size(x,2)*size(m,2),size(x,3))\n",
    "\n",
    "using BenchmarkTools\n",
    "#@btime MyReverseDiff.dif_convolution(x,m,g)\n",
    "@btime MyReverseDiff.multi_convolution(x,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8402976d",
   "metadata": {},
   "source": [
    "- 9.833 ms (416835 allocations: 25.48 MiB)\n",
    "- 9.533 ms (416646 allocations: 23.85 MiB)\n",
    "- 8.691 ms (416457 allocations: 19.16 MiB)\n",
    "- 8.274 ms (416268 allocations: 16.03 MiB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ade2272",
   "metadata": {},
   "source": [
    "-  284.843 ms (851209 allocations: 545.02 MiB)\n",
    "- 269.293 ms (851017 allocations: 541.87 MiB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74424a3",
   "metadata": {},
   "source": [
    "## Przygotowanie danych IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2a09dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = load(\"../../dataset/imdb_dataset_prepared.jld2\", \"X_train\");\n",
    "y_train = load(\"../../dataset/imdb_dataset_prepared.jld2\", \"y_train\");\n",
    "X_test = load(\"../../dataset/imdb_dataset_prepared.jld2\", \"X_test\");\n",
    "y_test = load(\"../../dataset/imdb_dataset_prepared.jld2\", \"y_test\");\n",
    "embeddings = load(\"../../dataset/imdb_dataset_prepared.jld2\", \"embeddings\")\n",
    "# vocab = load(\"../../dataset/imdb_dataset_prepared.jld2\", \"vocab\");\n",
    "\n",
    "input_size = size(X_train,1) # Liczba cech\n",
    "embedding_dim = size(embeddings,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e89f70",
   "metadata": {},
   "source": [
    "##  Trening modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc20ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "model = Chain(\n",
    "    Embedding(embeddings, name = \"embedding\"),\n",
    "    ConvolutionBlock(1,3,name=\"layer1\"),\n",
    "    Dense3D(embedding_dim, 8, relu, name=\"dense1\"),\n",
    "    PoolingBlock(8),\n",
    "    FlattenBlock(name=\"flatten\"),\n",
    "    Dense(input_size, 1, σ, name=\"softnet\")\n",
    ")\n",
    "\n",
    "#   Utworzenie początkowych węzłów Constant dla danych wejściowych i etykiet\n",
    "x_input_node = Constant(zeros(Float32, input_size, batch_size))\n",
    "y_label_node = Constant(zeros(Float32, 1, batch_size))\n",
    "\n",
    "#   Budowanie grafu treningowego\n",
    "loss_node, model_output_node, order = build_graph!(model, binarycrossentropy, x_input_node, y_label_node; loss_name=\"loss\")\n",
    "\n",
    "optimizer_state = setup_optimizer(Adam(), model)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "println(\"--- Rozpoczynam profilowanie treningu ---\")\n",
    "\n",
    "for epoch in 1:epochs\n",
    "    permutation = randperm(size(X_train, 2))\n",
    "    X_train_shuffled_epoch = X_train[:,permutation]\n",
    "    y_train_shuffled_epoch = y_train[:, permutation]\n",
    "    num_batches = ceil(Int, size(X_train, 2) / batch_size)\n",
    "\n",
    "    loss_value = 0.0\n",
    "\n",
    "    println(\"\\nEpoch: $epoch\")\n",
    "    total_batch_time = 0.0\n",
    "    total_batch_alloc = 0\n",
    "    total_batch_gc_time = 0.0\n",
    "\n",
    "    for i in 1:num_batches\n",
    "        start_idx = (i - 1) * batch_size + 1\n",
    "        end_idx = min(i * batch_size, size(X_train, 2))\n",
    "        x_batch_view = view(X_train_shuffled_epoch, :,  start_idx:end_idx)\n",
    "        y_batch_view = view(y_train_shuffled_epoch, :, start_idx:end_idx)\n",
    "\n",
    "        current_batch_size = size(x_batch_view, 2)\n",
    "        view(x_input_node.output, :, 1:current_batch_size) .= x_batch_view\n",
    "        view(y_label_node.output, :, 1:current_batch_size) .= y_batch_view\n",
    "\n",
    "        stats = @timed begin # `timed` zwraca strukturę z wynikami, `time` tylko czas\n",
    "            forward!(order)\n",
    "            backward!(order)\n",
    "            step!(optimizer_state) # Zakładam, że masz już zaimplementowane step!\n",
    "        end\n",
    "        loss_value += loss_node.output # Upewnij się, że loss_node.output jest odświeżane po forward\n",
    "\n",
    "        total_batch_time += stats.time\n",
    "        total_batch_alloc += stats.bytes\n",
    "        total_batch_gc_time += stats.gctime\n",
    "    end\n",
    "    \n",
    "    avg_loss_epoch = loss_value / num_batches\n",
    "\n",
    "    println(@sprintf(\"Epoch: %d \\tTrain: (l: %.4f) \\tTotal Batch Time: %.4fs \\tTotal Alloc: %s \\tGC Time: %.4fs\", \n",
    "                     epoch, avg_loss_epoch, total_batch_time, Base.format_bytes(total_batch_alloc), total_batch_gc_time))\n",
    "end\n",
    "\n",
    "println(\"\\n--- Koniec profilowania treningu ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122bc3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test Evaluation ---\n",
    "\n",
    "batch_size = 64\n",
    "num_test_samples = size(X_test, 2)\n",
    "num_batches = ceil(Int, num_test_samples / batch_size)\n",
    "total_test_loss_sum = 0.0\n",
    "total_correct_predictions = 0.0\n",
    "\n",
    "t_test = @elapsed begin\n",
    "    for i in 1:num_batches\n",
    "\n",
    "        start_idx = (i - 1) * batch_size + 1\n",
    "        end_idx = min(i * batch_size, num_test_samples)\n",
    "        x_batch_test = X_test[:, start_idx:end_idx]\n",
    "        y_batch_test = y_test[:, start_idx:end_idx]\n",
    "\n",
    "        # Aktualna liczba próbek w bieżącym batchu (może być mniejsza dla ostatniego batcha)\n",
    "        current_test_batch_size = size(x_batch_test, 2)\n",
    "\n",
    "        view(x_input_node.output, :, 1:current_test_batch_size) .= x_batch_test\n",
    "        view(y_label_node.output, :, 1:current_test_batch_size) .= y_batch_test\n",
    "\n",
    "        forward!(order)\n",
    "\n",
    "        predictions = view(model_output_node.output, :, 1:current_test_batch_size)\n",
    "\n",
    "\n",
    "        batch_loss = loss_node.output\n",
    "        \n",
    "        total_test_loss_sum += batch_loss * current_test_batch_size # Sumuj stratę, uwzględniając rozmiar batcha\n",
    "\n",
    "        # --- Oblicz dokładność na bieżącym batchu testowym ---\n",
    "        # Dla klasyfikacji binarnej z progiem 0.5 (lub innym, w zależności od problemu)\n",
    "        batch_accuracy = sum((predictions .> 0.5f0) .== y_batch_test) / current_test_batch_size\n",
    "        total_correct_predictions += batch_accuracy * current_test_batch_size # Sumuj poprawne predykcje\n",
    "    end\n",
    "end\n",
    "\n",
    "# --- Oblicz średnią stratę i średnią dokładność na całym zbiorze testowym ---\n",
    "avg_test_loss = total_test_loss_sum / num_test_samples\n",
    "avg_test_accuracy = total_correct_predictions / num_test_samples * 100.0\n",
    "\n",
    "println(@sprintf(\"Test Loss (czas: %.2fs): %.4f\", t_test, avg_test_loss))\n",
    "println(\"Test Accuracy: $avg_test_accuracy %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
