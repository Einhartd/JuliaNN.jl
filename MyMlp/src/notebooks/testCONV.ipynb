{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ab3d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../MyReverseDiff.jl\")\n",
    "include(\"../MyEmbedding.jl\")\n",
    "include(\"../MyMlp.jl\")\n",
    "include(\"../TensorOperations.jl\")\n",
    "\n",
    "using .MyReverseDiff\n",
    "using .MyMlp\n",
    "using .TensorOperations\n",
    "using JLD2\n",
    "using Printf\n",
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "664c1218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  62.341 ms (18260 allocations: 107.17 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Float32[1.0645225 1.0645225 … 1.0645225 1.0645225; 0.16612083 0.16612083 … 0.16612083 0.16612083; … ; 0.07603661 0.07603661 … 0.07603661 0.07603661; 0.07603661 0.07603661 … 0.07603661 0.07603661;;; 1.0645225 1.0645225 … 1.0645225 1.0645225; 0.16612083 0.16612083 … 0.16612083 0.16612083; … ; 0.07603661 0.07603661 … 0.07603661 0.07603661; 0.07603661 0.07603661 … 0.07603661 0.07603661;;; 1.0645225 1.0645225 … 1.0645225 1.0645225; 0.16612083 0.16612083 … 0.16612083 0.16612083; … ; 0.07603661 0.07603661 … 0.07603661 0.07603661; 0.07603661 0.07603661 … 0.07603661 0.07603661;;; … ;;; 1.0645225 1.0645225 … 1.0645225 1.0645225; 0.16612083 0.16612083 … 0.16612083 0.16612083; … ; 0.07603661 0.07603661 … 0.07603661 0.07603661; 0.07603661 0.07603661 … 0.07603661 0.07603661;;; 1.0645225 1.0645225 … 1.0645225 1.0645225; 0.16612083 0.16612083 … 0.16612083 0.16612083; … ; 0.07603661 0.07603661 … 0.07603661 0.07603661; 0.07603661 0.07603661 … 0.07603661 0.07603661;;; 1.0645225 1.0645225 … 1.0645225 1.0645225; 0.16612083 0.16612083 … 0.16612083 0.16612083; … ; 0.07603661 0.07603661 … 0.07603661 0.07603661; 0.07603661 0.07603661 … 0.07603661 0.07603661], Float32[-19.101608; -17.904005; -16.720213;;])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = randn(Float32, 50, 130, 64)\n",
    "m = randn(Float32, 3, 1)\n",
    "g = ones(Float32, size(x, 1), size(x, 2) * size(m, 2), size(x, 3))\n",
    "\n",
    "using BenchmarkTools\n",
    "@btime MyReverseDiff.dif_convolution(x, m, g)\n",
    "#@btime MyReverseDiff.multi_convolution(x,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8402976d",
   "metadata": {},
   "source": [
    "- 9.833 ms (416835 allocations: 25.48 MiB)\n",
    "- 9.533 ms (416646 allocations: 23.85 MiB)\n",
    "- 8.691 ms (416457 allocations: 19.16 MiB)\n",
    "- 8.274 ms (416268 allocations: 16.03 MiB)\n",
    "- 2.293 ms (268 allocations: 3.33 MiB)\n",
    "- 2.170 ms (79 allocations: 1.76 MiB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ade2272",
   "metadata": {},
   "source": [
    "- 284.843 ms (851209 allocations: 545.02 MiB)\n",
    "- 269.293 ms (851017 allocations: 541.87 MiB)\n",
    "- 297.682 ms (19017 allocations: 516.48 MiB)\n",
    "- 265.192 ms (18825 allocations: 514.88 MiB)\n",
    "- 250.897 ms (18831 allocations: 518.13 MiB)\n",
    "- 184.586 ms (18447 allocations: 310.23 MiB)\n",
    "- 107.900 ms (18068 allocations: 105.58 MiB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74424a3",
   "metadata": {},
   "source": [
    "## Przygotowanie danych IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2a09dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = load(\"../../dataset/imdb_dataset_prepared.jld2\", \"X_train\");\n",
    "y_train = load(\"../../dataset/imdb_dataset_prepared.jld2\", \"y_train\");\n",
    "X_test = load(\"../../dataset/imdb_dataset_prepared.jld2\", \"X_test\");\n",
    "y_test = load(\"../../dataset/imdb_dataset_prepared.jld2\", \"y_test\");\n",
    "embeddings = load(\"../../dataset/imdb_dataset_prepared.jld2\", \"embeddings\")\n",
    "# vocab = load(\"../../dataset/imdb_dataset_prepared.jld2\", \"vocab\");\n",
    "\n",
    "input_size = size(X_train, 1) # Liczba cech\n",
    "embedding_dim = size(embeddings, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e89f70",
   "metadata": {},
   "source": [
    "##  Trening modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb6a008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "model = Chain(\n",
    "    Embedding(embeddings, name=\"embedding\"),\n",
    "    #TransposeBlock(),\n",
    "    ConvolutionBlock(1, 3, name=\"layer1\"),\n",
    "    #TransposeBlock(),\n",
    "    Dense3D(embedding_dim, 8, relu, name=\"dense1\"),\n",
    "    PoolingBlock(8),\n",
    "    FlattenBlock(name=\"flatten\"),\n",
    "    Dense(input_size, 1, σ, name=\"softnet\")\n",
    ")\n",
    "\n",
    "#   Utworzenie początkowych węzłów Constant dla danych wejściowych i etykiet\n",
    "x_input_node = Constant(zeros(Float32, input_size, batch_size))\n",
    "y_label_node = Constant(zeros(Float32, 1, batch_size))\n",
    "\n",
    "#   Budowanie grafu treningowego\n",
    "loss_node, model_output_node, order = build_graph!(model, binarycrossentropy, x_input_node, y_label_node; loss_name=\"loss\")\n",
    "\n",
    "optimizer_state = setup_optimizer(Adam(a=0.003f0), model)\n",
    "\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed06e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ustawienie wag\n",
    "order[5].output             # Maska konwolucji\n",
    "order[7].output             # Dense3D wagi\n",
    "order[8].output             # Dense3D bias\n",
    "order[14].inputs[1].output  # Dense wagi\n",
    "order[15].output            # Dense bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc20ef9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Rozpoczynam profilowanie treningu ---\n",
      "\n",
      "Epoch: 1\n",
      "Epoch: 1 \tTrain: (l: 0.7049) \tTotal Batch Time: 84.8330s \tTotal Alloc: 99.666 GiB \tGC Time: 17.8597s\n",
      "\n",
      "Epoch: 2\n",
      "Epoch: 2 \tTrain: (l: 0.6798) \tTotal Batch Time: 77.4106s \tTotal Alloc: 98.785 GiB \tGC Time: 16.8643s\n",
      "\n",
      "Epoch: 3\n",
      "Epoch: 3 \tTrain: (l: 0.6548) \tTotal Batch Time: 79.1648s \tTotal Alloc: 98.785 GiB \tGC Time: 16.7144s\n",
      "\n",
      "Epoch: 4\n",
      "Epoch: 4 \tTrain: (l: 0.5979) \tTotal Batch Time: 77.8049s \tTotal Alloc: 98.785 GiB \tGC Time: 16.0937s\n",
      "\n",
      "Epoch: 5\n",
      "Epoch: 5 \tTrain: (l: 0.4985) \tTotal Batch Time: 87.1895s \tTotal Alloc: 98.785 GiB \tGC Time: 17.7275s\n",
      "\n",
      "--- Koniec profilowania treningu ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "println(\"--- Rozpoczynam profilowanie treningu ---\")\n",
    "\n",
    "for epoch in 1:epochs\n",
    "    permutation = randperm(size(X_train, 2))\n",
    "    X_train_shuffled_epoch = X_train[:, permutation]\n",
    "    y_train_shuffled_epoch = y_train[:, permutation]\n",
    "    num_batches = ceil(Int, size(X_train, 2) / batch_size)\n",
    "\n",
    "    loss_value = 0.0\n",
    "\n",
    "    println(\"\\nEpoch: $epoch\")\n",
    "    total_batch_time = 0.0\n",
    "    total_batch_alloc = 0\n",
    "    total_batch_gc_time = 0.0\n",
    "\n",
    "    for i in 1:num_batches\n",
    "        start_idx = (i - 1) * batch_size + 1\n",
    "        end_idx = min(i * batch_size, size(X_train, 2))\n",
    "        x_batch_view = view(X_train_shuffled_epoch, :, start_idx:end_idx)\n",
    "        y_batch_view = view(y_train_shuffled_epoch, :, start_idx:end_idx)\n",
    "\n",
    "        current_batch_size = size(x_batch_view, 2)\n",
    "        view(x_input_node.output, :, 1:current_batch_size) .= x_batch_view\n",
    "        view(y_label_node.output, :, 1:current_batch_size) .= y_batch_view\n",
    "\n",
    "        stats = @timed begin # `timed` zwraca strukturę z wynikami, `time` tylko czas\n",
    "            forward!(order)\n",
    "            backward!(order)\n",
    "            step!(optimizer_state) # Zakładam, że masz już zaimplementowane step!\n",
    "        end\n",
    "        loss_value += loss_node.output # Upewnij się, że loss_node.output jest odświeżane po forward\n",
    "\n",
    "        total_batch_time += stats.time\n",
    "        total_batch_alloc += stats.bytes\n",
    "        total_batch_gc_time += stats.gctime\n",
    "    end\n",
    "\n",
    "    avg_loss_epoch = loss_value / num_batches\n",
    "\n",
    "    println(@sprintf(\"Epoch: %d \\tTrain: (l: %.4f) \\tTotal Batch Time: %.4fs \\tTotal Alloc: %s \\tGC Time: %.4fs\",\n",
    "        epoch, avg_loss_epoch, total_batch_time, Base.format_bytes(total_batch_alloc), total_batch_gc_time))\n",
    "end\n",
    "\n",
    "println(\"\\n--- Koniec profilowania treningu ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "122bc3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss (czas: 1.46s): 0.5637\n",
      "Test Accuracy: 72.28 %\n"
     ]
    }
   ],
   "source": [
    "# --- Test Evaluation ---\n",
    "\n",
    "batch_size = 64\n",
    "num_test_samples = size(X_test, 2)\n",
    "num_batches = ceil(Int, num_test_samples / batch_size)\n",
    "total_test_loss_sum = 0.0\n",
    "total_correct_predictions = 0.0\n",
    "\n",
    "t_test = @elapsed begin\n",
    "    for i in 1:num_batches\n",
    "\n",
    "        start_idx = (i - 1) * batch_size + 1\n",
    "        end_idx = min(i * batch_size, num_test_samples)\n",
    "        x_batch_test = X_test[:, start_idx:end_idx]\n",
    "        y_batch_test = y_test[:, start_idx:end_idx]\n",
    "\n",
    "        # Aktualna liczba próbek w bieżącym batchu (może być mniejsza dla ostatniego batcha)\n",
    "        current_test_batch_size = size(x_batch_test, 2)\n",
    "\n",
    "        view(x_input_node.output, :, 1:current_test_batch_size) .= x_batch_test\n",
    "        view(y_label_node.output, :, 1:current_test_batch_size) .= y_batch_test\n",
    "\n",
    "        forward!(order)\n",
    "\n",
    "        predictions = view(model_output_node.output, :, 1:current_test_batch_size)\n",
    "\n",
    "\n",
    "        batch_loss = loss_node.output\n",
    "\n",
    "        total_test_loss_sum += batch_loss * current_test_batch_size # Sumuj stratę, uwzględniając rozmiar batcha\n",
    "\n",
    "        # --- Oblicz dokładność na bieżącym batchu testowym ---\n",
    "        # Dla klasyfikacji binarnej z progiem 0.5 (lub innym, w zależności od problemu)\n",
    "        batch_accuracy = sum((predictions .> 0.5f0) .== y_batch_test) / current_test_batch_size\n",
    "        total_correct_predictions += batch_accuracy * current_test_batch_size # Sumuj poprawne predykcje\n",
    "    end\n",
    "end\n",
    "\n",
    "# --- Oblicz średnią stratę i średnią dokładność na całym zbiorze testowym ---\n",
    "avg_test_loss = total_test_loss_sum / num_test_samples\n",
    "avg_test_accuracy = total_correct_predictions / num_test_samples * 100.0\n",
    "\n",
    "println(@sprintf(\"Test Loss (czas: %.2fs): %.4f\", t_test, avg_test_loss))\n",
    "println(\"Test Accuracy: $avg_test_accuracy %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec18a0",
   "metadata": {},
   "source": [
    "## Perfomnance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4339f19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "perform_test (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Statistics\n",
    "# test params\n",
    "s = 10\n",
    "cut = 64 * s\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "function performance(k)\n",
    "    results = zeros(Float32,k)\n",
    "    for i=1:k\n",
    "        results[i]=perform_test()\n",
    "    end\n",
    "    println(\"K: \",k)\n",
    "    println(\"Results: \", results)\n",
    "    println(\"Avarage: \", mean(results))\n",
    "    println(\"Var: \", var(results))\n",
    "end\n",
    "\n",
    "function perform_test()\n",
    "\n",
    "    X_train = load(\"../../dataset/imdb_dataset_prepared.jld2\", \"X_train\")[:, 1:cut]\n",
    "    y_train = load(\"../../dataset/imdb_dataset_prepared.jld2\", \"y_train\")[:, 1:cut]\n",
    "    X_test = load(\"../../dataset/imdb_dataset_prepared.jld2\", \"X_test\")\n",
    "    y_test = load(\"../../dataset/imdb_dataset_prepared.jld2\", \"y_test\")\n",
    "    embeddings = load(\"../../dataset/imdb_dataset_prepared.jld2\", \"embeddings\")\n",
    "    # vocab = load(\"../../dataset/imdb_dataset_prepared.jld2\", \"vocab\");\n",
    "\n",
    "    input_size = size(X_train, 1) # Liczba cech\n",
    "    embedding_dim = size(embeddings, 1)\n",
    "\n",
    "    model = Chain(\n",
    "        Embedding(embeddings, name=\"embedding\"),\n",
    "        ConvolutionBlock(1, 3, name=\"layer1\"),\n",
    "        Dense3D(embedding_dim, 8, relu, name=\"dense1\"),\n",
    "        PoolingBlock(8),\n",
    "        FlattenBlock(name=\"flatten\"),\n",
    "        Dense(input_size, 1, σ, name=\"softnet\")\n",
    "    )\n",
    "\n",
    "    #   Utworzenie początkowych węzłów Constant dla danych wejściowych i etykiet\n",
    "    x_input_node = Constant(zeros(Float32, input_size, batch_size))\n",
    "    y_label_node = Constant(zeros(Float32, 1, batch_size))\n",
    "\n",
    "    #   Budowanie grafu treningowego\n",
    "    loss_node, model_output_node, order = build_graph!(model, binarycrossentropy, x_input_node, y_label_node; loss_name=\"loss\")\n",
    "\n",
    "    optimizer_state = setup_optimizer(Adam(), model)\n",
    "    avg_loss_epoch = 0.0f0\n",
    "    for epoch in 1:epochs\n",
    "        permutation = randperm(size(X_train, 2))\n",
    "        X_train_shuffled_epoch = X_train[:, permutation]\n",
    "        y_train_shuffled_epoch = y_train[:, permutation]\n",
    "        num_batches = ceil(Int, size(X_train, 2) / batch_size)\n",
    "\n",
    "        loss_value = 0.0\n",
    "\n",
    "        total_batch_time = 0.0\n",
    "        total_batch_alloc = 0\n",
    "        total_batch_gc_time = 0.0\n",
    "\n",
    "        for i in 1:num_batches\n",
    "            start_idx = (i - 1) * batch_size + 1\n",
    "            end_idx = min(i * batch_size, size(X_train, 2))\n",
    "            x_batch_view = view(X_train_shuffled_epoch, :, start_idx:end_idx)\n",
    "            y_batch_view = view(y_train_shuffled_epoch, :, start_idx:end_idx)\n",
    "\n",
    "            current_batch_size = size(x_batch_view, 2)\n",
    "            view(x_input_node.output, :, 1:current_batch_size) .= x_batch_view\n",
    "            view(y_label_node.output, :, 1:current_batch_size) .= y_batch_view\n",
    "\n",
    "            stats = @timed begin # `timed` zwraca strukturę z wynikami, `time` tylko czas\n",
    "                forward!(order)\n",
    "                backward!(order)\n",
    "                step!(optimizer_state) # Zakładam, że masz już zaimplementowane step!\n",
    "            end\n",
    "            loss_value += loss_node.output # Upewnij się, że loss_node.output jest odświeżane po forward\n",
    "\n",
    "            total_batch_time += stats.time\n",
    "            total_batch_alloc += stats.bytes\n",
    "            total_batch_gc_time += stats.gctime\n",
    "        end\n",
    "\n",
    "        avg_loss_epoch = loss_value / num_batches\n",
    "\n",
    "    end\n",
    "    avg_loss_epoch\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4177c3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 10\n",
      "Results: Float32[0.9373993, 0.73377645, 0.7593586, 0.95225406, 0.8279859, 0.80444765, 0.9878423, 1.1164854, 0.88346535, 0.69636226]\n",
      "Avarage: 0.8699378\n",
      "Var: 0.017015807\n"
     ]
    }
   ],
   "source": [
    "performance(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11bcae",
   "metadata": {},
   "source": [
    "### Normal\n",
    "K: 10\n",
    "Avarage: 0.6230623\n",
    "Var: 0.0007078699\n",
    "### Transpose block\n",
    "K: 10\n",
    "Avarage: 0.7378746\n",
    "Var: 0.0013544216\n",
    "### Bach resize\n",
    "K: 10\n",
    "Avarage: 0.6238495\n",
    "Var: 0.0006545016\n",
    "### No embedding training\n",
    "K: 10\n",
    "Avarage: 0.72589546\n",
    "Var: 0.001047738\n",
    "### Newest\n",
    "K: 10\n",
    "Avarage: 0.6180767\n",
    "Var: 0.00036857463\n",
    "### Learning\n",
    "K: 10\n",
    "Avarage: 0.4358875\n",
    "Var: 0.003978955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882a73f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
