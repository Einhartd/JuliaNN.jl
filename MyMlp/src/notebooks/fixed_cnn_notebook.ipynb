{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test CNN dla NLP - analiza sekwencji słów\n",
    "\n",
    "Ten notebook testuje CNN 1D na analizie prostych sekwencji \"słów\" (tokenów), podobnie jak w Flux do zadań NLP. Używamy bardzo małych danych, które można policzyć ręcznie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importowanie modułów i bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moduły załadowane pomyślnie!\n"
     ]
    }
   ],
   "source": [
    "include(\"../MyReverseDiff.jl\")\n",
    "include(\"../MyEmbedding.jl\")\n",
    "include(\"../MyMlp.jl\")\n",
    "\n",
    "using .MyReverseDiff\n",
    "using .MyEmbedding\n",
    "using .MyMlp\n",
    "using Printf\n",
    "using LinearAlgebra\n",
    "using Random\n",
    "\n",
    "# Ustaw seed dla reprodukowalności\n",
    "Random.seed!(42)\n",
    "\n",
    "println(\"Moduły załadowane pomyślnie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Przygotowanie danych tekstowych\n",
    "\n",
    "Utworzymy mini-słownik i 2 proste \"zdania\" z różnymi wzorami sekwencyjnymi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Słownik:\n",
      "1: good\n",
      "2: bad\n",
      "3: movie\n",
      "4: very\n",
      "5: <PAD>\n",
      "\n",
      "Parametry:\n",
      "- Rozmiar słownika: 5\n",
      "- Wymiar embedding: 3\n",
      "- Długość sekwencji: 4\n"
     ]
    }
   ],
   "source": [
    "# Mini słownik (bardzo mały dla prostoty)\n",
    "vocab = [\"good\", \"bad\", \"movie\", \"very\", \"<PAD>\"]\n",
    "vocab_size = length(vocab)\n",
    "embedding_dim = 3  # Małe embedding dla łatwych obliczeń\n",
    "sequence_length = 4  # Długość sekwencji\n",
    "\n",
    "println(\"Słownik:\")\n",
    "for (i, word) in enumerate(vocab)\n",
    "    println(\"$i: $word\")\n",
    "end\n",
    "\n",
    "println(\"\\nParametry:\")\n",
    "println(\"- Rozmiar słownika: $vocab_size\")\n",
    "println(\"- Wymiar embedding: $embedding_dim\")\n",
    "println(\"- Długość sekwencji: $sequence_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sekwencja 1 (pozytywna): [\"very\", \"good\", \"movie\", \"<PAD>\"]\n",
      "Indeksy: Float32[4.0, 1.0, 3.0, 5.0]\n",
      "\n",
      "Sekwencja 2 (negatywna): [\"very\", \"bad\", \"movie\", \"<PAD>\"]\n",
      "Indeksy: Float32[4.0, 2.0, 3.0, 5.0]\n",
      "\n",
      "Shape danych X: (4, 2)\n",
      "Etykiety y: Float32[1.0 0.0]\n",
      "Shape etykiet: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# Sekwencje tekstowe jako indeksy\n",
    "# Sekwencja 1: \"very good movie <PAD>\" → [5, 2, 4, 1] → pozytywna (klasa 1)\n",
    "# Sekwencja 2: \"very bad movie <PAD>\" → [5, 3, 4, 1] → negatywna (klasa 0)\n",
    "\n",
    "sequence_1 = Float32[4, 1, 3, 5]  # very good movie <PAD>\n",
    "sequence_2 = Float32[4, 2, 3, 5]  # very bad movie <PAD>\n",
    "\n",
    "# Konwertuj na format batch (sequence_length, batch_size)\n",
    "X_batch = zeros(Float32, sequence_length, 2)\n",
    "X_batch[:, 1] = sequence_1\n",
    "X_batch[:, 2] = sequence_2\n",
    "\n",
    "# Etykiety: sekwencja 1 → pozytywna (1), sekwencja 2 → negatywna (0)\n",
    "y_batch = Float32[1.0 0.0]  # (1, 2) \n",
    "\n",
    "println(\"Sekwencja 1 (pozytywna): \", [vocab[Int(i)] for i in sequence_1])\n",
    "println(\"Indeksy: \", sequence_1)\n",
    "println(\"\\nSekwencja 2 (negatywna): \", [vocab[Int(i)] for i in sequence_2])\n",
    "println(\"Indeksy: \", sequence_2)\n",
    "println(\"\\nShape danych X: \", size(X_batch))\n",
    "println(\"Etykiety y: \", y_batch)\n",
    "println(\"Shape etykiet: \", size(y_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definicja modelu CNN dla NLP\n",
    "\n",
    "Architektura podobna do Flux:\n",
    "- **Embedding**: słowa → wektory\n",
    "- **Conv1D**: wykrywa n-gramy (wzory w sekwencjach)\n",
    "- **MaxPool**: wybiera najważniejsze cechy\n",
    "- **Flatten**: spłaszczenie\n",
    "- **Dense**: klasyfikacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parametry modelu NLP:\n",
      "- Embedding: 5 słów → 3 wymiarów\n",
      "- CNN: 2 filtrów, kernel size 2 (analizuje 2 słowa naraz)\n",
      "- Warstwa ukryta: 4 neuronów\n",
      "- Wyjście: 1 neuron (sentiment pozytywny/negatywny)\n",
      "- Batch size: 2\n"
     ]
    }
   ],
   "source": [
    "# Parametry modelu\n",
    "vocab_size = 5\n",
    "embedding_dim = 3\n",
    "conv_filters = 2        # Liczba filtrów CNN\n",
    "kernel_size = 2         # Rozmiar kernela (analizuje 2 kolejne słowa)\n",
    "hidden_size = 4         # Rozmiar warstwy ukrytej\n",
    "output_size = 1         # Klasyfikacja binarna\n",
    "batch_size = 2\n",
    "\n",
    "println(\"Parametry modelu NLP:\")\n",
    "println(\"- Embedding: $vocab_size słów → $embedding_dim wymiarów\")\n",
    "println(\"- CNN: $conv_filters filtrów, kernel size $kernel_size (analizuje $kernel_size słowa naraz)\")\n",
    "println(\"- Warstwa ukryta: $hidden_size neuronów\")\n",
    "println(\"- Wyjście: $output_size neuron (sentiment pozytywny/negatywny)\")\n",
    "println(\"- Batch size: $batch_size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model CNN dla NLP utworzony pomyślnie!\n",
      "Liczba warstw: 6\n",
      "\n",
      "Architektura:\n",
      "1. Embedding: słowa → wektory 3-wymiarowe\n",
      "2. Conv1D: wykrywa wzory w 2 kolejnych słowach\n",
      "3. MaxPool: wybiera najważniejsze cechy\n",
      "4. Flatten: spłaszczenie do wektora\n",
      "5. Dense: ukryta warstwa z ReLU\n",
      "6. Dense: klasyfikacja z sigmoid\n"
     ]
    }
   ],
   "source": [
    "# Model: Embedding → Conv1D → Pool → Flatten → Dense → Dense\n",
    "model = Chain(\n",
    "    Embedding(vocab_size, embedding_dim; name=\"embedding\"),\n",
    "    ConvolutionBlock(conv_filters, kernel_size; name=\"conv1d\"),\n",
    "    PoolingBlock(2; name=\"maxpool\"),  # Max pooling\n",
    "    FlattenBlock(name=\"flatten\"),\n",
    "    Dense(3, hidden_size, relu; name=\"hidden\"),  # 3 cechy po flatten (sprawdzimy to)\n",
    "    Dense(hidden_size, output_size, σ; name=\"output\")\n",
    ")\n",
    "\n",
    "println(\"Model CNN dla NLP utworzony pomyślnie!\")\n",
    "println(\"Liczba warstw: \", length(model.layers))\n",
    "\n",
    "println(\"\\nArchitektura:\")\n",
    "println(\"1. Embedding: słowa → wektory $embedding_dim-wymiarowe\")\n",
    "println(\"2. Conv1D: wykrywa wzory w $kernel_size kolejnych słowach\")\n",
    "println(\"3. MaxPool: wybiera najważniejsze cechy\")\n",
    "println(\"4. Flatten: spłaszczenie do wektora\")\n",
    "println(\"5. Dense: ukryta warstwa z ReLU\")\n",
    "println(\"6. Dense: klasyfikacja z sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ręczne ustawienie wag dla prostoty obliczeń\n",
    "\n",
    "Ustawimy proste wartości, które pozwolą na łatwe obliczenia ręczne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding weights (3×5 - wymiary × słowa):\n",
      "\n",
      "Interpretacja embeddingów:\n",
      "good: Float32[1.0, 0.5, 1.0]\n",
      "bad: Float32[-1.0, 0.5, -1.0]\n",
      "movie: Float32[0.5, 0.0, 1.0]\n",
      "very: Float32[0.0, 1.0, 0.0]\n",
      "<PAD>: Float32[0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3×5 Matrix{Float32}:\n",
       " 1.0  -1.0  0.5  0.0  0.0\n",
       " 0.5   0.5  0.0  1.0  0.0\n",
       " 1.0  -1.0  1.0  0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Embedding weights - każde słowo ma swój unikalny wektor\n",
    "embedding_weights = Float32[\n",
    "    1.0  -1.0   0.5  0.0 0.0;   # wymiar 1: <PAD>, good, bad, movie, very\n",
    "    0.5   0.5   0.0  1.0 0.0;   # wymiar 2\n",
    "    1.0  -1.0   1.0  0.0 0.0    # wymiar 3\n",
    "]\n",
    "model.layers[1].W.output = embedding_weights\n",
    "\n",
    "println(\"Embedding weights (3×5 - wymiary × słowa):\")\n",
    "display(embedding_weights)\n",
    "println(\"\\nInterpretacja embeddingów:\")\n",
    "for (i, word) in enumerate(vocab)\n",
    "    vec = embedding_weights[:, i]\n",
    "    println(\"$word: $vec\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtry konwolucyjne (kernel_size×num_filters = 2×2):\n",
      "\n",
      "Filtr 1: [1.0, -1.0] - wykrywa pozytywne trendy\n",
      "Filtr 2: [0.0, 1.0] - wykrywa drugie słowo w parze\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Float32}:\n",
       "  1.0  0.0\n",
       " -1.0  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filtry konwolucyjne - wykrywają różne wzory 2-gramów\n",
    "conv_weights = Float32[\n",
    "    1.0   0.0;    # Filtr 1: wykrywa pozytywne przejścia\n",
    "    -1.0  1.0     # Filtr 2: wykrywa negatywne wzory\n",
    "]\n",
    "model.layers[2].masks.output = conv_weights\n",
    "\n",
    "println(\"Filtry konwolucyjne (kernel_size×num_filters = 2×2):\")\n",
    "display(conv_weights)\n",
    "println(\"\\nFiltr 1: [1.0, -1.0] - wykrywa pozytywne trendy\")\n",
    "println(\"Filtr 2: [0.0, 1.0] - wykrywa drugie słowo w parze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×3 Matrix{Float32}:\n",
       "  1.0   0.5  -0.5\n",
       "  0.5   1.0   1.0\n",
       " -0.5   0.0   1.0\n",
       "  1.0  -1.0   0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1×4 Matrix{Float32}:\n",
       " 0.5  -0.5  1.0  -1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wagi warstwy ukrytej (4×3):\n",
      "\n",
      "Wagi warstwy wyjściowej (1×4):\n"
     ]
    }
   ],
   "source": [
    "# Wagi Dense layers\n",
    "model.layers[5].W.output = Float32[  # hidden layer weights (4×3)\n",
    "    1.0   0.5  -0.5;\n",
    "    0.5   1.0   1.0;\n",
    "    -0.5  0.0   1.0;\n",
    "    1.0  -1.0   0.5\n",
    "]\n",
    "\n",
    "bias_vector = Float32[0.0; 0.0; 0.0; 0.0]\n",
    "bias_matrix = reshape(bias_vector, 4, 1)\n",
    "model.layers[5].b.output = bias_matrix\n",
    "\n",
    "bias_vector = Float32[0.5 -0.5 1.0 -1.0]\n",
    "bias_matrix = reshape(bias_vector, 1, 4)\n",
    "model.layers[6].W.output = bias_matrix\n",
    "\n",
    "bias_vector = Float32[0.0]\n",
    "bias_matrix = reshape(bias_vector, 1, 1)\n",
    "model.layers[6].b.output = bias_matrix\n",
    "\n",
    "println(\"Wagi warstwy ukrytej (4×3):\")\n",
    "display(model.layers[5].W.output)\n",
    "println(\"\\nWagi warstwy wyjściowej (1×4):\")\n",
    "display(model.layers[6].W.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Budowanie grafu obliczeniowego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×2 Matrix{Float32}:\n",
       " 4.0  4.0\n",
       " 1.0  2.0\n",
       " 3.0  3.0\n",
       " 5.0  5.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1×2 Matrix{Float32}:\n",
       " 1.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dane wejściowe ustawione:\n",
      "Shape sekwencji X: (4, 2)\n",
      "Shape etykiet y: (1, 2)\n",
      "\n",
      "Sekwencje jako indeksy:\n",
      "\n",
      "Etykiety:\n"
     ]
    }
   ],
   "source": [
    "# Utworzenie węzłów wejściowych\n",
    "x_input_node = Constant(zeros(Float32, sequence_length, batch_size))\n",
    "y_label_node = Constant(zeros(Float32, output_size, batch_size))\n",
    "\n",
    "# Ustaw dane wejściowe\n",
    "x_input_node.output = X_batch\n",
    "y_label_node.output = y_batch\n",
    "\n",
    "println(\"Dane wejściowe ustawione:\")\n",
    "println(\"Shape sekwencji X: \", size(x_input_node.output))\n",
    "println(\"Shape etykiet y: \", size(y_label_node.output))\n",
    "println(\"\\nSekwencje jako indeksy:\")\n",
    "display(x_input_node.output)\n",
    "println(\"\\nEtykiety:\")\n",
    "display(y_label_node.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graf obliczeniowy zbudowany pomyślnie!\n",
      "Liczba węzłów w grafie: 20\n",
      "Typ węzła loss: ScalarOperator{typeof(Main.MyReverseDiff.binary_cross_entropy_loss_impl)}\n",
      "Typ węzła output: BroadcastedOperator{typeof(σ)}\n"
     ]
    }
   ],
   "source": [
    "# Zbuduj graf obliczeniowy\n",
    "loss_node, model_output_node, order = build_graph!(model, binarycrossentropy, x_input_node, y_label_node; loss_name=\"loss\")\n",
    "\n",
    "println(\"Graf obliczeniowy zbudowany pomyślnie!\")\n",
    "println(\"Liczba węzłów w grafie: \", length(order))\n",
    "println(\"Typ węzła loss: \", typeof(loss_node))\n",
    "println(\"Typ węzła output: \", typeof(model_output_node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Forward Pass - analiza krok po kroku\n",
    "\n",
    "Przeanalizujemy jak model przetwarza sekwencje słów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DANE WEJŚCIOWE - SEKWENCJE SŁÓW ===\n",
      "\n",
      "Sekwencja 1 (pozytywna):\n",
      "Słowa: [\"very\", \"good\", \"movie\", \"<PAD>\"]\n",
      "Indeksy: Float32[4.0, 1.0, 3.0, 5.0]\n",
      "\n",
      "Sekwencja 2 (negatywna):\n",
      "Słowa: [\"very\", \"bad\", \"movie\", \"<PAD>\"]\n",
      "Indeksy: Float32[4.0, 2.0, 3.0, 5.0]\n",
      "\n",
      "Zadanie: Klasyfikuj sentiment (pozytywny=1, negatywny=0)\n"
     ]
    }
   ],
   "source": [
    "println(\"=== DANE WEJŚCIOWE - SEKWENCJE SŁÓW ===\")\n",
    "println(\"\\nSekwencja 1 (pozytywna):\")\n",
    "seq1_words = [vocab[Int(i)] for i in X_batch[:, 1]]\n",
    "seq1_indices = X_batch[:, 1]\n",
    "println(\"Słowa: \", seq1_words)\n",
    "println(\"Indeksy: \", seq1_indices)\n",
    "\n",
    "println(\"\\nSekwencja 2 (negatywna):\")\n",
    "seq2_words = [vocab[Int(i)] for i in X_batch[:, 2]]\n",
    "seq2_indices = X_batch[:, 2]\n",
    "println(\"Słowa: \", seq2_words)\n",
    "println(\"Indeksy: \", seq2_indices)\n",
    "\n",
    "println(\"\\nZadanie: Klasyfikuj sentiment (pozytywny=1, negatywny=0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FORWARD PASS ===\n",
      "Wykonuję forward pass...\n"
     ]
    },
    {
     "ename": "DimensionMismatch",
     "evalue": "DimensionMismatch: A has dimensions (4,3) but B has dimensions (8,2)",
     "output_type": "error",
     "traceback": [
      "DimensionMismatch: A has dimensions (4,3) but B has dimensions (8,2)\n",
      "\n",
      "Stacktrace:\n",
      "  [1] gemm_wrapper!(C::Matrix{Float32}, tA::Char, tB::Char, A::Matrix{Float32}, B::Matrix{Float32}, _add::LinearAlgebra.MulAddMul{true, true, Bool, Bool})\n",
      "    @ LinearAlgebra /opt/julia/julia-1.11.0/share/julia/stdlib/v1.11/LinearAlgebra/src/matmul.jl:629\n",
      "  [2] generic_matmatmul!\n",
      "    @ /opt/julia/julia-1.11.0/share/julia/stdlib/v1.11/LinearAlgebra/src/matmul.jl:381 [inlined]\n",
      "  [3] _mul!\n",
      "    @ /opt/julia/julia-1.11.0/share/julia/stdlib/v1.11/LinearAlgebra/src/matmul.jl:287 [inlined]\n",
      "  [4] mul!\n",
      "    @ /opt/julia/julia-1.11.0/share/julia/stdlib/v1.11/LinearAlgebra/src/matmul.jl:285 [inlined]\n",
      "  [5] mul!\n",
      "    @ /opt/julia/julia-1.11.0/share/julia/stdlib/v1.11/LinearAlgebra/src/matmul.jl:253 [inlined]\n",
      "  [6] *\n",
      "    @ /opt/julia/julia-1.11.0/share/julia/stdlib/v1.11/LinearAlgebra/src/matmul.jl:124 [inlined]\n",
      "  [7] forward(::BroadcastedOperator{typeof(mul!)}, A::Matrix{Float32}, x::Matrix{Float32})\n",
      "    @ Main.MyReverseDiff ~/Repos/AWiD/MyMlp/src/MyReverseDiff.jl:78\n",
      "  [8] compute!(node::BroadcastedOperator{typeof(mul!)})\n",
      "    @ Main.MyReverseDiff ~/Repos/AWiD/MyMlp/src/MyReverseDiff.jl:407\n",
      "  [9] forward!(order::Vector{GraphNode})\n",
      "    @ Main.MyReverseDiff ~/Repos/AWiD/MyMlp/src/MyReverseDiff.jl:434\n",
      " [10] top-level scope\n",
      "    @ ~/Repos/AWiD/MyMlp/src/notebooks/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X24sZmlsZQ==.jl:4"
     ]
    }
   ],
   "source": [
    "# Wykonaj forward pass\n",
    "println(\"=== FORWARD PASS ===\")\n",
    "println(\"Wykonuję forward pass...\")\n",
    "forward_result = forward!(order)\n",
    "println(\"Forward pass zakończony pomyślnie!\")\n",
    "println(\"\\nWartość loss: \", loss_node.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"=== WYNIKI KLASYFIKACJI SENTIMENTU ===\")\n",
    "println(\"\\nPredykcje modelu: \", model_output_node.output)\n",
    "println(\"Prawdziwe etykiety: \", y_label_node.output)\n",
    "println(\"Binary Cross Entropy Loss: \", loss_node.output)\n",
    "\n",
    "# Interpretacja wyników\n",
    "predictions = model_output_node.output\n",
    "labels = y_label_node.output\n",
    "\n",
    "println(\"\\n=== INTERPRETACJA WYNIKÓW ===\")\n",
    "for i in 1:batch_size\n",
    "    words = [vocab[Int(j)] for j in X_batch[:, i]]\n",
    "    pred_prob = round(predictions[1, i], digits=4)\n",
    "    pred_class = pred_prob > 0.5 ? \"POZYTYWNY\" : \"NEGATYWNY\"\n",
    "    true_class = Int(labels[1, i]) == 1 ? \"POZYTYWNY\" : \"NEGATYWNY\"\n",
    "    correct = (pred_prob > 0.5) == (labels[1, i] == 1.0) ? \"✓\" : \"✗\"\n",
    "    \n",
    "    println(\"\\nSekwencja $i: $(join(words, \" \"))\")\n",
    "    println(\"  Predykcja: $pred_prob → $pred_class\")\n",
    "    println(\"  Prawda: $true_class $correct\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Weryfikacja ręczna - obliczenia embedding i konwolucji\n",
    "\n",
    "Policzymy ręcznie pierwsze kroki, żeby zrozumieć jak CNN analizuje tekst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"=== WERYFIKACJA RĘCZNA - EMBEDDING ===\")\n",
    "println(\"\\nEmbedding weights:\")\n",
    "display(embedding_weights)\n",
    "\n",
    "println(\"\\n1. EMBEDDING SEKWENCJI 1: 'very good movie <PAD>'\")\n",
    "println(\"Indeksy: [5, 2, 4, 1]\")\n",
    "\n",
    "println(\"\\nSlowo 'very' (indeks 5):\")\n",
    "very_embedding = embedding_weights[:, 5]\n",
    "println(\"Embedding: \", very_embedding)\n",
    "\n",
    "println(\"\\nSlowo 'good' (indeks 2):\")\n",
    "good_embedding = embedding_weights[:, 2]\n",
    "println(\"Embedding: \", good_embedding)\n",
    "\n",
    "println(\"\\nSlowo 'movie' (indeks 4):\")\n",
    "movie_embedding = embedding_weights[:, 4]\n",
    "println(\"Embedding: \", movie_embedding)\n",
    "\n",
    "println(\"\\nSlowo '<PAD>' (indeks 1):\")\n",
    "pad_embedding = embedding_weights[:, 1]\n",
    "println(\"Embedding: \", pad_embedding)\n",
    "\n",
    "println(\"\\nEmbedded sekwencja 1 (3×4 - embedding_dim × sequence_length):\")\n",
    "embedded_seq1 = hcat(very_embedding, good_embedding, movie_embedding, pad_embedding)\n",
    "display(embedded_seq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"\\n2. KONWOLUCJA 1D - WYKRYWANIE N-GRAMÓW\")\n",
    "println(\"\\nFiltry konwolucyjne (kernel size 2):\")\n",
    "display(conv_weights)\n",
    "println(\"Filtr 1: [1.0, -1.0] - wykrywa wzory pozytywne\")\n",
    "println(\"Filtr 2: [0.0, 1.0] - skupia się na drugim słowie w parze\")\n",
    "\n",
    "println(\"\\nKonwolucja analizuje pary słów:\")\n",
    "println(\"- Para 1: 'very' + 'good'\")\n",
    "println(\"- Para 2: 'good' + 'movie'\")\n",
    "println(\"- Para 3: 'movie' + '<PAD>'\")\n",
    "\n",
    "println(\"\\nPrzykład obliczenia dla pary 'very' + 'good':\")\n",
    "println(\"Very embedding: \", very_embedding)\n",
    "println(\"Good embedding: \", good_embedding)\n",
    "println(\"\\nFiltr 1 [1.0, -1.0] na wymiarze 1:\")\n",
    "println(\"1.0 × 0.0 + (-1.0) × 1.0 = -1.0\")\n",
    "println(\"\\nFiltr 2 [0.0, 1.0] na wymiarze 1:\")\n",
    "println(\"0.0 × 0.0 + 1.0 × 1.0 = 1.0\")\n",
    "\n",
    "println(\"\\nTo pokazuje jak CNN wykrywa różnice między 'very good' a 'very bad'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Backward Pass - uczenie się wzorów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"=== BACKWARD PASS - UCZENIE SIĘ ===\")\n",
    "println(\"Wykonuję backward pass...\")\n",
    "\n",
    "# Wykonaj backward pass\n",
    "backward!(order)\n",
    "println(\"Backward pass zakończony!\")\n",
    "\n",
    "println(\"\\n=== OBLICZONE GRADIENTY ===\")\n",
    "\n",
    "println(\"\\n1. Gradienty embeddingów (które słowa potrzebują poprawy):\")\n",
    "embedding_grads = model.layers[1].W.gradient\n",
    "display(embedding_grads)\n",
    "\n",
    "println(\"\\n2. Gradienty filtrów konwolucyjnych (które wzory poprawić):\")\n",
    "conv_grads = model.layers[2].masks.gradient\n",
    "display(conv_grads)\n",
    "\n",
    "println(\"\\n3. Gradienty warstwy klasyfikacyjnej:\")\n",
    "output_grads = model.layers[6].W.gradient\n",
    "display(output_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Podsumowanie i porównanie z Flux\n",
    "\n",
    "Analiza jak nasza implementacja porównuje się z profesjonalnymi bibliotekami NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"=== PODSUMOWANIE TESTU CNN DLA NLP ===\")\n",
    "\n",
    "println(\"\\n✓ Zadanie: Klasyfikacja sentimentu tekstu\")\n",
    "println(\"✓ Dane: 2 sekwencje po 4 słowa\")\n",
    "println(\"✓ Model: Embedding → Conv1D → Pool → Dense\")\n",
    "println(\"✓ Forward pass: wykonany\")\n",
    "println(\"✓ Backward pass: wykonany\")\n",
    "println(\"✓ Loss: \", round(loss_node.output, digits=6))\n",
    "\n",
    "println(\"\\n📊 Wyniki klasyfikacji sentimentu:\")\n",
    "for i in 1:batch_size\n",
    "    words = [vocab[Int(j)] for j in X_batch[:, i]]\n",
    "    pred = round(model_output_node.output[1, i], digits=4)\n",
    "    sentiment = pred > 0.5 ? \"POZYTYWNY\" : \"NEGATYWNY\"\n",
    "    true_label = Int(y_label_node.output[1, i]) == 1 ? \"POZYTYWNY\" : \"NEGATYWNY\"\n",
    "    println(\"  '$(join(words, \" \"))': $pred → $sentiment (cel: $true_label)\")\n",
    "end\n",
    "\n",
    "println(\"\\n🔍 Podobieństwa z Flux CNN:\")\n",
    "println(\"  ✓ Embedding layer - słowa na wektory\")\n",
    "println(\"  ✓ Conv1D - wykrywanie n-gramów\")\n",
    "println(\"  ✓ MaxPooling - selekcja najważniejszych cech\")\n",
    "println(\"  ✓ Dense - klasyfikacja finalna\")\n",
    "\n",
    "println(\"\\n🎯 Zastosowania:\")\n",
    "println(\"  - Analiza sentimentu (pozytywny/negatywny)\")\n",
    "println(\"  - Klasyfikacja tematyczna tekstów\")\n",
    "println(\"  - Wykrywanie spamu\")\n",
    "println(\"  - Analiza emocji w social media\")\n",
    "\n",
    "println(\"\\n💡 Kluczowe koncepty:\")\n",
    "println(\"  - N-gramy: CNN wykrywa wzory w parach/trójkach słów\")\n",
    "println(\"  - Embeddingi: przekształcają słowa w wektory liczb\")\n",
    "println(\"  - Pooling: wybiera najważniejsze cechy z całego tekstu\")\n",
    "println(\"  - End-to-end learning: cały model uczy się razem\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.0",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
