{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e231298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module MyReverseDiff.\n",
      "WARNING: replacing module MyMlp.\n"
     ]
    }
   ],
   "source": [
    "include(\"../MyReverseDiff.jl\")\n",
    "include(\"../MyEmbedding.jl\")\n",
    "include(\"../MyMlp.jl\")\n",
    "\n",
    "using .MyReverseDiff\n",
    "using .MyEmbedding\n",
    "using .MyMlp\n",
    "using JLD2\n",
    "using Printf\n",
    "using BenchmarkTools\n",
    "using LinearAlgebra\n",
    "using Distributions\n",
    "using Random\n",
    "using MLDatasets\n",
    "using Plots\n",
    "using Statistics\n",
    "using DataFrames\n",
    "using MLDataUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37288ea0",
   "metadata": {},
   "source": [
    "## Przygotowanie danych IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaed8015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17703"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = Matrix(load(\"../../dataset/imdb_dataset_prepared.jld2\", \"X_train\"));\n",
    "y_train = Matrix(load(\"../../dataset/imdb_dataset_prepared.jld2\", \"y_train\"));\n",
    "X_test = Matrix(load(\"../../dataset/imdb_dataset_prepared.jld2\", \"X_test\"));\n",
    "y_test = Matrix(load(\"../../dataset/imdb_dataset_prepared.jld2\", \"y_test\"));\n",
    "input_size = size(X_train, 1) # Liczba cech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b50633d",
   "metadata": {},
   "source": [
    "## Przygotowanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6bce9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamState(Adam(0.001f0, 0.9f0, 0.999f0, 1.0f-8), Dict{String, Matrix{Float32}}(\"layer2_b\" => [0.0;;], \"layer2_w\" => [0.0 0.0 … 0.0 0.0], \"layer1_b\" => [0.0; 0.0; … ; 0.0; 0.0;;], \"layer1_w\" => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]), Dict{String, Matrix{Float32}}(\"layer2_b\" => [0.0;;], \"layer2_w\" => [0.0 0.0 … 0.0 0.0], \"layer1_b\" => [0.0; 0.0; … ; 0.0; 0.0;;], \"layer1_w\" => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]), 0, Tuple{String, Variable}[(\"layer1_w\", var layer1_w\n",
       " ┣━ ^ 8×17703 Matrix{Float32}\n",
       " ┗━ ∇ 8×17703 Matrix{Float32}), (\"layer1_b\", var layer1_b\n",
       " ┣━ ^ 8×1 Matrix{Float32}\n",
       " ┗━ ∇ 8×1 Matrix{Float32}), (\"layer2_w\", var layer2_w\n",
       " ┣━ ^ 1×8 Matrix{Float32}\n",
       " ┗━ ∇ 1×8 Matrix{Float32}), (\"layer2_b\", var layer2_b\n",
       " ┣━ ^ 1×1 Matrix{Float32}\n",
       " ┗━ ∇ 1×1 Matrix{Float32})])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#   Definicja rozmiarów modelu\n",
    "input_size = size(X_train, 1) # Liczba cech\n",
    "hidden_size = 8\n",
    "output_size = 1\n",
    "batch_size = 64\n",
    "\n",
    "#   Inicjalizacja modelu (Chain) (raz)\n",
    "model = Chain(\n",
    "    Dense(input_size, hidden_size, relu; weight_init=xavier_uniform,  name=\"layer1\"),\n",
    "    Dense(hidden_size, output_size, σ; weight_init=xavier_uniform, name=\"layer2\")\n",
    ")\n",
    "\n",
    "#   Utworzenie początkowych węzłów Constant dla danych wejściowych i etykiet\n",
    "x_input_node = Constant(zeros(Float32, input_size, batch_size))\n",
    "y_label_node = Constant(zeros(Float32, output_size, batch_size))\n",
    "\n",
    "#   Budowanie grafu treningowego\n",
    "loss_node, model_output_node, order = build_graph!(model, binarycrossentropy, x_input_node, y_label_node; loss_name=\"loss\")\n",
    "\n",
    "optimizer_state = setup_optimizer(Adam(), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310a6521",
   "metadata": {},
   "source": [
    "##  Trening modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb93cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# epochs = 5\n",
    "\n",
    "\n",
    "# for epoch in 1:epochs\n",
    "#     # --- Tasowanie zbioru treningowego NA NOWO w każdej epoce ---\n",
    "#     permutation = randperm(size(X_train, 2))\n",
    "#     X_train_shuffled_epoch = X_train[:, permutation]\n",
    "#     y_train_shuffled_epoch = y_train[:, permutation]\n",
    "#     num_batches = ceil(Int, size(X_train, 2) / batch_size)\n",
    "\n",
    "#     loss_value = 0.0\n",
    "\n",
    "#     t = @elapsed begin\n",
    "\n",
    "#     for i in 1:num_batches\n",
    "\n",
    "#         start_idx = (i - 1) * batch_size + 1\n",
    "#         end_idx = min(i * batch_size, size(X_train, 2))\n",
    "#         x_batch = X_train_shuffled_epoch[:, start_idx:end_idx]\n",
    "#         y_batch = y_train_shuffled_epoch[:, start_idx:end_idx]\n",
    "\n",
    "#         current_batch_size = size(x_batch, 2)\n",
    "#         view(x_input_node.output, :, 1:current_batch_size) .= x_batch\n",
    "#         view(y_label_node.output, :, 1:current_batch_size) .= y_batch\n",
    "\n",
    "\n",
    "#         forward!(order)\n",
    "\n",
    "#         backward!(order)\n",
    "\n",
    "#         step!(optimizer_state)\n",
    "#         loss_value += loss_node.output\n",
    "\n",
    "#     end\n",
    "# end\n",
    "#     avg_loss_epoch = loss_value / num_batches\n",
    "\n",
    "#     println(@sprintf(\"Epoch: %d (%.2fs) \\tTrain: (l: %.2f)\", epoch, t, avg_loss_epoch))\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da77997c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Rozpoczynam profilowanie treningu ---\n",
      "\n",
      "Epoch: 1\n",
      "Epoch: 1 \tTrain: (l: 0.6720) \tTotal Batch Time: 4.3515s \tTotal Alloc: 1.817 GiB \tGC Time: 0.3709s\n",
      "\n",
      "Epoch: 2\n",
      "Epoch: 2 \tTrain: (l: 0.5920) \tTotal Batch Time: 0.6493s \tTotal Alloc: 1.258 GiB \tGC Time: 0.3036s\n",
      "\n",
      "Epoch: 3\n",
      "Epoch: 3 \tTrain: (l: 0.4958) \tTotal Batch Time: 0.6269s \tTotal Alloc: 1.258 GiB \tGC Time: 0.2955s\n",
      "\n",
      "Epoch: 4\n",
      "Epoch: 4 \tTrain: (l: 0.4042) \tTotal Batch Time: 0.6393s \tTotal Alloc: 1.258 GiB \tGC Time: 0.2931s\n",
      "\n",
      "Epoch: 5\n",
      "Epoch: 5 \tTrain: (l: 0.3267) \tTotal Batch Time: 0.6498s \tTotal Alloc: 1.258 GiB \tGC Time: 0.2906s\n",
      "\n",
      "--- Koniec profilowania treningu ---\n"
     ]
    }
   ],
   "source": [
    "using Printf # Dla @sprintf\n",
    "\n",
    "# ... (Twój istniejący kod: definicja modelu, x_input_node, y_label_node, build_graph!, setup_optimizer)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "println(\"--- Rozpoczynam profilowanie treningu ---\")\n",
    "\n",
    "for epoch in 1:epochs\n",
    "    # --- Tasowanie zbioru treningowego NA NOWO w każdej epoce ---\n",
    "    permutation = randperm(size(X_train, 2))\n",
    "    X_train_shuffled_epoch = X_train[:, permutation]\n",
    "    y_train_shuffled_epoch = y_train[:, permutation]\n",
    "    num_batches = ceil(Int, size(X_train, 2) / batch_size)\n",
    "\n",
    "    loss_value = 0.0\n",
    "\n",
    "    # Profilowanie pętli batchowej\n",
    "    # Użyj @time do pomiaru czasu i alokacji wewnątrz pętli.\n",
    "    # Ważne: pierwsze uruchomienie `@time` może być wolniejsze z powodu kompilacji JIT.\n",
    "    # Zrób jedno \"suche\" uruchomienie przed właściwym profilowaniem, jeśli to konieczne.\n",
    "    println(\"\\nEpoch: $epoch\")\n",
    "    total_batch_time = 0.0\n",
    "    total_batch_alloc = 0\n",
    "    total_batch_gc_time = 0.0\n",
    "\n",
    "    for i in 1:num_batches\n",
    "        start_idx = (i - 1) * batch_size + 1\n",
    "        end_idx = min(i * batch_size, size(X_train, 2))\n",
    "        x_batch_view = view(X_train_shuffled_epoch, :, start_idx:end_idx)\n",
    "        y_batch_view = view(y_train_shuffled_epoch, :, start_idx:end_idx)\n",
    "\n",
    "        current_batch_size = size(x_batch_view, 2)\n",
    "        view(x_input_node.output, :, 1:current_batch_size) .= x_batch_view\n",
    "        view(y_label_node.output, :, 1:current_batch_size) .= y_batch_view\n",
    "\n",
    "        # Profilowanie pojedynczego kroku batcha\n",
    "        # Użyj `@time` dla całego bloku operacji\n",
    "        # Wynik `@time` to tuple: (time, bytes, gctime, compile_time, reclaim_ratio)\n",
    "        # Bierzemy tylko czas i alokacje\n",
    "        \n",
    "        # Aby uzyskać czyste pomiary alokacji, najlepiej zrobić to dla jednego batcha\n",
    "        # i wywołać `GC.gc()` przed każdym pomiarem, aby upewnić się, że mierzymy świeże alokacje.\n",
    "        # W pętli treningowej to jednak zaburzyłoby realny czas wykonania.\n",
    "        # Na potrzeby wstępnego profilowania, `@time` w pętli jest ok.\n",
    "\n",
    "        stats = @timed begin # `timed` zwraca strukturę z wynikami, `time` tylko czas\n",
    "            forward!(order)\n",
    "            backward!(order)\n",
    "            step!(optimizer_state) # Zakładam, że masz już zaimplementowane step!\n",
    "        end\n",
    "        loss_value += loss_node.output # Upewnij się, że loss_node.output jest odświeżane po forward\n",
    "\n",
    "        total_batch_time += stats.time\n",
    "        total_batch_alloc += stats.bytes\n",
    "        total_batch_gc_time += stats.gctime\n",
    "    end\n",
    "    \n",
    "    avg_loss_epoch = loss_value / num_batches\n",
    "\n",
    "    println(@sprintf(\"Epoch: %d \\tTrain: (l: %.4f) \\tTotal Batch Time: %.4fs \\tTotal Alloc: %s \\tGC Time: %.4fs\", \n",
    "                     epoch, avg_loss_epoch, total_batch_time, Base.format_bytes(total_batch_alloc), total_batch_gc_time))\n",
    "end\n",
    "\n",
    "println(\"\\n--- Koniec profilowania treningu ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ef078",
   "metadata": {},
   "source": [
    "##  Test modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f308f319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss (czas: 0.53s): 0.4062\n",
      "Test Accuracy: 87.1 %\n"
     ]
    }
   ],
   "source": [
    "# --- Test Evaluation ---\n",
    "\n",
    "batch_size = 64\n",
    "num_test_samples = size(X_test, 2)\n",
    "num_batches = ceil(Int, num_test_samples / batch_size)\n",
    "total_test_loss_sum = 0.0\n",
    "total_correct_predictions = 0.0\n",
    "\n",
    "t_test = @elapsed begin\n",
    "    for i in 1:num_batches\n",
    "\n",
    "        start_idx = (i - 1) * batch_size + 1\n",
    "        end_idx = min(i * batch_size, num_test_samples)\n",
    "        x_batch_test = X_test[:, start_idx:end_idx]\n",
    "        y_batch_test = y_test[:, start_idx:end_idx]\n",
    "\n",
    "        # Aktualna liczba próbek w bieżącym batchu (może być mniejsza dla ostatniego batcha)\n",
    "        current_test_batch_size = size(x_batch_test, 2)\n",
    "\n",
    "        view(x_input_node.output, :, 1:current_test_batch_size) .= x_batch_test\n",
    "        view(y_label_node.output, :, 1:current_test_batch_size) .= y_batch_test\n",
    "\n",
    "        forward!(order)\n",
    "\n",
    "        predictions = view(model_output_node.output, :, 1:current_test_batch_size)\n",
    "\n",
    "\n",
    "        batch_loss = loss_node.output\n",
    "        \n",
    "        total_test_loss_sum += batch_loss * current_test_batch_size # Sumuj stratę, uwzględniając rozmiar batcha\n",
    "\n",
    "        # --- Oblicz dokładność na bieżącym batchu testowym ---\n",
    "        # Dla klasyfikacji binarnej z progiem 0.5 (lub innym, w zależności od problemu)\n",
    "        batch_accuracy = sum((predictions .> 0.5f0) .== y_batch_test) / current_test_batch_size\n",
    "        total_correct_predictions += batch_accuracy * current_test_batch_size # Sumuj poprawne predykcje\n",
    "    end\n",
    "end\n",
    "\n",
    "# --- Oblicz średnią stratę i średnią dokładność na całym zbiorze testowym ---\n",
    "avg_test_loss = total_test_loss_sum / num_test_samples\n",
    "avg_test_accuracy = total_correct_predictions / num_test_samples * 100.0\n",
    "\n",
    "println(@sprintf(\"Test Loss (czas: %.2fs): %.4f\", t_test, avg_test_loss))\n",
    "println(\"Test Accuracy: $avg_test_accuracy %\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
